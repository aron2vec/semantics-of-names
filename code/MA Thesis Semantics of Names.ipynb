{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "22531ca9-e756-46d1-b70a-3e471d5c92a8",
      "metadata": {
        "id": "22531ca9-e756-46d1-b70a-3e471d5c92a8"
      },
      "source": [
        "# Master Thesis on the Semantics of (made-up) Names\n",
        "\n",
        "* Author: Aron Joosse\n",
        "* Supervisor: Giovanni Cassani\n",
        "* Institution: Tilburg University\n",
        "\n",
        "Can take inspiration from: https://github.com/Masetto96/BA-Thesis-form-meaning-mapping/blob/master/form_meaning_mapping.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6549cc30-115e-4d41-a905-85e232d32540",
      "metadata": {
        "id": "6549cc30-115e-4d41-a905-85e232d32540"
      },
      "source": [
        "# Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9e01d4f6-9943-40fb-9f85-ca2aa53b4650",
      "metadata": {
        "id": "9e01d4f6-9943-40fb-9f85-ca2aa53b4650",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36abe4e0-05ee-41e9-e547-d08295ba69e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.5)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.2.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.13)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext --progress-bar off\n",
        "!pip install -U spacy --progress-bar off\n",
        "!python -m spacy download en_core_web_sm\n",
        "import fasttext\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f1ffd0-4597-433a-b9c7-bb2423556386",
      "metadata": {
        "id": "92f1ffd0-4597-433a-b9c7-bb2423556386"
      },
      "source": [
        "# Data Import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Being able to access Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True) "
      ],
      "metadata": {
        "id": "_kihXMIxsBnq",
        "outputId": "469940f1-5e6d-4c9d-f576-6bcdef4de7f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_kihXMIxsBnq",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Getting the list of madeup names:\n",
        "\n",
        "ratings_csv = pd.read_csv(\"drive/MyDrive/Thesis/Data/giovanni_email_data/avgRatings_annotated.csv\",\n",
        "                          usecols = [\"name\", \"name_type\"])\n",
        "\n",
        "ratings_csv.head(10)\n",
        "\n",
        "madeup_names = []\n",
        "\n",
        "for i in ratings_csv.index:                                           ## I can do exactly the same thing for talking & real\n",
        "  if ratings_csv[\"name_type\"][i] == \"madeup\":\n",
        "    madeup_names.append(str(ratings_csv[\"name\"][i]))\n",
        "\n",
        "madeup_names_lower = list(map(lambda x: x.lower(), madeup_names))\n",
        "\n",
        "print(madeup_names[:5])\n",
        "print(len(madeup_names))\n",
        "print(madeup_names_lower[:5])\n",
        "print(len(madeup_names_lower))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AUEyOhEpf74",
        "outputId": "7f6e28a9-6a5c-45fb-cc9e-6676c0f12810"
      },
      "id": "0AUEyOhEpf74",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Alastor', 'Alecto', 'Amabala', 'Araminta', 'Arcturus']\n",
            "60\n",
            "['alastor', 'alecto', 'amabala', 'araminta', 'arcturus']\n",
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NdAKmpO_raz-"
      },
      "id": "NdAKmpO_raz-",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "34910b3e-c2f9-4d54-983d-e75c2127f4db",
      "metadata": {
        "id": "34910b3e-c2f9-4d54-983d-e75c2127f4db"
      },
      "source": [
        "## COCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bf7f61e8-e967-4ab9-895e-6fb974df4fc4",
      "metadata": {
        "id": "bf7f61e8-e967-4ab9-895e-6fb974df4fc4"
      },
      "outputs": [],
      "source": [
        "path = \"drive/My Drive/Thesis/Data/CoCA/Text/\"\n",
        "unclean_path = path + \"texts_combined/all_texts_combined.txt\"\n",
        "unclean_corpus = open(unclean_path).read()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(unclean_corpus))\n",
        "print(unclean_corpus[:100])"
      ],
      "metadata": {
        "id": "cuIyN1lBMe05",
        "outputId": "db16849f-680d-4356-b582-e62aaadcf61c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cuIyN1lBMe05",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2977527143\n",
            "@@4170367 Headnote # A puzzle has long pervaded the criminal law : why are two offenders who commit \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a35ba177-63c9-43e0-8eb0-cc41ada452d6",
      "metadata": {
        "id": "a35ba177-63c9-43e0-8eb0-cc41ada452d6"
      },
      "source": [
        "## Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "efb0d50d-543f-4483-9b58-83812e4c7418",
      "metadata": {
        "id": "efb0d50d-543f-4483-9b58-83812e4c7418"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b17c25-4631-4971-8c9e-9b13b9322a08",
      "metadata": {
        "id": "45b17c25-4631-4971-8c9e-9b13b9322a08"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c46490a-70a1-41ae-8f63-ccf7fb08da08",
      "metadata": {
        "id": "6c46490a-70a1-41ae-8f63-ccf7fb08da08"
      },
      "source": [
        "\n",
        "## Cleaning Corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading the English spacy pipeline and removing stopwords\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.max_length = 10000000000\n",
        "\n",
        "nlp.Defaults.stop_words.remove('him')\n",
        "nlp.Defaults.stop_words.remove('her')\n",
        "nlp.Defaults.stop_words.remove('hers')\n",
        "nlp.Defaults.stop_words.remove('his')\n",
        "nlp.Defaults.stop_words.remove('he')\n",
        "nlp.Defaults.stop_words.remove('she')\n",
        "nlp.Defaults.stop_words.remove('himself')\n",
        "nlp.Defaults.stop_words.remove('herself')"
      ],
      "metadata": {
        "id": "xqknf8oNouf3"
      },
      "id": "xqknf8oNouf3",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a34cc32e-33c0-4f47-a08f-e7b37dd30f8a",
      "metadata": {
        "id": "a34cc32e-33c0-4f47-a08f-e7b37dd30f8a",
        "outputId": "823df0a6-0370-4e01-c39d-ceb8617d37be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-163e697f8b7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Remove words with freq < XX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mclean_corpus_unsentenced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munclean_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#[:1000000])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-163e697f8b7c>\u001b[0m in \u001b[0;36mclean_corpus_unsentenced\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_corpus_unsentenced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1003\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \"\"\"\n\u001b[0;32m-> 1005\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE866\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m             )\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ensure_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E025] String is too long: 2977527143 characters. Max is 2**30."
          ]
        }
      ],
      "source": [
        "def clean_corpus_unsentenced(data):\n",
        "    # Tokenization\n",
        "    with nlp.select_pipes(disable=[\"lemmatizer\", \"tok2vec\", \"tagger\", \"parser\"]):\n",
        "      nlp.enable_pipe(\"senter\")\n",
        "      doc = nlp(data)\n",
        "    print(doc[:150])\n",
        "\n",
        "    doc_filtered = []\n",
        "\n",
        "    for token in doc:\n",
        "      if token.is_upper is True:\n",
        "        continue\n",
        "      elif token.is_stop is True:\n",
        "        continue\n",
        "      elif str(token).lower() in madeup_names_lower:\n",
        "        continue\n",
        "      elif token.is_alpha:\n",
        "        doc_filtered.append(str(token).lower())\n",
        "      else: \n",
        "        continue\n",
        "\n",
        "    doc_filtered = \" \".join(doc_filtered)\n",
        "\n",
        "    print(doc_filtered[:500])\n",
        "\n",
        "    # Remove words with freq < XX\n",
        "\n",
        "clean_corpus_unsentenced(unclean_corpus)#[:1000000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_corpus_sentenced(data):\n",
        "    # Tokenization\n",
        "    doc = nlp(data)\n",
        "    print(doc[:50])\n",
        "    print(list(doc.sents)[:15])\n",
        "\n",
        "    doc_filtered = []\n",
        "\n",
        "    sentence = []\n",
        "\n",
        "    for token in doc:\n",
        "      if token.is_sent_start is True:\n",
        "        doc_filtered.append(\" \".join(sentence))\n",
        "        sentence = []\n",
        "      \n",
        "      if token.is_upper is True:\n",
        "        continue\n",
        "      elif token.is_stop is True:\n",
        "        continue\n",
        "      elif str(token).lower() in madeup_names_lower:\n",
        "        continue\n",
        "      elif token.is_alpha:\n",
        "        sentence.append(str(token).lower())\n",
        "\n",
        "    doc_filtered = list(filter(lambda x: x != \"\", doc_filtered))\n",
        "    print(doc_filtered[:15])\n",
        "\n",
        "    # Remove words with freq < XX\n",
        "    \n",
        "    ## Because I'm using fastText, I think it is best to remove words whose lemma occurs < X times,\n",
        "    ## since I don't really care if e.g., 'running' occurs only once if 'run' and '-ing' both occur\n",
        "    ## a bunch of times. So it's probably best to calculate the frequency of the lemmas and then remove\n",
        "    ## words whose lemma occurs less than 3 times? \n",
        "    ## I still also don't know how to set my frequency threshold properly, so idk man.\n",
        "\n",
        "    ## Also, I would like to do a stop_words removed vs. stop_words not removed comparison to see whether\n",
        "    ## performance actually increases when I remove stop words, because I don't know whether it actually\n",
        "    ## does for the purpose of my specific analysis \n",
        "    \n",
        "    ## --> I can look at what other papers did maybe, for minimum word freq. and stop words!\n",
        "\n",
        "clean_corpus_sentenced(unclean_corpus[:1000000])"
      ],
      "metadata": {
        "id": "_GxXQaOBOa9N",
        "outputId": "7e513723-c0af-49b0-bc1e-1c1ededb279b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_GxXQaOBOa9N",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@@4170367 Headnote # A puzzle has long pervaded the criminal law : why are two offenders who commit the same criminal act punished differently when one of them , due to circumstances beyond her control , causes more harm than the other ? This tradition of result-based differential\n",
            "[@@4170367, Headnote # A puzzle has long pervaded the criminal law : why are two offenders who commit the same criminal act punished differently when one of them , due to circumstances beyond her control , causes more harm than the other ?, This tradition of result-based differential punishment-the practice of varying offenders ' punishment based on whether or not they cause specific \" statutory harms \" -has long stood as an intractable problem for scholars and jurists alike ., # This Article proposes a solution to this long-standing conceptual problem ., We begin by introducing a dichotomy between two broad and exhaustive categories of ideological justifications for punishing criminal offenders ., The first category , offender-facing justifications , includes many of the most familiar theories of punishment : deterrence , retribution , incapacitation , and rehabilitation ., These offender-facing theories seek to justify punishment solely on the basis of facts about a criminal offender , such as her behavior , mental states , and perceived level of dangerousness ., Yet , as we demonstrate , because offender-facing theories turn exclusively on facts about an offender and her conduct-rather than on @ @ @ @ @ @ @ @ @ @ can not provide adequate justification for the practice of differential punishment ., # We also identify a second category of justifications for punishment that , at least in part , conditions the severity of criminal punishment on the effects that a particular criminal offense has on its victims ., These victim-facing justifications include both \" expressive \" theories of punishment , according to which offenders should be punished out of respect for the victims they have harmed , and vengeance-based theories of punishment , according to which punishment serves to recognize and legitimate victims ' desire for revenge against their offenders ., Because victim-facing justifications focus on the harm that crimes cause to victims , they are , if valid , theoretically capable of justifying differential punishment ., #, However , we will show that victim-facing justifications for punishment are not available for every instance of criminal misconduct ., When a criminal offense ( 1 ) has no \" object \" ( in that it is not \" done \" to anyone ) , ( 2 ) has a \" victim \" who either consented to , or was otherwise @ @ @ @ @ @ @, @ @ @ ( 3 ) has a victim who desires to show \" mercy \" to the offender , victim-facing theories can not justify differential punishment , rendering the practice categorically unjustifiable in such cases .]\n",
            "['headnote puzzle long pervaded criminal law offenders commit criminal act punished differently circumstances her control causes harm', 'tradition result based differential punishment practice varying offenders punishment based cause specific statutory harms long stood intractable problem scholars jurists alike', 'article proposes solution long standing conceptual problem', 'begin introducing dichotomy broad exhaustive categories ideological justifications punishing criminal offenders', 'category offender facing justifications includes familiar theories punishment deterrence retribution incapacitation rehabilitation', 'offender facing theories seek justify punishment solely basis facts criminal offender her behavior mental states perceived level dangerousness', 'demonstrate offender facing theories turn exclusively facts offender her conduct provide adequate justification practice differential punishment', 'identify second category justifications punishment conditions severity criminal punishment effects particular criminal offense victims', 'victim facing justifications include expressive theories punishment according offenders punished respect victims harmed vengeance based theories punishment according punishment serves recognize legitimate victims desire revenge offenders', 'victim facing justifications focus harm crimes cause victims valid theoretically capable justifying differential punishment', 'victim facing justifications punishment available instance criminal misconduct', 'criminal offense object victim consented', 'victim desires mercy offender victim facing theories justify differential punishment rendering practice categorically unjustifiable cases', 'conclude arguing instances differential punishment unjustified offenders punished brought harmful result subject heightened punishment', 'roommates purdue university indiana drinking living room saturday night landon siela headed bathroom']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "MCJ592ZYMSXB",
        "outputId": "7ef56a52-2e44-478d-de55-13a6292ad0cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MCJ592ZYMSXB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All changes made in this colab session should now be visible in Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09e790a-e08f-4144-b01e-25af29a8816f",
      "metadata": {
        "id": "a09e790a-e08f-4144-b01e-25af29a8816f"
      },
      "source": [
        "## Training fastText and Validating on Word Embeddings Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3909f0-5298-4ef8-b578-c759a62ab17c",
      "metadata": {
        "id": "0d3909f0-5298-4ef8-b578-c759a62ab17c"
      },
      "outputs": [],
      "source": [
        "# Skipgram model :\n",
        "#model = fasttext.train_unsupervised('data.txt', model='skipgram')\n",
        "\n",
        "#model.save_model(\"model_filename.bin\")\n",
        "\n",
        "#model = fasttext.load_model(\"model_filename.bin\")\n",
        "\n",
        "#model.get_nearest_neighbors('asparagus')\n",
        "\n",
        "#In a similar spirit, one can play around with word analogies. For example, we can see if our model can guess what is to France, and what Berlin is to Germany.\n",
        "#This can be done with the analogies functionality. It takes a word triplet (like Germany Berlin France) and outputs the analogy:\n",
        "#model.get_analogies(\"berlin\", \"germany\", \"france\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa3cc3c-e1a4-489b-b0c6-e25bd045783f",
      "metadata": {
        "id": "aaa3cc3c-e1a4-489b-b0c6-e25bd045783f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "MA Thesis Semantics of Names.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}