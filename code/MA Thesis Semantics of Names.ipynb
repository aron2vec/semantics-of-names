{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "22531ca9-e756-46d1-b70a-3e471d5c92a8",
      "metadata": {
        "id": "22531ca9-e756-46d1-b70a-3e471d5c92a8"
      },
      "source": [
        "# Master Thesis on the Semantics of (made-up) Names\n",
        "\n",
        "* Author: Aron Joosse\n",
        "* Supervisor: Giovanni Cassani\n",
        "* Institution: Tilburg University\n",
        "\n",
        "Can take inspiration from: https://github.com/Masetto96/BA-Thesis-form-meaning-mapping/blob/master/form_meaning_mapping.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6549cc30-115e-4d41-a905-85e232d32540",
      "metadata": {
        "id": "6549cc30-115e-4d41-a905-85e232d32540"
      },
      "source": [
        "# Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9e01d4f6-9943-40fb-9f85-ca2aa53b4650",
      "metadata": {
        "id": "9e01d4f6-9943-40fb-9f85-ca2aa53b4650",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18d537d1-2b56-4cee-a9b1-09f59379e5bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3148301 sha256=3e05061880c764968d920da873c224637ebccba6c054c8157cbf35d549acfa85\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.2\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.2.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (653 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hCollecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hRequirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.1.2)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.8)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: typing-extensions, catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "Successfully installed catalogue-2.0.7 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.4 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.15 typer-0.4.1 typing-extensions-3.10.0.2\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 491 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.6)\n",
            "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext --progress-bar off\n",
        "!pip install -U spacy --progress-bar off\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install keras\n",
        "!pip install --upgrade gensim\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Preprocessing\n",
        "import re\n",
        "import os\n",
        "import spacy\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "\n",
        "# FastText\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "\n",
        "# Word2Vec\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "# MEN and SimLex Benchmarks\n",
        "from os import listdir\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ElasticNet and ANN\n",
        "import sklearn\n",
        "from sklearn.model_selection import cross_val_score, RepeatedKFold, train_test_split, StratifiedKFold, KFold\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score, mean_squared_error, median_absolute_error\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import absolute\n",
        "from numpy.random import seed\n",
        "\n",
        "from statistics import median\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f1ffd0-4597-433a-b9c7-bb2423556386",
      "metadata": {
        "id": "92f1ffd0-4597-433a-b9c7-bb2423556386"
      },
      "source": [
        "# Data Import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Getting the list of madeup names:\n",
        "drive.mount(\"/content/drive\", force_remount=True) \n",
        "ratings_csv = pd.read_csv(\"drive/MyDrive/Thesis/Data/giovanni_email_data/avgRatings_annotated.csv\",\n",
        "                          usecols = [\"name\", \"name_type\"])                      ## Importing the names and name types\n",
        "\n",
        "ratings_csv.head(10)\n",
        "\n",
        "madeup_names = []\n",
        "\n",
        "for i in ratings_csv.index:                                                     ## Only choosing madeup names so I can filter them out of the FT vocab\n",
        "  if ratings_csv[\"name_type\"][i] == \"madeup\":\n",
        "    madeup_names.append(str(ratings_csv[\"name\"][i]))\n",
        "\n",
        "madeup_names_lower = list(map(lambda x: x.lower(), madeup_names))               ## Lowercasting the names since my entire vocab will be lowercast\n",
        "\n",
        "print(madeup_names[:5])\n",
        "print(len(madeup_names))\n",
        "print(madeup_names_lower[:5])\n",
        "print(len(madeup_names_lower))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AUEyOhEpf74",
        "outputId": "83caf6fd-40c1-448f-ea98-91a92926501f"
      },
      "id": "0AUEyOhEpf74",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['Alastor', 'Alecto', 'Amabala', 'Araminta', 'Arcturus']\n",
            "60\n",
            "['alastor', 'alecto', 'amabala', 'araminta', 'arcturus']\n",
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"drive/My Drive/Thesis/Data/CoCA/Text/\"                                  ## These are the paths to easily export/import my dicts, txts, models, and pickles\n",
        "dict_path = \"drive/My Drive/Thesis/Data/CoCA/dict_pickles/\"\n",
        "unclean_path = path + \"texts_combined/all_texts_combined.txt\"\n",
        "model_path = \"drive/My Drive/Thesis/Data/CoCA/models/\"\n",
        "pickle_path = \"drive/MyDrive/Thesis/Data/fastText and others/\"\n",
        "norms_path = \"drive/My Drive/Thesis/Data/Norms/\""
      ],
      "metadata": {
        "id": "UwhQ--sVUGHl"
      },
      "id": "UwhQ--sVUGHl",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "34910b3e-c2f9-4d54-983d-e75c2127f4db",
      "metadata": {
        "id": "34910b3e-c2f9-4d54-983d-e75c2127f4db"
      },
      "source": [
        "## COCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf7f61e8-e967-4ab9-895e-6fb974df4fc4",
      "metadata": {
        "id": "bf7f61e8-e967-4ab9-895e-6fb974df4fc4"
      },
      "outputs": [],
      "source": [
        "unclean_corpus = open(unclean_path).read()                                      ## Importing the entire coca"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(unclean_corpus))                                                      ## Showing the length and first 100 characters of the coca\n",
        "print(unclean_corpus[:100])"
      ],
      "metadata": {
        "id": "cuIyN1lBMe05",
        "outputId": "486626ae-0cf2-45ff-d70e-eafae963a54c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cuIyN1lBMe05",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2977527143\n",
            "@@4170367 Headnote # A puzzle has long pervaded the criminal law : why are two offenders who commit \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a35ba177-63c9-43e0-8eb0-cc41ada452d6",
      "metadata": {
        "id": "a35ba177-63c9-43e0-8eb0-cc41ada452d6"
      },
      "source": [
        "## Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "efb0d50d-543f-4483-9b58-83812e4c7418",
      "metadata": {
        "id": "efb0d50d-543f-4483-9b58-83812e4c7418",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c007ff7d-5003-42d6-f957-93ec4f49a11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "119\n",
            "179\n",
            "63\n",
            "       name  rating.mean_age    age name_type\n",
            "0  Adelaide        -0.617647    old      real\n",
            "2  Alasdair        18.709677  young      real\n",
            "3   Alastor        13.812500    old    madeup\n",
            "4    Alecto         3.593750    old    madeup\n",
            "5     Alice       -13.969697  young      real 119\n",
            "       name  rating.mean_gender  gender name_type\n",
            "0  Adelaide           45.727273  female      real\n",
            "1   Adelina           47.771429  female      real\n",
            "2  Alasdair          -35.657143    male      real\n",
            "3   Alastor          -38.833333    male    madeup\n",
            "4    Alecto          -35.722222  female    madeup 179\n",
            "        name  rating.mean_valence polarity name_type\n",
            "1    Adelina            31.621622      bad      real\n",
            "7    Amabala             5.935484     good    madeup\n",
            "8      Apple            32.444444     good   talking\n",
            "11  Arcturus           -11.166667     good    madeup\n",
            "13   Arobynn             7.645161      bad    madeup 63\n"
          ]
        }
      ],
      "source": [
        "### Read CSV File and Delete Unimportant Columns (i.e., everything that isn't the name, name type, rating, or the author's choice)\n",
        "\n",
        "### This is input for the FT model, which itself is the input for the ElasticNet and ANN regressions\n",
        "\n",
        "names_ratings = read_csv(\"drive/MyDrive/Thesis/Data/giovanni_email_data/avgRatings_annotated.csv\")\n",
        "\n",
        "#print(names_ratings.head())\n",
        "\n",
        "print(names_ratings['rating.mean_age'].notna().sum())                           ## Choosing only those rows where all columns are not NA\n",
        "print(names_ratings['rating.mean_gender'].notna().sum())\n",
        "print(names_ratings['rating.mean_valence'].notna().sum())\n",
        "\n",
        "df_age = names_ratings.loc[names_ratings['rating.mean_age'].notna(), ['name', 'rating.mean_age', 'age', 'name_type']]   ## Choosing the relevant columns\n",
        "print(df_age.head(), len(df_age))\n",
        "\n",
        "df_gender = names_ratings.loc[names_ratings['rating.mean_gender'].notna(), ['name', 'rating.mean_gender', 'gender', 'name_type']]\n",
        "print(df_gender.head(), len(df_gender))\n",
        "\n",
        "df_polarity = names_ratings.loc[names_ratings['rating.mean_valence'].notna(), ['name', 'rating.mean_valence', 'polarity', 'name_type']]\n",
        "print(df_polarity.head(), len(df_polarity))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b17c25-4631-4971-8c9e-9b13b9322a08",
      "metadata": {
        "id": "45b17c25-4631-4971-8c9e-9b13b9322a08"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c46490a-70a1-41ae-8f63-ccf7fb08da08",
      "metadata": {
        "id": "6c46490a-70a1-41ae-8f63-ccf7fb08da08"
      },
      "source": [
        "\n",
        "## Cleaning Corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading the English spacy pipeline and removing stopwords (since we are interested in gender bias, it's best to leave these words in)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.max_length = 10000000000\n",
        "\n",
        "nlp.Defaults.stop_words.remove('him')\n",
        "nlp.Defaults.stop_words.remove('her')\n",
        "nlp.Defaults.stop_words.remove('hers')\n",
        "nlp.Defaults.stop_words.remove('his')\n",
        "nlp.Defaults.stop_words.remove('he')\n",
        "nlp.Defaults.stop_words.remove('she')\n",
        "nlp.Defaults.stop_words.remove('himself')\n",
        "nlp.Defaults.stop_words.remove('herself')"
      ],
      "metadata": {
        "id": "xqknf8oNouf3"
      },
      "id": "xqknf8oNouf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_corpus_sentenced(data, corpus_dict, index):\n",
        "  ## Input: \n",
        "  # - Data = A (very) large string of corpus text\n",
        "  # - Corpus_dict = a dictionary to store individual sentences in\n",
        "  # - Index = the last index from the previous batch\n",
        "\n",
        "  ## Process: \n",
        "  # Remove all unwanted tokens, and store individual sentences in the dictionary\n",
        "\n",
        "  ## Output: \n",
        "  # - The dictionary of preprocessed sentences\n",
        "  # - The last sentence index, for the next batch to continue with (so that the order of the sentences is kept)\n",
        "\n",
        "  # Tokenization\n",
        "  with nlp.select_pipes(disable=[\"lemmatizer\", \"tok2vec\", \"tagger\", \"parser\"]):\n",
        "    nlp.enable_pipe(\"senter\")                                                   ## Helps with better segmenting into sentences\n",
        "    doc = nlp(data)\n",
        "\n",
        "  sentence = \"\"                                                                 ## Initialize an empty sentence\n",
        "\n",
        "  for token in doc:\n",
        "    if token.is_sent_start is True:                                             ## If token is the star of the sentence, add the previous sentence to the dictionary\n",
        "      if sentence == \"\":                                                        ## and create a new, clean sentence\n",
        "        continue\n",
        "      else:\n",
        "        corpus_dict[index] = sentence\n",
        "        sentence = \"\"\n",
        "        index += 1\n",
        "    \n",
        "    if token.is_upper is True:                                                  ## Remove all full-caps words\n",
        "      continue\n",
        "    elif token.is_stop is True:                                                 ## Remove all stopwords\n",
        "      continue\n",
        "    elif str(token).lower() in madeup_names_lower:                              ## Remove all words that are in my list of made-up names\n",
        "      continue\n",
        "    elif token.is_alpha:                                                        ## If the token has passed all previous tests, and it consists only of alphabetic\n",
        "      sentence += str(token).lower() + \" \"                                      ## characters, lowercast it and add an extra space to the end; continue to the next\n",
        "                                                                                ## token\n",
        "  return corpus_dict, index"
      ],
      "metadata": {
        "id": "_GxXQaOBOa9N"
      },
      "id": "_GxXQaOBOa9N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def corpus_dict_maker(data, start, end, index):\n",
        "  ## Input: \n",
        "  # - Data = The entire uncleaned corpus\n",
        "  # - Start = The character index to indicate the start of the current batch to preprocess\n",
        "  # - End = The character index to indicate the end of the current batch to preprocess\n",
        "  # - Index = The sentence index from the previous batch, to keep track of the number and order of the sentences\n",
        "\n",
        "  ## Process: \n",
        "  # Preprocess the corpus in batches, since there was not enough RAM to preprocess the entire corpus at once\n",
        "  # So, for every batch, all the characters between the start index and the end index are fed into the clean_corpus_sentenced() function\n",
        "  # and then this dictionary of sentences is saved as a pickle to Google drive\n",
        "\n",
        "  ## Output: \n",
        "  # Nothing; except that the sentence index is printed, which is used as input for the next batch (this was printed, so that it couldn't be lost if the \n",
        "  # runtime would disconnect (which it sadly did very often))\n",
        "\n",
        "  drive.mount(\"/content/drive\", force_remount=True)                             ## Connect to google drive\n",
        "  \n",
        "  corpus_dict = {}                                                              ## Create empty dictionary\n",
        "\n",
        "  prev_i = (start-2)*1000000                                                    ## Start with preprocessing the two million characters before the previous index\n",
        "                                                                                ## since the next range only preprocesses up to but not including the 'end' index\n",
        "\n",
        "  for i in range(start, end, 2):                                                ## In batches of 2 million characters, feed the batch to clean_corpus_sentenced()\n",
        "    print(i)                                                                   \n",
        "    i *= 1000000\n",
        "    corpus_dict, index = clean_corpus_sentenced(data[prev_i:i],\n",
        "                                                corpus_dict,\n",
        "                                                index)\n",
        "    prev_i = i\n",
        "  \n",
        "  if prev_i == 2976000000:                                                      ## Hardcoded; if we get near the end of the corpus, don't preprocess in a batch of\n",
        "    corpus_dict, index = clean_corpus_sentenced(data[prev_i:],                  ## 2 million characters (we would get an out of range error), but rather just \n",
        "                                                corpus_dict,                    ## preprocess the remaining characters, however many that may be\n",
        "                                                index)\n",
        "\n",
        "  print(index)\n",
        "\n",
        "  pickle_out = open(dict_path + \"corpus_dict_until_\" + str(end) + \".pickle\", \"wb\")  ## Save the dictionary as a pickle\n",
        "  pickle.dump(corpus_dict, pickle_out)\n",
        "  pickle_out.close()\n",
        "\n",
        "  drive.flush_and_unmount()                                                     ## Flush the pickle to my drive\n",
        "  print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n"
      ],
      "metadata": {
        "id": "pt5GFTOnoKSQ"
      },
      "id": "pt5GFTOnoKSQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### All of the individual batches:"
      ],
      "metadata": {
        "id": "pFnzewRdf6LG"
      },
      "id": "pFnzewRdf6LG"
    },
    {
      "cell_type": "code",
      "source": [
        "## doing it in batches to \n",
        "## (1) make it possible in terms of time and the Google afk-checker captcha pop-up, and \n",
        "## (2) to not blow out the RAM and have it break down\n",
        "\n",
        "#corpus_dict_maker(unclean_corpus, 2, 500, 0)                   \n",
        "#corpus_dict_maker(unclean_corpus, 500, 640, 3217086)           \n",
        "#corpus_dict_maker(unclean_corpus, 640, 760, 4232218)           \n",
        "#corpus_dict_maker(unclean_corpus, 760, 800, 5439287)           \n",
        "#corpus_dict_maker(unclean_corpus, 800, 900, 5888161)\n",
        "#corpus_dict_maker(unclean_corpus, 900, 980, 7020129)\n",
        "#corpus_dict_maker(unclean_corpus, 980, 1150, 7891661)\n",
        "#corpus_dict_maker(unclean_corpus, 1150, 1200, 9903820) \n",
        "#corpus_dict_maker(unclean_corpus, 1200, 1204, 10502592)"
      ],
      "metadata": {
        "id": "UnBc-d9pjKgx"
      },
      "id": "UnBc-d9pjKgx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (1202)*1000000\n",
        "#i = 1203*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            10547544)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(1203) + \"_post_1204\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################\n",
        "\n",
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (1203)*1000000\n",
        "#i = 1204*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            index + 1)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(1204) + \"_post_1204\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################\n",
        "\n",
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (1204)*1000000\n",
        "#i = 1205*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            10580543)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(1205) + \"_post_1204\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################\n",
        "\n",
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (1205)*1000000\n",
        "#i = 1206*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            index + 1)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(1206) + \"_post_1204\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "057QoMHol9Tb"
      },
      "id": "057QoMHol9Tb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#corpus_dict_maker(unclean_corpus, 1208, 1300, 10611881)\n",
        "#corpus_dict_maker(unclean_corpus, 1300, 1500, 11486498)\n",
        "#corpus_dict_maker(unclean_corpus, 1500, 1750, 13172710)\n",
        "#corpus_dict_maker(unclean_corpus, 1750, 1846, 15332847)\n",
        "#corpus_dict_maker(unclean_corpus, 1846, 1848, 16123425)\n",
        "#corpus_dict_maker(unclean_corpus, 1848, 1850, 16147433)\n",
        "#corpus_dict_maker(unclean_corpus, 1850, 1900, 16172965)\n",
        "#corpus_dict_maker(unclean_corpus, 1900, 1968, 16855832)\n",
        "#corpus_dict_maker(unclean_corpus, 1968, 1970, 17790964)\n",
        "#corpus_dict_maker(unclean_corpus, 1970, 2000, 17819821)\n",
        "#corpus_dict_maker(unclean_corpus, 2000, 2022, 18244076)\n",
        "#corpus_dict_maker(unclean_corpus, 2022, 2024, 18536113)\n",
        "#corpus_dict_maker(unclean_corpus, 2024, 2026, 18558956)\n",
        "#corpus_dict_maker(unclean_corpus, 2026, 2068, 18583534)\n",
        "#corpus_dict_maker(unclean_corpus, 2068, 2070, 19154020)\n",
        "#corpus_dict_maker(unclean_corpus, 2070, 2100, 19179335)\n",
        "#corpus_dict_maker(unclean_corpus, 2100, 2166, 19598984)\n",
        "#corpus_dict_maker(unclean_corpus, 2166, 2168, 20488725)\n",
        "#corpus_dict_maker(unclean_corpus, 2168, 2188, 20524278)"
      ],
      "metadata": {
        "id": "fWVWwqEAmMnb"
      },
      "id": "fWVWwqEAmMnb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (2186)*1000000\n",
        "#i = 2187*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            20779021)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(2187) + \"_post_2188\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################\n",
        "\n",
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (2187)*1000000\n",
        "#i = 2188*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            index + 1)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(2188) + \"_post_2188\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "rzysL5YBbz1M"
      },
      "id": "rzysL5YBbz1M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#corpus_dict_maker(unclean_corpus, 2190, 2200, 20812123)\n",
        "#corpus_dict_maker(unclean_corpus, 2200, 2310, 20945992)\n",
        "#corpus_dict_maker(unclean_corpus, 2310, 2312, 22397914)\n",
        "#corpus_dict_maker(unclean_corpus, 2312, 2400, 22424124)\n",
        "#corpus_dict_maker(unclean_corpus, 2400, 2600, 23465826)\n",
        "#corpus_dict_maker(unclean_corpus, 2600, 2800, 25199888)\n",
        "#corpus_dict_maker(unclean_corpus, 2800, 2977, 26938737)"
      ],
      "metadata": {
        "id": "tyV6gFTWb0VJ"
      },
      "id": "tyV6gFTWb0VJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This block of code opens all of the corpus dict pickles, and combines them together\n",
        "## into one big dictionary: corpus_dict_complete\n",
        "\n",
        "file_number = 1\n",
        "file_list = []\n",
        "for file_name in os.listdir(dict_path):                                         ## Locate all dicts in the folder\n",
        "  with open(dict_path + str(file_name), 'rb') as f:\n",
        "    exec(\"dict_\" + str(file_number) + \" = \" + \"pickle.load(f)\")\n",
        "    file_list.append(\"dict_\" + str(file_number))    \n",
        "    file_number += 1\n",
        "  \n",
        "corpus_dict_complete = {}\n",
        "for file_name in file_list:                                                     \n",
        "  corpus_dict_complete = {**corpus_dict_complete, **globals()[file_name]}       ## Add the contents of the dicts to dict_complete\n",
        "  del globals()[file_name]\n",
        "\n",
        "#print(len(corpus_dict_complete))\n",
        "#del file_number\n",
        "#del file_list\n",
        "\n",
        "#pickle_out = open(dict_path + \"corpus_dict_complete.pickle\", \"wb\")             ## Create a new pickle\n",
        "#pickle.dump(corpus_dict_complete, pickle_out)\n",
        "#del corpus_dict_complete\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()                                                      ## Flush the pickle to my drive\n",
        "#print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "HCY3IGTxeuXN"
      },
      "id": "HCY3IGTxeuXN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This block of code opens the corpus_dict_complete pickle, and loops through the keys by index (from lowest to highest).\n",
        "## So, we loop through every sentence, in order. These are stored in two different .txt files, one where the sentence structure is remained \n",
        "## (i.e., between every sentence, we add a newline character), and one that's unsentenced (i.e., no newline character between sentences).\n",
        "\n",
        "with open(dict_path + \"corpus_dict_complete.pickle\", \"rb\") as d:                ## Open corpus_dict_complete\n",
        "  corpus_dict_complete = pickle.load(d)\n",
        "\n",
        "  with open(path + \"cleaned_sentenced_corpus_complete.txt\", \"w\") as f:          ## Open sentenced corpus .txt file\n",
        "    for key in sorted(corpus_dict_complete):\n",
        "      if len(str(corpus_dict_complete[key]).split()) < 2:                       ## Remove sentences with only 1 word (since there's no 'context' in that case)\n",
        "        continue\n",
        "      else:\n",
        "        if str(corpus_dict_complete[key])[:2] in [\"m \", \"p \", \"s \"]:            ## I have to add this, because based on manual inspection, a significant amount of \n",
        "          f.write(str(corpus_dict_complete[key])[2:] + \"\\n\")                    ## sentences start with just a \"p\", \"m\", or \"s\"\n",
        "        else:\n",
        "          f.write(str(corpus_dict_complete[key]) + \"\\n\")\n",
        "\n",
        "  with open(path + \"cleaned_unsentenced_corpus_complete.txt\", \"w\") as f2:       ## Open unsentenced corpus .txt file\n",
        "    for key in sorted(corpus_dict_complete): \n",
        "      if len(str(corpus_dict_complete[key])) < 4:                               ## Remove sentences with less than 4 characers, since based on visual inspection, I\n",
        "        continue                                                                ## saw that such sentences are mostly nonsense (i.e., not real words)\n",
        "      else:\n",
        "        if str(corpus_dict_complete[key])[:2] in [\"m \", \"p \", \"s \"]:\n",
        "          f2.write(str(corpus_dict_complete[key])[2:])\n",
        "        else:\n",
        "          f2.write(str(corpus_dict_complete[key]))\n",
        "  \n",
        "drive.flush_and_unmount()                                                       ## Flush to drive\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "YkdDjN1AodpH",
        "outputId": "b2b8493b-8584-459f-a74c-6c023c7743f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YkdDjN1AodpH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All changes made in this colab session should now be visible in Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09e790a-e08f-4144-b01e-25af29a8816f",
      "metadata": {
        "id": "a09e790a-e08f-4144-b01e-25af29a8816f"
      },
      "source": [
        "## Training fastText and Validating on MEN and SimLex999 + Training W2V"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fastText"
      ],
      "metadata": {
        "id": "Mk9EPG5rwc0H"
      },
      "id": "Mk9EPG5rwc0H"
    },
    {
      "cell_type": "code",
      "source": [
        "# def fasttext_tuner(data_type):                                                ## I have chosen Skipgram, I won't play around with epochs or learning rate\n",
        "#   for dimensionality in [100, 300]:                                           ## I'm first checking data type and dimensionality, and choosing the most promising combination.\n",
        "#     for window_size in [2, 3, 4, 5, 6, 7]:                                    ## Then I'll check window size and choose the 3 most promising ones\n",
        "#       for min_n in [1, 2, 3]:                                                 ## Then I want to check min_n 2 and 1 to see whether adding n-gram size of 1 makes any sense\n",
        "#         for max_n in [5, 6, 7]:                                               ## Lastly, I will iterate through the 3 * 2 * 3 most promising models, finally choosing the best one and then double checking that with\n",
        "#           model = fasttext.train_unsupervised(input = path + \"cleaned_\" +     ## the 100/300 sentenced/unsentenced options, just to be sure! In total, this means training 4 + 5 + ~16 + 3 = ~30 models instead of 216\n",
        "#                                               data_type + \n",
        "#                                               \"_corpus_complete.txt\",\n",
        "#                                               model = \"skipgram\",\n",
        "#                                               dim = dimensionality, \n",
        "#                                               ws = window_size, \n",
        "#                                               minn = min_n,\n",
        "#                                               maxn = max_n)\n",
        "          \n",
        "#           model.save_model(model_path + data_type + \"_dim\" + str(dimensionality) + \n",
        "#                             \"_ws\" + str(window_size) + \"_minn\" + str(min_n) + \n",
        "#                             \"_maxn\" + str(max_n) + \".bin\")\n",
        "          \n",
        "#           print(data_type + \"_dim\" + str(dimensionality) + \"_ws\" + str(window_size) + \n",
        "#                 \"_minn\" + str(min_n) + \"_maxn\" + str(max_n))\n",
        "\n",
        "#           del model\n",
        "\n",
        "#   drive.flush_and_unmount()\n",
        "#   print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "SVSZbTIchM28"
      },
      "id": "SVSZbTIchM28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa3cc3c-e1a4-489b-b0c6-e25bd045783f",
      "metadata": {
        "id": "aaa3cc3c-e1a4-489b-b0c6-e25bd045783f"
      },
      "outputs": [],
      "source": [
        "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "ft = fasttext.load_model('cc.en.300.bin')\n",
        "ft.save_model(model_path + \"pretrained_model.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fasttext_tuner(data_type, dimensionality = 300, window_size = 5, min_n = 3, max_n = 6):\n",
        "  ## Input: \n",
        "  # - Data Type: Sentenced or unsentenced, to load the corpus type to train model\n",
        "  # - Dimensionality: List of dimensionalities\n",
        "  # - Window size: List of window sizes \n",
        "  # - Min_n: List of minimum n-gram sizes \n",
        "  # - Max-n: List of maximum n-gram sizes \n",
        "\n",
        "  ## Process: \n",
        "  # For every combination of input variables, a fastText model is trained and saved\n",
        "\n",
        "  ## Output:\n",
        "  # Nothing; fastText model is trained and saved\n",
        "\n",
        "  for d in dimensionality:                                           \n",
        "    for ws in window_size:                                    \n",
        "      for minn in min_n:                                                 \n",
        "        for maxn in max_n:\n",
        "          drive.mount(\"/content/drive\", force_remount=True)                     ## Mount drive\n",
        "\n",
        "          model = fasttext.train_unsupervised(input = path + \"cleaned_\" + data_type + \"_corpus_complete_without_p_s_m.txt\",   ## Train model\n",
        "                                              model = \"skipgram\", dim = d, ws = ws, minn = minn, maxn = maxn)\n",
        "\n",
        "          model.save_model(model_path + data_type + \"_dim\" + str(d) + \"_ws\" + str(ws) + \"_minn\" + str(minn) + \"_maxn\" + str(maxn) + \".bin\")   ## Save model\n",
        "\n",
        "          print(data_type + \"_dim\" + str(d) + \"_ws\" + str(ws) + \"_minn\" + str(minn) + \"_maxn\" + str(maxn))\n",
        "\n",
        "          drive.flush_and_unmount()                                             ## Flush model to drive\n",
        "          print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "          del model"
      ],
      "metadata": {
        "id": "QCnCgy1QqXmQ"
      },
      "id": "QCnCgy1QqXmQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Trying out some variations of fastText with Data = sentenced & Dimensionality = 100\n",
        "\n",
        "fasttext_tuner(\"sentenced\", [100], [2], [2], [3])\n",
        "fasttext_tuner(\"sentenced\", [100], [2, 3, 4, 5, 6, 7], [2], [5, 6, 7])\n",
        "fasttext_tuner(\"sentenced\", [100], [5], [3], [6])\n",
        "fasttext_tuner(\"sentenced\", [100], [5], [1], [6])\n",
        "\n",
        "## Trying out some variations of fastText with Data = sentenced & Dimensionality = 300\n",
        "\n",
        "fasttext_tuner(\"sentenced\", [300], [3, 4], [2], [5, 6, 7])\n",
        "fasttext_tuner(\"sentenced\", [300], [5], [2], [5, 6])\n",
        "fasttext_tuner(\"sentenced\", [300], [5], [3], [6])\n",
        "fasttext_tuner(\"sentenced\", [300], [3], [3], [5])\n",
        "fasttext_tuner(\"sentenced\", [300], [3], [2], [4])\n",
        "fasttext_tuner(\"sentenced\", [300], [2, 6, 7], [2], [5])\n",
        "\n",
        "## Trying out some variations of fastText with Data = unsentenced \n",
        "\n",
        "fasttext_tuner(\"unsentenced\", [300], [5], [3], [6])\n",
        "fasttext_tuner(\"unsentenced\", [100], [5], [3], [6])"
      ],
      "metadata": {
        "id": "BEfZcRmNqXfi"
      },
      "id": "BEfZcRmNqXfi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MEN and SimLex Benchmarks"
      ],
      "metadata": {
        "id": "auG91yFzwfxP"
      },
      "id": "auG91yFzwfxP"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity(model, word1, word2):\n",
        "  # Input:\n",
        "  # - Model = the model that is used to get embeddings for word1 and word2\n",
        "  # - Word1 = the first word\n",
        "  # - Word2 = the second word\n",
        "\n",
        "  # Process: \n",
        "  # Given the embeddings for word1 and word2 derived from the model, calculate\n",
        "  # the cosine similarity between these words\n",
        "\n",
        "  # Output:\n",
        "  # The cosine similarity between word1 and word2 given the current model\n",
        "\n",
        "    e1 = model[word1]\n",
        "    e2 = model[word2]\n",
        "    s = cosine_similarity(e1.reshape(1, -1), e2.reshape(1, -1))\n",
        "    return s[0][0]"
      ],
      "metadata": {
        "id": "GVAVzmt0wixs"
      },
      "id": "GVAVzmt0wixs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_correlation_men(norms, model):\n",
        "  # Input:\n",
        "  # - Norms = The MEN similarity scores for word1 and word2\n",
        "  # - Model = the model that will be scored\n",
        "\n",
        "  # Process: \n",
        "  # For every model, note down the true similarity score (as dictated by the MEN\n",
        "  # benchmark) and then estimate the similarity for the current model using the \n",
        "  # compute_similarity() function\n",
        "\n",
        "  # Output:\n",
        "  # The Spearman R between the true and estimated similarities for the current\n",
        "  # model.\n",
        "\n",
        "        true_similarities = []\n",
        "        estimated_similarities = []\n",
        "        for _, row in norms.iterrows():\n",
        "            s = compute_similarity(model, row['w1'], row['w2'])\n",
        "\n",
        "            estimated_similarities.append(s)\n",
        "            true_similarities.append(row['sim'])\n",
        "        \n",
        "        return spearmanr(true_similarities, estimated_similarities)[0]"
      ],
      "metadata": {
        "id": "qRPpzYPdzqvD"
      },
      "id": "qRPpzYPdzqvD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_correlation_simlex(norms, model):\n",
        "  # Input:\n",
        "  # - Norms = The SimLex similarity scores for word1 and word2\n",
        "  # - Model = the model that will be scored\n",
        "\n",
        "  # Process: \n",
        "  # For every model, note down the true similarity score (as dictated by the SimLex\n",
        "  # benchmark) and then estimate the similarity for the current model using the \n",
        "  # compute_similarity() function\n",
        "\n",
        "  # Output:\n",
        "  # The Spearman R between the true and estimated similarities for the current\n",
        "  # model.\n",
        "\n",
        "        true_similarities = []\n",
        "        estimated_similarities = []\n",
        "        for _, row in norms.iterrows():\n",
        "            s = compute_similarity(model, row['word1'], row['word2'])\n",
        "\n",
        "            estimated_similarities.append(s)\n",
        "            true_similarities.append(row['SimLex999'])\n",
        "        \n",
        "        return spearmanr(true_similarities, estimated_similarities)[0]"
      ],
      "metadata": {
        "id": "dcsEKh_KzrcZ"
      },
      "id": "dcsEKh_KzrcZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "men_norms = pd.read_csv(norms_path + 'norms.dev.csv', sep = ' ')                # Read the MEN benchmark CSV file\n",
        "\n",
        "combi_score = {}\n",
        "men_dict = {}\n",
        "\n",
        "for model in os.listdir(model_path):                                            # For every saved model, calculate the scores on the MEN benchmarks\n",
        "  men_dict[str(model)] = compute_correlation_men(men_norms, model)\n",
        "  combi_score[str(model)] = compute_correlation_men(men_norms, model)\n",
        "\n",
        "men_dict['pretrained_model'] = compute_correlation_men(men_norms, ft)           # Calculating the scores for the baseline (pretrained) model\n",
        "combi_score['pretrained_model'] = compute_correlation_men(men_norms, ft)"
      ],
      "metadata": {
        "id": "kjTCodSdt7OY"
      },
      "id": "kjTCodSdt7OY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simlex_norms = pd.read_csv(norms_path + 'SimLex-999.txt', sep = '\\t')           # Read the SimLex benchmark text file\n",
        "\n",
        "simlex_dict = {}\n",
        "\n",
        "for model in os.listdir(model_path):                                            # For every saved model, calculate the scores on the SimLex benchmarks\n",
        "  simlex_dict[str(model)] = compute_correlation_simlex(simlex_norms, model)\n",
        "  combi_score[str(model)] = compute_correlation_simlex(simlex_norms, model)\n",
        "\n",
        "simlex_dict['pretrained_model')] = compute_correlation_simlex(simlex_norms, ft) # Calculating the scores for the baseline (pretrained) model\n",
        "combi_score['pretrained_model'] = compute_correlation_simlex(simlex_norms, ft)"
      ],
      "metadata": {
        "id": "lOuL_V6Kt8zt"
      },
      "id": "lOuL_V6Kt8zt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_men = open(pickle_path + \"men_scores.pickle\", \"wb\")                       # Save the MEN scores as a pickle\n",
        "pickle.dump(men_dict, pickle_men)\n",
        "pickle_men.close()\n",
        "\n",
        "pickle_simlex = open(pickle_path + \"simlex_scores.pickle\", \"wb\")                 # Save the SimLex scores as a pickle\n",
        "pickle.dump(simlex_dict, pickle_simlex)\n",
        "pickle_simlex.close()\n",
        "\n",
        "pickle_combi = open(pickle_path + \"combi_scores.pickle\", \"wb\")                   # Save the combined scores as a pickle\n",
        "pickle.dump(combi_score, pickle_combi)\n",
        "pickle_combi.close()"
      ],
      "metadata": {
        "id": "dVz29qVhzvbh"
      },
      "id": "dVz29qVhzvbh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sorted(men_dict, key = men_dict.get, reverse = True):\n",
        "    print(i, men_dict[i])"
      ],
      "metadata": {
        "id": "an2nkEf9zv2u"
      },
      "id": "an2nkEf9zv2u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sorted(simlex_dict, key = simlex_dict.get, reverse = True):\n",
        "    print(i, simlex_dict[i])"
      ],
      "metadata": {
        "id": "YzNq-FeJz-CS"
      },
      "id": "YzNq-FeJz-CS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sorted(combi_score, key = combi_score.get, reverse = True):\n",
        "    print(i, combi_score[i])"
      ],
      "metadata": {
        "id": "QNqql7wIz-hs"
      },
      "id": "QNqql7wIz-hs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Model"
      ],
      "metadata": {
        "id": "2MhYKRfpPEVJ"
      },
      "id": "2MhYKRfpPEVJ"
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.load_model(\"drive/MyDrive/Thesis/Data/fastText and others/sentenced_dim300_ws2_minn2_maxn5.bin\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e64L1cxtPGmv",
        "outputId": "7a92693f-a56f-4fc8-bb1f-bb8023bb28d1"
      },
      "id": "e64L1cxtPGmv",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec"
      ],
      "metadata": {
        "id": "mlHQorUDmRqv"
      },
      "id": "mlHQorUDmRqv"
    },
    {
      "cell_type": "code",
      "source": [
        "#w2v_model = Word2Vec(corpus_file = path + \"cleaned_sentenced_corpus_complete_without_p_s_m.txt\",\n",
        "#                     sg = 1, window = 2, vector_size = 300, seed = 17042020)\n",
        "\n",
        "#w2v_model.save(model_path + \"word2vec.model\")\n",
        "\n",
        "#w2v_model.save(model_path + \"word2vec.wordvectors\")\n",
        "\n",
        "w2v_vectors = KeyedVectors.load(model_path + \"word2vec.wordvectors\", mmap='r')  # Load back with memory-mapping = read-only, shared across processes."
      ],
      "metadata": {
        "id": "omU4JQh9mUVC"
      },
      "id": "omU4JQh9mUVC",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cosine_similarity(model['dog'].reshape(1, -1), w2v_vectors.wv['dog'].reshape(1, -1)))\n",
        "print(cosine_similarity(model['tree'].reshape(1, -1), w2v_vectors.wv['tree'].reshape(1, -1)))\n",
        "print(cosine_similarity(model['apple'].reshape(1, -1), w2v_vectors.wv['apple'].reshape(1, -1)))"
      ],
      "metadata": {
        "id": "1HxuIvDUDybm",
        "outputId": "89ea817a-3e4e-441b-ba71-445b376ee1ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1HxuIvDUDybm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.03258707]]\n",
            "[[-0.00994198]]\n",
            "[[-0.03611472]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ElasticNet and Neural Network Regressions"
      ],
      "metadata": {
        "id": "caLUM8tC6_BY"
      },
      "id": "caLUM8tC6_BY"
    },
    {
      "cell_type": "code",
      "source": [
        "def fasttext_xifyer(input_data):\n",
        "  # Input:\n",
        "  # - input_data = dataframe containing the names of the characters\n",
        "\n",
        "  # Process:\n",
        "  # Convert the character names to embeddings using FT\n",
        "\n",
        "  # Output:\n",
        "  # - df_output = a dataframe of word embeddings; one embedding per character name\n",
        "\n",
        "  df_output = np.zeros((len(input_data), 300))                                  # Create an array of length = number of input rows & width = 300 (because of ft)\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  for row in input_data.iterrows():                                             # For every row in the input data\n",
        "    index = row[0]\n",
        "    name = row[1][0].lower()                                                    # Lowercast the name\n",
        "    df_output[i] = model[name]                                                  # Compute the fastText score of the name, and add it to the output array\n",
        "    i += 1\n",
        "\n",
        "  return df_output"
      ],
      "metadata": {
        "id": "k7lbNPag7CrC"
      },
      "id": "k7lbNPag7CrC",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fasttext_xifyer_formless(input_data):\n",
        "  # Input & Output are exactly the same as fasttext_xifyer\n",
        "\n",
        "  # Process:\n",
        "  # Similar to fasttext_xifyer, however, if the name has a 'surface' form that \n",
        "  # is already present in the model vocabulary, then we only use the embeddings\n",
        "  # for the subwords. I.e., as compared to fasttext_xifyer, this function does \n",
        "  # NOT consider the 'surface' form embedding for the character names\n",
        "\n",
        "  df_output = np.zeros((len(input_data), 300))                                  # Create an array of length = number of input rows & width = 300 (because of ft)\n",
        "\n",
        "  c = 0\n",
        "\n",
        "  for row in input_data.iterrows():                                             # For every row in the input data\n",
        "    index = row[0]\n",
        "    name = row[1][0].lower()\n",
        "    if name == model.get_subwords(name)[0][0]:                                  # If the surface form of the name is present in the model vocabulary\n",
        "      df_output[c] = model[name] - w2v_vectors.wv[name]                         # Create an array of length = amount of subwords, and width = 300\n",
        "                                    \n",
        "    else:\n",
        "      df_output[c] = model[name]                                                # Else (if no surface name is present in the model vocabulary), just take the embedding of the name as is\n",
        "    c += 1\n",
        "\n",
        "  return df_output"
      ],
      "metadata": {
        "id": "naiHtQXc7R-2"
      },
      "id": "naiHtQXc7R-2",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fasttext_xifyer_formonly(input_data):\n",
        "  # Input & Output are exactly the same as fasttext_xifyer\n",
        "\n",
        "  # Process:\n",
        "  # Similar to fasttext_xifyer, however, if the name has a 'surface' form that \n",
        "  # is already present in the model vocabulary, then we only use the embeddings\n",
        "  # for the subwords. I.e., as compared to fasttext_xifyer, this function does \n",
        "  # NOT consider the 'surface' form embedding for the character names\n",
        "\n",
        "  df_output = np.zeros((len(input_data), 300))                                  # Create an array of length = number of input rows & width = 300 (because of ft)\n",
        "\n",
        "  c = 0\n",
        "\n",
        "  for row in input_data.iterrows():                                             # For every row in the input data\n",
        "    index = row[0]\n",
        "    name = row[1][0].lower()\n",
        "    if name == model.get_subwords(name)[0][0]:                                  # If the surface form of the name is present in the model vocabulary\n",
        "      df_output[c] = w2v_vectors.wv[name]                                                 \n",
        "    else:\n",
        "      df_output[c] = mean_vector                                                \n",
        "    c += 1\n",
        "\n",
        "  return df_output"
      ],
      "metadata": {
        "id": "c-OqBfJn77PS"
      },
      "id": "c-OqBfJn77PS",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_vector = np.zeros((len(model.words), 300))                                 # Create a mean vector array of length = amount of words in the model vocab, and width = 300\n",
        "\n",
        "i = 0\n",
        "\n",
        "for word in model.words:                          \n",
        "  mean_vector[i] = model.get_word_vector(word)                                  # For every word in the vocab, get it's word embedding vector and add it to the mean_vector array\n",
        "  i += 1\n",
        "\n",
        "mean_vector = np.mean(mean_vector, axis = 0)                                    # Take the mean of the array so that we get a mean_vector of length = 1, and width = 300"
      ],
      "metadata": {
        "id": "SLBg426YYSGw"
      },
      "id": "SLBg426YYSGw",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluator(trained_model, x_train, x_test, y_train, y_test, pred_dict, \n",
        "                    test_names, test_name_types):\n",
        "  \n",
        "  y_pred_train = trained_model.predict(x_train)\n",
        "  mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "  \n",
        "\n",
        "  y_pred = trained_model.predict(x_test)\n",
        "  mean_vec_prediction = float(trained_model.predict(mean_vector.reshape(1, -1)))\n",
        "  \n",
        "  mae_test = mean_absolute_error(y_test, y_pred)\n",
        "  mse_test = mean_squared_error(y_test, y_pred)\n",
        "  median_ae_test = median_absolute_error(y_test,y_pred)\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "  \n",
        "  ##### MAE per name type ######################################################\n",
        "\n",
        "  type_dict = {}\n",
        "  type_dict_median = {'madeup' : [], 'real' : [], 'talking' : []}\n",
        "  type_counter = {}\n",
        "  for n, i, j in zip(test_name_types, y_test, x_test):                          # For every name type (i.e., real, talking, and madeup)\n",
        "    if n in type_dict.keys():\n",
        "      type_dict[n] = type_dict[n] + abs(i - trained_model.predict(j.reshape(1, -1)))    # Append the MAE for every name given that type (so that you get a sum of MAEs; one for each name)\n",
        "      type_dict_median[n].append(abs(i - trained_model.predict(j.reshape(1, -1))))\n",
        "      type_counter[n] = type_counter[n] + 1                                     # And count the number of names given that type\n",
        "    else:\n",
        "      type_dict[n] = abs(i - trained_model.predict(j.reshape(1, -1)))\n",
        "      type_dict_median[n].append(abs(i - trained_model.predict(j.reshape(1, -1))))\n",
        "      type_counter[n] = 1\n",
        "\n",
        "  for i in type_dict.keys():\n",
        "    globals()[f\"mae_{i}\"] = float(type_dict[i])/float(type_counter[i])          # Calculate the average MAE per name type: (sum of MAEs for name type / name counter for name type)\n",
        "    globals()[f\"median_ae_{i}\"] = float(median(type_dict_median[i]))\n",
        "\n",
        "  ##### mean_or_form + real&predictions dict maker #############################\n",
        "\n",
        "  mean_or_form_r2_array = np.zeros((len(x_test), 300))\n",
        "\n",
        "  mean_or_form_abs = 0\n",
        "  mean_or_form_abs_median = []\n",
        "  mean_or_form_counter = 0\n",
        "  for n, t, i, j in zip(test_names, test_name_types, y_test, x_test):           # For every name in the test set\n",
        "      n = n.lower()                                                             # Convert name to lowercase\n",
        "      pred_dict[n] = [t, i, float(trained_model.predict(j.reshape(1, -1)))]\n",
        "      pred_dict[n + '_mean_vector'] = [t, i, mean_vec_prediction]\n",
        "      if n == model.get_subwords(n)[0][0]:                                      # If the name is present in the model vocabulary\n",
        "          mean_or_form_abs += abs(i - trained_model.predict(j.reshape(1, -1)))           # Retrieve the MAE of the name using the model embedding, add it to the MAE summation variable\n",
        "          mean_or_form_abs_median.append(abs(i - trained_model.predict(j.reshape(1, -1))))\n",
        "          mean_or_form_r2_array[mean_or_form_counter] = model[n]                # and add the embedding to the R2 array\n",
        "          mean_or_form_counter += 1\n",
        "      else:                                                                     # If it isn't present in the model vocabulary\n",
        "          mean_or_form_abs += abs(i - trained_model.predict(mean_vector.reshape(1, -1))) # Take the mean vector and retrieve the MAE, add it to the MAE summation variable\n",
        "          mean_or_form_abs_median.append(abs(i - trained_model.predict(mean_vector.reshape(1, -1))))\n",
        "          mean_or_form_r2_array[mean_or_form_counter] = mean_vector             # and add the embedding to the R2 array\n",
        "          mean_or_form_counter += 1\n",
        "\n",
        "  mean_or_form_mae = float(mean_or_form_abs / mean_or_form_counter)             # Calculate the average MAE by taking the summation variable and dividing it by the counter variable  \n",
        "  mean_or_form_median_ae = float(median(mean_or_form_abs_median))\n",
        "\n",
        "  mean_or_form_r2 = r2_score(y_test, trained_model.predict(mean_or_form_r2_array))\n",
        " \n",
        "  ##### Mean Only ##############################################################\n",
        "\n",
        "  mean_vec_array = np.full((len(x_test), 300), mean_vector)                     # Create a mean vector array with length = test_set_length, and width = 300\n",
        "\n",
        "  mean_vec_mae_test = mean_absolute_error(y_test, trained_model.predict(mean_vec_array))# Retrieve the MAE for the mean vector array\n",
        "  mean_vec_mse_test = mean_squared_error(y_test, trained_model.predict(mean_vec_array)) # Retrieve the MSE for the mean vector array\n",
        "  mean_vec_median_ae_test = median_absolute_error(y_test, trained_model.predict(mean_vec_array))\n",
        "\n",
        "  mean_vec_r2 = r2_score(y_test, trained_model.predict(mean_vec_array))         # Retrieve the R2 for the mean_vector array\n",
        "\n",
        "  return mse_train, mae_test, mse_test, median_ae_test, r2, mae_madeup, \\\n",
        "  mae_real, mae_talking, median_ae_madeup, median_ae_real, median_ae_talking, \\\n",
        "  mean_or_form_mae, mean_or_form_median_ae, mean_or_form_r2, mean_vec_mae_test, \\\n",
        "  mean_vec_mse_test, mean_vec_median_ae_test, mean_vec_r2"
      ],
      "metadata": {
        "id": "TkYmoAHHhkMn"
      },
      "id": "TkYmoAHHhkMn",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ElasticNet"
      ],
      "metadata": {
        "id": "npGJDFp77bHD"
      },
      "id": "npGJDFp77bHD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Functions"
      ],
      "metadata": {
        "id": "ggMAemsMSobu"
      },
      "id": "ggMAemsMSobu"
    },
    {
      "cell_type": "code",
      "source": [
        "def elasticnetifyer(x_train, y_train, x_test, y_test, pred_dict, test_names, test_name_types):\n",
        "  # Input:\n",
        "  # - x_train = array of embeddings used to train the model\n",
        "  # - y_train = array of ratings used to train the model\n",
        "  # - x_test = array of embeddings used to test the model\n",
        "  # - y_test = array of ratings used to test the model\n",
        "  # - pred_dict = a dictionary that will be filled with predictions per name\n",
        "  # - test_names = dataframe containing the full names (i.e., not the embeddings) \n",
        "  # - test_name_types = dataframe containing the name type (real, madeup, talking)\n",
        "\n",
        "  # Process:\n",
        "  # Given x_train and y_train, train the ElasticNet CV model. When it converges,\n",
        "  # calculate the MSE, MAE, and R2 for a bunch of different settings (test set,\n",
        "  # name types, mean_vector-only model, mean_vector_or_surface_form model)\n",
        "\n",
        "  # Output:\n",
        "  # --> Too big to all individually explain here. The ouput contains the alpha, \n",
        "  # l1_ratio, n_iters, intercept, and MSE of the trained model.\n",
        "  # Furthermore, it outputs the MAE, MSE, and R2 of the test set, as well as the\n",
        "  # MAE for the three different name types, the MAE for a mean_vector_or_surface\n",
        "  # _form-model, and the MAE and R2 for a mean_vector-only model.  \n",
        "\n",
        "  regr = ElasticNetCV(l1_ratio = [0.01, 0.05, .1, 0.2, .5, .7, .9, .95, .99, 1], \n",
        "                      n_alphas = 250,\n",
        "                      max_iter = 10000,\n",
        "                      cv = len(x_train),\n",
        "                      selection = 'random', \n",
        "                      random_state=17042020,)                                   # Set the hyperparameters of the ElasticNet CV\n",
        "\n",
        "  regr.fit(x_train, y_train)                                                    # Fit the model on the train set\n",
        "\n",
        "  alpha = regr.alpha_                                                           # Retrieve the alpha\n",
        "  l1_ratio = regr.l1_ratio_                                                     # Retrieve the L1-ratio\n",
        "  n_iters = regr.n_iter_                                                        # Retrieve the number of iterations needed to train the model\n",
        "  intercept = regr.intercept_                                                   # Retrieve the intercept of the model\n",
        "\n",
        "  mse_train, mae_test, mse_test, median_ae_test, r2, mae_madeup, mae_real, \\\n",
        "  mae_talking, median_ae_madeup, median_ae_real, median_ae_talking, \\\n",
        "  mean_or_form_mae, mean_or_form_median_ae, mean_or_form_r2, mean_vec_mae_test, \\\n",
        "  mean_vec_mse_test, mean_vec_median_ae_test, mean_vec_r2 = \\\n",
        "  model_evaluator(regr, x_train, x_test, y_train, y_test, \n",
        "                  pred_dict, test_names, test_name_types)\n",
        "\n",
        "  return alpha, l1_ratio, n_iters, intercept, mse_train, mae_test, mse_test, \\\n",
        "  median_ae_test, r2, mae_madeup, mae_real, mae_talking, median_ae_madeup, \\\n",
        "  median_ae_real, median_ae_talking, mean_or_form_mae, mean_or_form_median_ae, \\\n",
        "  mean_or_form_r2, mean_vec_mae_test, mean_vec_mse_test, mean_vec_median_ae_test, \\\n",
        "  mean_vec_r2"
      ],
      "metadata": {
        "id": "MnIN74N27cu2"
      },
      "id": "MnIN74N27cu2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nested_cross_validator(df, rating, dictionary):\n",
        "  # Input:\n",
        "  # - df = a dataframe with the name, name_type, and rating for the dimension at  \n",
        "  # hand (i.e., age, gender, or polarity)\n",
        "  # - rating = a string indicating what rating to extract from the df\n",
        "  # - dictionary = an empty dictionary to store all of the variables from \n",
        "  # elasticnetifyer() in\n",
        "\n",
        "  # Process:\n",
        "  # Given the df, get 5 train/test splits, and per fold, train a LOOCV ElasticNet\n",
        "  # model using elasticnetifyer() for both the regular and formless data. \n",
        "  # Then, average the scores retrieved on the different folds, and print it.\n",
        "\n",
        "  # Output: \n",
        "  # - pred_dict_regular: y_true and y_pred per name (regular)\n",
        "  # - pred_dict_formless: y_true and y_pred per name (formless)\n",
        "  #\n",
        "  # Furthermore, the results for all of the variables output by elasticnetifyer()\n",
        "  # are averaged (so that we get a single score, combining scores of all of the \n",
        "  # different folds). These scores are subsequently printed.\n",
        "  \n",
        "  pred_dict_regular = {}\n",
        "  pred_dict_formless = {}\n",
        "  pred_dict_formonly = {}\n",
        "\n",
        "  skf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=17042020)      # Set up a stratified 5-fold splitter\n",
        "\n",
        "  for train_index, test_index in skf.split(df, df[['name_type']]):              # For every fold (stratified on the name type, i.e., real, madeup, or talking)\n",
        "    \n",
        "    x_train_unfasttexted = df.iloc[train_index]                                 # Split the data into x_train and x_test\n",
        "    x_test_unfasttexted = df.iloc[test_index]\n",
        "\n",
        "\n",
        "    y_train = df.iloc[train_index][rating]                                      # Split the data into y_train and y_test\n",
        "    y_test = df.iloc[test_index][rating]\n",
        "    \n",
        "    test_names = df.iloc[test_index]['name']\n",
        "    test_name_types = df.iloc[test_index]['name_type']\n",
        "\n",
        "    x_train = fasttext_xifyer(x_train_unfasttexted)                             # Get the regular embeddings (train)\n",
        "    x_train_formless = fasttext_xifyer_formless(x_train_unfasttexted)           # And get the surface_form-less embeddings (train)\n",
        "    x_train_formonly = fasttext_xifyer_formonly(x_train_unfasttexted)\n",
        "\n",
        "    x_test = fasttext_xifyer(x_test_unfasttexted)                               # Get the regular embeddings (test)\n",
        "    x_test_formless = fasttext_xifyer_formless(x_test_unfasttexted)             # And get the surface_form-less embeddings (test)\n",
        "    x_test_formonly = fasttext_xifyer_formonly(x_test_unfasttexted)\n",
        "\n",
        "\n",
        "    alpha, l1_ratio, n_iters, intercept, mse_train, mae_test, mse_test, \\\n",
        "    median_ae_test, r2, mae_madeup, mae_real, mae_talking, median_ae_madeup, \\\n",
        "    median_ae_real, median_ae_talking, mean_or_form_mae, mean_or_form_median_ae, \\\n",
        "    mean_or_form_r2, mean_vec_mae_test, mean_vec_mse_test, mean_vec_median_ae_test, \\\n",
        "    mean_vec_r2 = elasticnetifyer(x_train, y_train, x_test, y_test, \n",
        "                                  pred_dict_regular, test_names, test_name_types)  # Train the ElasticNetCV given the fold, and return all of the variables of interest (regular)\n",
        "    \n",
        "    dictionary['regular'].append([alpha, l1_ratio, n_iters, intercept, mse_train, \n",
        "                                  mae_test, mse_test, median_ae_test, r2, \n",
        "                                  mae_madeup, mae_real, mae_talking, median_ae_madeup,\n",
        "                                  median_ae_real, median_ae_talking, mean_or_form_mae, \n",
        "                                  mean_or_form_median_ae, mean_or_form_r2, \n",
        "                                  mean_vec_mae_test, mean_vec_mse_test, \n",
        "                                  mean_vec_median_ae_test, mean_vec_r2])                                 # Append the variables of interest to the dictionary\n",
        "    \n",
        "    alpha_formless, l1_ratio_formless, n_iters_formless, intercept_formless, \\\n",
        "    mse_train_formless, mae_test_formless, mse_test_formless, \\\n",
        "    median_ae_test_formless, r2_formless, mae_madeup_formless, mae_real_formless, \\\n",
        "    mae_talking_formless, median_ae_madeup_formless, median_ae_real_formless, \\\n",
        "    median_ae_talking_formless, mean_or_form_mae_formless, mean_or_form_median_ae_formless, \\\n",
        "    mean_or_form_r2_formless, mean_vec_mae_test_formless, mean_vec_mse_test_formless, \\\n",
        "    mean_vec_median_ae_test_formless, mean_vec_r2_formless \\\n",
        "    = elasticnetifyer(x_train_formless, y_train, x_test_formless, y_test, \n",
        "                      pred_dict_formless, test_names, test_name_types)          # Train the ElasticNetCV given the fold, and return all of the variables of interest (formless)\n",
        "\n",
        "    dictionary['formless'].append([alpha_formless, l1_ratio_formless, n_iters_formless,\n",
        "                                   intercept_formless, mse_train_formless, \n",
        "                                   mae_test_formless, mse_test_formless, \n",
        "                                   median_ae_test_formless, r2_formless,\n",
        "                                   mae_madeup_formless, mae_real_formless, mae_talking_formless,\n",
        "                                   median_ae_madeup_formless, median_ae_real_formless,\n",
        "                                   median_ae_talking_formless, mean_or_form_mae_formless, \n",
        "                                   mean_or_form_median_ae_formless, mean_or_form_r2_formless, \n",
        "                                   mean_vec_mae_test_formless, mean_vec_mse_test_formless,\n",
        "                                   mean_vec_median_ae_test_formless, mean_vec_r2_formless])   # Append the variables of interest to the dictionary\n",
        "\n",
        "    alpha_formonly, l1_ratio_formonly, n_iters_formonly, intercept_formonly, \\\n",
        "    mse_train_formonly, mae_test_formonly, mse_test_formonly, \\\n",
        "    median_ae_test_formonly, r2_formonly, mae_madeup_formonly, mae_real_formonly, \\\n",
        "    mae_talking_formonly, median_ae_madeup_formonly, median_ae_real_formonly, \\\n",
        "    median_ae_talking_formonly, mean_or_form_mae_formonly, mean_or_form_median_ae_formonly, \\\n",
        "    mean_or_form_r2_formonly, mean_vec_mae_test_formonly, mean_vec_mse_test_formonly, \\\n",
        "    mean_vec_median_ae_test_formonly, mean_vec_r2_formonly \\\n",
        "    = elasticnetifyer(x_train_formonly, y_train, x_test_formonly, y_test, \n",
        "                      pred_dict_formonly, test_names, test_name_types)          \n",
        "\n",
        "    dictionary['form_only'].append([alpha_formonly, l1_ratio_formonly, n_iters_formonly,\n",
        "                                   intercept_formonly, mse_train_formonly, \n",
        "                                   mae_test_formonly, mse_test_formonly, \n",
        "                                   median_ae_test_formonly, r2_formonly,\n",
        "                                   mae_madeup_formonly, mae_real_formonly, mae_talking_formonly,\n",
        "                                   median_ae_madeup_formonly, median_ae_real_formonly,\n",
        "                                   median_ae_talking_formonly, mean_or_form_mae_formonly, \n",
        "                                   mean_or_form_median_ae_formonly, mean_or_form_r2_formonly, \n",
        "                                   mean_vec_mae_test_formonly, mean_vec_mse_test_formonly,\n",
        "                                   mean_vec_median_ae_test_formonly, mean_vec_r2_formonly])\n",
        "  \n",
        "  return pred_dict_regular, pred_dict_formless, pred_dict_formonly"
      ],
      "metadata": {
        "id": "xG2I1J0m7j9z"
      },
      "id": "xG2I1J0m7j9z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nested_cv_addotron(dictionary, name):\n",
        "  # Input:\n",
        "  # - dictionary = the dictionary of values for the dimension of interest\n",
        "  # - name = string indicating the dimension of interest (i.e., age, gender, or \n",
        "  # polarity). This is used to make printing easier and more organised.\n",
        "\n",
        "  # Process:\n",
        "  # Given the dimension of interest, for all 5 folds, average the scores and \n",
        "  # print all of the variables for both the 'regular' and 'formless' models\n",
        "\n",
        "  variable_list = ('alpha', 'l1_ratio', 'n_iters', 'intercept', 'mse_train',    # List indicating all of the variables of interest\n",
        "                   'mae_test', 'mse_test', 'median_ae_test', 'r2', 'mae_madeup', \n",
        "                   'mae_real', 'mae_talking', 'median_ae_madeup', 'median_ae_real',\n",
        "                   'median_ae_talking', 'mean_or_form_mae', 'mean_or_form_median_ae',\n",
        "                   'mean_or_form_r2', 'mean_vec_mae_test', 'mean_vec_mse_test', \n",
        "                   'mean_vec_median_ae_test', 'mean_vec_r2')\n",
        "  \n",
        "  type_list = ['regular', 'formless', 'form_only']                                           # List indicating the model type\n",
        "\n",
        "  regular_list = dictionary['regular']\n",
        "  formless_list = dictionary['formless']\n",
        "  formonly_list = dictionary['form_only']\n",
        "  \n",
        "  regular_list = [sum(x) for x in zip(*regular_list)]                           # For all of the variables in the list, sum the scores (regular)\n",
        "  regular_list = [x / 5 for x in regular_list]                                  # For all of the variables in the list, divide the sum by 5 to get the average score (regular)\n",
        "\n",
        "  formless_list = [sum(x) for x in zip(*formless_list)]                         # For all of the variables in the list, sum the scores (formless)\n",
        "  formless_list = [x / 5 for x in formless_list]                                # For all of the variables in the list, divide the sum by 5 to get the average score (formless)\n",
        "\n",
        "  formonly_list = [sum(x) for x in zip(*formonly_list)]\n",
        "  formonly_list = [x / 5 for x in formonly_list]\n",
        "\n",
        "  for value_list, analysis_type in zip([regular_list, formless_list, \n",
        "                                        formonly_list], type_list):\n",
        "    for value, variable in zip(value_list, variable_list):\n",
        "      print(f\"Average {analysis_type} {variable} for {name} = {value}\")         # Per variable, and per analysis type, print the corresponding average value\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "tPKBvQzk8anF"
      },
      "id": "tPKBvQzk8anF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Running the functions"
      ],
      "metadata": {
        "id": "DwJF9u5xStc_"
      },
      "id": "DwJF9u5xStc_"
    },
    {
      "cell_type": "code",
      "source": [
        "age_dict = {'regular' : [], 'formless' : [], 'form_only' : []}                                    # initialize the score dictionary for age\n",
        "\n",
        "age_pred_dict_regular, age_pred_dict_formless, age_pred_dict_formonly = \\\n",
        "nested_cross_validator(df_age, 'rating.mean_age', age_dict)                     # Perform the 5-fold cross validation\n",
        "\n",
        "pickle_age_ncv = open(pickle_path + \"age_ncv.pickle\", \"wb\")                     # Save the dictionary to a pickle\n",
        "pickle.dump(age_dict, pickle_age_ncv)\n",
        "pickle_age_ncv.close()\n",
        "\n",
        "pickle_age_preds_regular = open(pickle_path + \"age_preds_regular.pickle\", \"wb\") # Save the regular predictions dictionary to a pickle\n",
        "pickle.dump(age_pred_dict_regular, pickle_age_preds_regular)\n",
        "pickle_age_preds_regular.close()\n",
        "\n",
        "pickle_age_preds_formless = open(pickle_path + \"age_preds_formless.pickle\", \"wb\") # Save the formless predictions dictionary to a pickle\n",
        "pickle.dump(age_pred_dict_formless, pickle_age_preds_formless)\n",
        "pickle_age_preds_formless.close()\n",
        "\n",
        "pickle_age_preds_formonly = open(pickle_path + \"age_preds_formonly.pickle\", \"wb\") # Save the formless predictions dictionary to a pickle\n",
        "pickle.dump(age_pred_dict_formonly, pickle_age_preds_formonly)\n",
        "pickle_age_preds_formonly.close()\n",
        "\n",
        "\n",
        "nested_cv_addotron(age_dict, \"age\")                                             # Print all of the values in an orderly fashion"
      ],
      "metadata": {
        "id": "wQyTwkIf7loU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "7c194788-4341-419b-de3e-a8739516a3bf"
      },
      "id": "wQyTwkIf7loU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e803bb2dbc7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mage_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'regular'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'formless'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'form_only'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m                                    \u001b[0;31m# initialize the score dictionary for age\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mage_pred_dict_regular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_pred_dict_formless\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_pred_dict_formonly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_cross_validator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_age\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rating.mean_age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_dict\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# Perform the 5-fold cross validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpickle_age_ncv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"age_ncv.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# Save the dictionary to a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-2643371f5aa2>\u001b[0m in \u001b[0;36mnested_cross_validator\u001b[0;34m(df, rating, dictionary)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     alpha_formonly, l1_ratio_formonly, n_iters_formonly, intercept_formonly,     mse_train_formonly, mae_test_formonly, mse_test_formonly,     median_ae_test_formonly, r2_formonly, mae_madeup_formonly, mae_real_formonly,     mae_talking_formonly, median_ae_madeup_formonly, median_ae_real_formonly,     median_ae_talking_formonly, mean_or_form_mae_formonly, mean_or_form_median_ae_formonly,     mean_or_form_r2_formonly, mean_vec_mae_test_formonly, mean_vec_mse_test_formonly,     mean_vec_median_ae_test_formonly, mean_vec_r2_formonly     = elasticnetifyer(x_train_formonly, y_train, x_test_formonly, y_test, \n\u001b[0;32m---> 75\u001b[0;31m                       pred_dict_formonly, test_names, test_name_types)          \n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     dictionary['form_only'].append([alpha_formonly, l1_ratio_formonly, n_iters_formonly,\n",
            "\u001b[0;32m<ipython-input-3-f1796ac81b00>\u001b[0m in \u001b[0;36melasticnetifyer\u001b[0;34m(x_train, y_train, x_test, y_test, pred_dict, test_names, test_name_types)\u001b[0m\n\u001b[1;32m     28\u001b[0m                       random_state=17042020,)                                   # Set the hyperparameters of the ElasticNet CV\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m                                                    \u001b[0;31m# Fit the model on the train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m                                                           \u001b[0;31m# Retrieve the alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1672\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"threads\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         )(jobs)\n\u001b[0m\u001b[1;32m   1675\u001b[0m         \u001b[0mmse_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_l1_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0;31m# The mean is computed over folds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36m_path_residuals\u001b[0;34m(X, y, sample_weight, train, test, normalize, fit_intercept, path, path_params, alphas, l1_ratio, X_order, dtype)\u001b[0m\n\u001b[1;32m   1400\u001b[0m     \u001b[0;31m# X is copied and a reference is kept here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpath_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36menet_path\u001b[0;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mprecompute\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             model = cd_fast.enet_coordinate_descent(\n\u001b[0;32m--> 648\u001b[0;31m                 \u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m             )\n\u001b[1;32m    650\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msklearn/linear_model/_cd_fast.pyx\u001b[0m in \u001b[0;36msklearn.linear_model._cd_fast.enet_coordinate_descent\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     46\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[1;32m     47\u001b[0m          initial=_NoValue, where=True):\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gender_dict = {'regular' : [], 'formless' : [], 'form_only' : []}                                 # initialize the score dictionary for gender\n",
        "\n",
        "gender_pred_dict_regular, gender_pred_dict_formless, \\\n",
        "gender_pred_dict_formonly = \\\n",
        "nested_cross_validator(df_gender, 'rating.mean_gender', gender_dict)            # Perform the 5-fold cross validation\n",
        "\n",
        "pickle_gender_ncv = open(pickle_path + \"gender_ncv.pickle\", \"wb\")               # Save the dictionary to a pickle\n",
        "pickle.dump(gender_dict, pickle_gender_ncv)\n",
        "pickle_gender_ncv.close()\n",
        "\n",
        "pickle_gender_preds_regular = open(pickle_path + \"gender_preds_regular.pickle\", \"wb\")    # Save the regular predictions dictionary to a pickle\n",
        "pickle.dump(gender_pred_dict_regular, pickle_gender_preds_regular)\n",
        "pickle_gender_preds_regular.close()\n",
        "\n",
        "pickle_gender_preds_formless = open(pickle_path + \"gender_preds_formless.pickle\", \"wb\")  # Save the formless predictions dictionary to a pickle\n",
        "pickle.dump(gender_pred_dict_formless, pickle_gender_preds_formless)\n",
        "pickle_gender_preds_formless.close()\n",
        "\n",
        "pickle_gender_preds_formonly = open(pickle_path + \"gender_preds_formonly.pickle\", \"wb\")  # Save the formless predictions dictionary to a pickle\n",
        "pickle.dump(gender_pred_dict_formonly, pickle_gender_preds_formonly)\n",
        "pickle_gender_preds_formonly.close()\n",
        "\n",
        "nested_cv_addotron(gender_dict, \"gender\")                                       # Print all of the values in an orderly fashion"
      ],
      "metadata": {
        "id": "zGTfReHa7sVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1477fce7-1b62-40ac-86f9-e82ecd0f97af"
      },
      "id": "zGTfReHa7sVd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average regular alpha for gender = 0.15315895481366898\n",
            "Average regular l1_ratio for gender = 0.9299999999999999\n",
            "Average regular n_iters for gender = 39.8\n",
            "Average regular intercept for gender = -8.20463045373086\n",
            "Average regular mse_train for gender = 905.1063392504739\n",
            "Average regular mae_test for gender = 19.233139844182546\n",
            "Average regular mse_test for gender = 551.8804822794039\n",
            "Average regular r2 for gender = 0.5898452840428152\n",
            "Average regular mae_madeup for gender = 23.48531610952016\n",
            "Average regular mae_real for gender = 15.995646847388306\n",
            "Average regular mae_talking for gender = 18.15510298702322\n",
            "Average regular mean_or_form_mae for gender = 21.051897181265367\n",
            "Average regular mean_or_form_r2 for gender = 0.514324362099883\n",
            "Average regular mean_vec_mae_test for gender = 33.65809119694448\n",
            "Average regular mean_vec_mse_test for gender = 1423.0961711118614\n",
            "Average regular mean_vec_r2 for gender = -0.06793078160867369\n",
            "\n",
            "\n",
            "Average formless alpha for gender = 0.17331042386763676\n",
            "Average formless l1_ratio for gender = 0.7000000000000001\n",
            "Average formless n_iters for gender = 33.4\n",
            "Average formless intercept for gender = -17.2785000815558\n",
            "Average formless mse_train for gender = 1474.022512442651\n",
            "Average formless mae_test for gender = 27.843664495238777\n",
            "Average formless mse_test for gender = 1040.607381058971\n",
            "Average formless r2 for gender = 0.21768454955900682\n",
            "Average formless mae_madeup for gender = 25.428880963738322\n",
            "Average formless mae_real for gender = 33.41050191150029\n",
            "Average formless mae_talking for gender = 24.87238057484024\n",
            "Average formless mean_or_form_mae for gender = 29.237809905005054\n",
            "Average formless mean_or_form_r2 for gender = 0.40462827733486717\n",
            "Average formless mean_vec_mae_test for gender = 33.6849720219342\n",
            "Average formless mean_vec_mse_test for gender = 1383.615664957752\n",
            "Average formless mean_vec_r2 for gender = -0.03787072774003666\n",
            "\n",
            "\n",
            "{'amabala': ['madeup', 38.6388888888889, 22.083573962531396], 'apple': ['talking', 34.2105263157895, 5.34389540466475], 'ball': ['talking', -32.8684210526316, -39.51318075960249], 'blackberry': ['talking', 22.2972972972973, 9.201611993870326], 'blue': ['talking', -9.66666666666667, 9.532585745159459], 'cardan': ['madeup', -29.53125, 3.9390744837970457], 'chloe': ['real', 48.5, 27.747823988560558], 'derrick': ['real', -49.1351351351351, -23.75355349079355], 'dumpa': ['madeup', -12.6571428571429, -1.4516008246711127], 'enid': ['real', 28.6486486486486, 19.991948919897393], 'ethel': ['real', 46.8333333333333, 24.896345635000806], 'flora': ['talking', 46.972972972973, 39.645393090170586], 'fortuna': ['madeup', 42.7777777777778, -8.697860978103249], 'ginny': ['real', 44.1891891891892, 6.134721807461628], 'gmork': ['madeup', -37.2972972972973, -7.187913293683121], 'goha': ['madeup', -1.4375, -1.491311122951016], 'hepzibah': ['madeup', -0.263157894736842, -0.1390667759720179], 'hope': ['talking', 46.4864864864865, 7.427588099550703], 'inej': ['madeup', 4.63636363636364, -11.619604682980219], 'jackie': ['real', 31.9117647058824, 41.037731764217554], 'lark': ['talking', -14.4838709677419, 13.234041433759415], 'marion': ['real', 10.7777777777778, 20.561661016068157], 'mildred': ['real', 46.2702702702703, 34.05773188668752], 'muffet': ['madeup', 22.2222222222222, -9.097213480560685], 'mundungus': ['madeup', -43.3428571428571, -13.927544682279049], 'natasha': ['real', 48.5135135135135, 41.13399692785826], 'peter': ['real', -49.3888888888889, -35.735137988101876], 'roach': ['talking', -40.7567567567568, -42.71670040830028], 'schaffa': ['madeup', 17.1176470588235, -15.51177054128599], 'scullery': ['talking', -33.2972972972973, 4.88887534418877], 'simon': ['real', -47.5833333333333, -30.1763786693705], 'sway': ['talking', -11.5675675675676, -13.493501262302539], 'tansy': ['talking', 37.0, 19.527197617779827], 'tasha': ['madeup', 47.4054054054054, 32.05945331705385], 'thresh': ['talking', -32.5675675675676, -33.62368230930514], 'william': ['real', -49.4594594594595, -37.22461617569478], 'adelina': ['real', 47.7714285714286, 44.86092585926534], 'alastor': ['madeup', -38.8333333333333, -10.431470304837788], 'argus': ['madeup', -40.5833333333333, -15.479687591519367], 'claire': ['real', 49.1142857142857, 32.59134200727384], 'clove': ['talking', 20.6111111111111, -6.974378120996474], 'damon': ['real', -47.1428571428571, -38.314469976458795], 'day': ['talking', 23.3333333333333, 13.054843061033552], 'elide': ['madeup', 29.2777777777778, -23.17368793447656], 'fenrir': ['madeup', -33.4864864864865, -28.420504528924805], 'fox': ['talking', -3.58064516129032, -9.06736647769095], 'fred': ['real', -46.5555555555556, -22.18402486178352], 'freddie': ['real', -38.4054054054054, -17.6676881629308], 'george': ['real', -48.5384615384615, -35.54311850561648], 'gerard': ['real', -45.6857142857143, -29.414182734428653], 'goth': ['talking', -16.25, -9.923196427627344], 'guy': ['talking', -48.1621621621622, -62.48972341354835], 'joseph': ['real', -48.0263157894737, -17.18781070400435], 'kaisa': ['madeup', 40.6944444444444, 64.27694759456227], 'kaz': ['madeup', -21.3939393939394, -20.08680939652261], 'klaus': ['real', -47.1714285714286, -35.17747417812841], 'lyra': ['real', 44.3783783783784, 49.51151543585997], 'mina': ['real', 42.6388888888889, 56.74661695124312], 'minna': ['madeup', 41.8888888888889, 21.658091510791124], 'miskouri': ['madeup', 8.6875, -8.884591438363442], 'mogget': ['madeup', -19.2571428571429, -8.845566742583749], 'nergui': ['madeup', -10.2258064516129, -5.105581379181], 'nitasha': ['madeup', 45.8333333333333, 18.102985106364773], 'parrish': ['talking', -19.4857142857143, -33.99445585263145], 'pearl': ['talking', 47.5714285714286, 37.42485492417072], 'ralph': ['real', -47.8055555555556, -32.16986877904999], 'spark': ['talking', -22.0, -21.035318986533014], 'touchstone': ['talking', -24.2727272727273, 2.7744889224270075], 'violet': ['talking', 45.1111111111111, 50.31073153130754], 'wolf': ['talking', -46.2162162162162, 0.8246477370146259], 'yak': ['talking', -37.7222222222222, -44.89730026885799], 'yozadah': ['madeup', -15.5, 13.635506097232692], 'alice': ['real', 49.1351351351351, 35.14259058318018], 'allie': ['real', 46.2285714285714, 59.17540015949028], 'april': ['talking', 48.3714285714286, -6.462342157950151], 'berry': ['talking', -13.1621621621622, 27.798394170610127], 'chaol': ['madeup', -16.8611111111111, -0.4386394741482773], 'cooki': ['madeup', 36.7297297297297, 50.81494469807483], 'coot': ['talking', -28.9473684210526, -3.361691705544321], 'ditto': ['talking', -21.2702702702703, 1.2409938586288765], 'elena': ['real', 48.5277777777778, 53.29954782145558], 'farder': ['madeup', -40.8918918918919, -28.71494423128139], 'gale': ['talking', 29.6052631578947, -15.600172184248397], 'glimmer': ['talking', 38.4324324324324, 35.76470597818027], 'griphook': ['madeup', -39.8378378378378, 5.858488555470045], 'harold': ['real', -48.3888888888889, -57.916897403921524], 'henry': ['real', -48.8157894736842, -37.499606436152284], 'ian': ['real', -49.2121212121212, -23.886263847146317], 'jadis': ['real', 12.1621621621622, 26.446125443191637], 'joy': ['talking', 47.7567567567568, 38.53722697562243], 'jungli': ['madeup', -17.4722222222222, 4.054430414218929], 'kitty': ['talking', 47.3235294117647, 57.012805753752914], 'levana': ['madeup', 44.7105263157895, 14.24663202706278], 'luft': ['madeup', -27.3243243243243, -19.554360088007893], 'marisa': ['real', 46.8, 51.72172492753222], 'matthias': ['real', -46.8888888888889, -36.537094604813404], 'myrtle': ['talking', 41.972972972973, 32.803575323289934], 'nova': ['talking', 35.7837837837838, 20.520565034691504], 'penthe': ['madeup', 10.0810810810811, -4.397623621144537], 'robbo': ['madeup', -43.75, -11.218335491812965], 'rosalind': ['real', 46.6, 12.284907171076465], 'scotch': ['talking', -38.4473684210526, 12.66331495752868], 'serafina': ['madeup', 44.4722222222222, 16.934791677746844], 'skellig': ['madeup', -39.8285714285714, -19.008793351174276], 'sprout': ['talking', -8.75, -15.66084251797048], 'tobias': ['real', -47.6666666666667, -29.53372300069934], 'will': ['real', -48.1470588235294, -21.31088124758114], 'zubaida': ['madeup', 29.3030303030303, -6.993699086207295], 'alasdair': ['real', -35.6571428571429, -37.893344453625225], 'alecto': ['madeup', -35.7222222222222, -16.392568501860197], 'araminta': ['madeup', 41.9714285714286, 23.341508479164716], 'bonny': ['talking', 41.8611111111111, 18.377336840452045], 'brum': ['madeup', -39.4594594594595, -19.681311752270627], 'caroline': ['real', 48.5945945945946, 21.572381315981197], 'chi': ['madeup', 12.8333333333333, -5.35458233330818], 'dark': ['talking', -21.8529411764706, -15.254724519271608], 'desire': ['talking', 40.6060606060606, 14.783491691955726], 'fiorella': ['real', 47.4324324324324, 14.078241103120606], 'flick': ['talking', -29.3611111111111, -22.924520298636935], 'francis': ['real', -15.1315789473684, -43.57622703464324], 'gerald': ['real', -48.8, -29.98037453176252], 'hannerl': ['madeup', -19.4545454545455, -15.918225088189226], 'ione': ['madeup', -10.5714285714286, 15.973182952667129], 'jewel': ['talking', 46.8235294117647, 37.78313994685737], 'julia': ['real', 49.027027027027, 35.157330713388774], 'kreacher': ['madeup', -39.9189189189189, -17.25873658188851], 'lak': ['madeup', -29.4166666666667, -22.371195619125352], 'lavender': ['talking', 45.3333333333333, 27.603990665946228], 'lee': ['real', -32.5277777777778, -21.118125841511066], 'marvel': ['talking', -19.3243243243243, -14.9419192153448], 'molly': ['real', 48.7297297297297, 37.160187295464056], 'nina': ['real', 48.0, 48.347065938119655], 'poop': ['talking', -35.0540540540541, -12.791970140814229], 'saiorse': ['madeup', 21.0714285714286, -19.185595459683796], 'silky': ['talking', 36.8055555555556, 1.2681162611633336], 'sparrow': ['talking', -6.11111111111111, -4.133543392746859], 'spink': ['madeup', -7.51612903225806, 13.373252472108506], 'steepy': ['talking', -25.8235294117647, -24.383454893057838], 'steg': ['madeup', -39.6571428571429, -25.106259346220728], 'susan': ['real', 49.9565217391304, 31.38089405694563], 'sybill': ['real', 26.2647058823529, 24.713638360719855], 'titania': ['madeup', 43.972972972973, 6.383562151556825], 'winky': ['talking', -4.0625, -9.69010759268701], 'wren': ['talking', 8.2, -5.888608460336212], 'adelaide': ['real', 45.7272727272727, 35.34495641417488], 'arcturus': ['madeup', -41.5405405405405, -13.80239106337371], 'arobynn': ['madeup', -9.12121212121212, -3.444351602276271], 'asim': ['madeup', -36.3055555555556, 12.56427045378074], 'beatrice': ['real', 47.9166666666667, 29.238631536503384], 'brilliantine': ['talking', 30.8285714285714, 2.849981291034478], 'cackle': ['talking', -12.0512820512821, -30.08753878544424], 'dal': ['talking', -29.6176470588235, -14.158078643750454], 'dalip': ['madeup', 0.804878048780488, -14.040228526670791], 'diamond': ['talking', 40.3243243243243, -7.820175820621221], 'dove': ['talking', 42.0833333333333, 12.329444520080408], 'drill': ['talking', -40.5588235294118, -48.480766174989945], 'ground': ['talking', -36.2571428571429, -6.781229578467916], 'grunter': ['madeup', -45.8529411764706, -57.66080708577928], 'iorek': ['madeup', -37.3809523809524, -7.758474303522061], 'jude': ['real', -9.19512195121951, 5.954750128556345], 'just': ['talking', -4.18181818181818, 0.945172577185355], 'karl': ['real', -47.0810810810811, -48.60234816605662], 'kathryn': ['real', 48.9473684210526, 4.714110395503024], 'lorcan': ['madeup', -36.1111111111111, -10.696367258702052], 'lucy': ['real', 48.8823529411765, 37.36396236760743], 'maud': ['real', 33.0882352941176, 28.33200235014592], 'morgra': ['madeup', 36.9189189189189, 2.6120023468573947], 'mouse': ['talking', 1.96969696969697, -16.579613232506055], 'neoma': ['madeup', 44.4285714285714, -42.057533842169136], 'nessa': ['real', 47.1578947368421, 27.542555320862462], 'skinner': ['talking', -42.0540540540541, -27.72949740595454], 'stanley': ['real', -49.0, -39.36206753580112], 'stuart': ['real', -48.2, -20.35697842164046], 'sunny': ['talking', 15.7435897435897, 28.983302617768416], 'talentino': ['madeup', -40.3783783783784, 4.3520754133356725], 'tenar': ['madeup', -26.3888888888889, -8.428951166010023], 'travers': ['talking', -40.6486486486486, -12.089838091354075], 'tristram': ['real', -35.9459459459459, -3.399020830557296], 'zahara': ['madeup', 41.6756756756757, 22.893180783747127]}\n",
            "{'amabala': ['madeup', 38.6388888888889, 4.067049318055883], 'apple': ['talking', 34.2105263157895, -3.5883090245346416], 'ball': ['talking', -32.8684210526316, -6.243029291965604], 'blackberry': ['talking', 22.2972972972973, 2.8948129708174406], 'blue': ['talking', -9.66666666666667, 2.027379630300077], 'cardan': ['madeup', -29.53125, 12.10668881110819], 'chloe': ['real', 48.5, 12.162918755769157], 'derrick': ['real', -49.1351351351351, -9.75602411667828], 'dumpa': ['madeup', -12.6571428571429, 27.2941140264347], 'enid': ['real', 28.6486486486486, 5.605663290572641], 'ethel': ['real', 46.8333333333333, 0.7291777166902822], 'flora': ['talking', 46.972972972973, 6.991708216962769], 'fortuna': ['madeup', 42.7777777777778, 30.24482032163658], 'ginny': ['real', 44.1891891891892, 4.251028293443072], 'gmork': ['madeup', -37.2972972972973, -19.374502571203703], 'goha': ['madeup', -1.4375, -0.43765438933693623], 'hepzibah': ['madeup', -0.263157894736842, 7.051091624077499], 'hope': ['talking', 46.4864864864865, -0.7848648953960495], 'inej': ['madeup', 4.63636363636364, 5.654456801068095], 'jackie': ['real', 31.9117647058824, -10.30692148199303], 'lark': ['talking', -14.4838709677419, 1.1978957469866778], 'marion': ['real', 10.7777777777778, 16.073807027311304], 'mildred': ['real', 46.2702702702703, 12.730173297454483], 'muffet': ['madeup', 22.2222222222222, -2.6091200506313363], 'mundungus': ['madeup', -43.3428571428571, -14.239844044741137], 'natasha': ['real', 48.5135135135135, 21.945855107778925], 'peter': ['real', -49.3888888888889, 8.044271139956463], 'roach': ['talking', -40.7567567567568, -3.7770432736623247], 'schaffa': ['madeup', 17.1176470588235, -25.39882696057471], 'scullery': ['talking', -33.2972972972973, -7.107728225489696], 'simon': ['real', -47.5833333333333, 1.8348326865935736], 'sway': ['talking', -11.5675675675676, -5.370571665298206], 'tansy': ['talking', 37.0, -10.690813525068009], 'tasha': ['madeup', 47.4054054054054, 35.91558047010166], 'thresh': ['talking', -32.5675675675676, -10.192045162511906], 'william': ['real', -49.4594594594595, 14.035363632318811], 'adelina': ['real', 47.7714285714286, 23.60501894866045], 'alastor': ['madeup', -38.8333333333333, -21.727585824509774], 'argus': ['madeup', -40.5833333333333, -38.39564426274948], 'claire': ['real', 49.1142857142857, 7.845383570088925], 'clove': ['talking', 20.6111111111111, 1.452812743857418], 'damon': ['real', -47.1428571428571, -1.964108078182928], 'day': ['talking', 23.3333333333333, 9.227294768870218], 'elide': ['madeup', 29.2777777777778, -27.68573525415603], 'fenrir': ['madeup', -33.4864864864865, -12.308115269759885], 'fox': ['talking', -3.58064516129032, -22.929024598351255], 'fred': ['real', -46.5555555555556, -3.359463705220673], 'freddie': ['real', -38.4054054054054, 4.010455653195136], 'george': ['real', -48.5384615384615, -11.476940336584212], 'gerard': ['real', -45.6857142857143, -7.987702471997024], 'goth': ['talking', -16.25, -11.676506023234955], 'guy': ['talking', -48.1621621621622, -11.891135974904035], 'joseph': ['real', -48.0263157894737, -0.34385963098670125], 'kaisa': ['madeup', 40.6944444444444, 63.16201482529592], 'kaz': ['madeup', -21.3939393939394, 3.3261141596926382], 'klaus': ['real', -47.1714285714286, -3.172207773178732], 'lyra': ['real', 44.3783783783784, 27.118722101413304], 'mina': ['real', 42.6388888888889, 32.42242024025525], 'minna': ['madeup', 41.8888888888889, 39.68830199821075], 'miskouri': ['madeup', 8.6875, -7.314317898556861], 'mogget': ['madeup', -19.2571428571429, -7.484378295766578], 'nergui': ['madeup', -10.2258064516129, -6.077321827423484], 'nitasha': ['madeup', 45.8333333333333, 28.958070122307404], 'parrish': ['talking', -19.4857142857143, -4.23967206610285], 'pearl': ['talking', 47.5714285714286, 2.50777360327273], 'ralph': ['real', -47.8055555555556, -11.392031317542028], 'spark': ['talking', -22.0, -13.441926746192461], 'touchstone': ['talking', -24.2727272727273, -0.9794938511163576], 'violet': ['talking', 45.1111111111111, 9.15977415086874], 'wolf': ['talking', -46.2162162162162, -14.02930208352509], 'yak': ['talking', -37.7222222222222, -6.8587058076567615], 'yozadah': ['madeup', -15.5, 14.095877800157874], 'alice': ['real', 49.1351351351351, 17.027328071706844], 'allie': ['real', 46.2285714285714, 26.52741181743882], 'april': ['talking', 48.3714285714286, 11.711629774131975], 'berry': ['talking', -13.1621621621622, -2.2286795024766057], 'chaol': ['madeup', -16.8611111111111, 40.42912856557437], 'cooki': ['madeup', 36.7297297297297, 64.12142966736623], 'coot': ['talking', -28.9473684210526, -5.66236327884015], 'ditto': ['talking', -21.2702702702703, -5.423791154155214], 'elena': ['real', 48.5277777777778, 33.683135390343], 'farder': ['madeup', -40.8918918918919, -15.044789676333647], 'gale': ['talking', 29.6052631578947, 0.2455390581188972], 'glimmer': ['talking', 38.4324324324324, 0.9984292295376207], 'griphook': ['madeup', -39.8378378378378, 3.7769026691593055], 'harold': ['real', -48.3888888888889, -16.680667965624878], 'henry': ['real', -48.8157894736842, -13.041904540800621], 'ian': ['real', -49.2121212121212, -0.39542679674255155], 'jadis': ['real', 12.1621621621622, 5.162003453817412], 'joy': ['talking', 47.7567567567568, 16.99407195015527], 'jungli': ['madeup', -17.4722222222222, -16.261180733753818], 'kitty': ['talking', 47.3235294117647, 16.838340425023585], 'levana': ['madeup', 44.7105263157895, 9.273296814874417], 'luft': ['madeup', -27.3243243243243, -15.693304989463778], 'marisa': ['real', 46.8, 36.940850717891976], 'matthias': ['real', -46.8888888888889, 10.662848615662394], 'myrtle': ['talking', 41.972972972973, -3.5289782171638358], 'nova': ['talking', 35.7837837837838, 41.146548599266254], 'penthe': ['madeup', 10.0810810810811, -4.479346693109516], 'robbo': ['madeup', -43.75, 40.35157942072658], 'rosalind': ['real', 46.6, 7.831607689781475], 'scotch': ['talking', -38.4473684210526, -10.889285338920317], 'serafina': ['madeup', 44.4722222222222, 51.3509505058191], 'skellig': ['madeup', -39.8285714285714, -18.62524727003224], 'sprout': ['talking', -8.75, -14.045038596529286], 'tobias': ['real', -47.6666666666667, -6.244722600182726], 'will': ['real', -48.1470588235294, -21.98876206560358], 'zubaida': ['madeup', 29.3030303030303, 25.915522826632984], 'alasdair': ['real', -35.6571428571429, 8.445067171106118], 'alecto': ['madeup', -35.7222222222222, 11.07672691332673], 'araminta': ['madeup', 41.9714285714286, 28.713760321432872], 'bonny': ['talking', 41.8611111111111, -8.853499164821278], 'brum': ['madeup', -39.4594594594595, -39.4843003927562], 'caroline': ['real', 48.5945945945946, 2.3007088661045216], 'chi': ['madeup', 12.8333333333333, 8.506718585482517], 'dark': ['talking', -21.8529411764706, -14.96391549652283], 'desire': ['talking', 40.6060606060606, 6.4105744264193305], 'fiorella': ['real', 47.4324324324324, 21.290868730335397], 'flick': ['talking', -29.3611111111111, -14.382283379654522], 'francis': ['real', -15.1315789473684, -12.916362094216414], 'gerald': ['real', -48.8, -21.07495050931768], 'hannerl': ['madeup', -19.4545454545455, -33.365580241427764], 'ione': ['madeup', -10.5714285714286, 44.666206825206515], 'jewel': ['talking', 46.8235294117647, 16.95637792031637], 'julia': ['real', 49.027027027027, 28.9993472554671], 'kreacher': ['madeup', -39.9189189189189, -46.059647192719254], 'lak': ['madeup', -29.4166666666667, -51.04428319668734], 'lavender': ['talking', 45.3333333333333, -2.116998072047174], 'lee': ['real', -32.5277777777778, 9.315203722691479], 'marvel': ['talking', -19.3243243243243, -3.1948905132043386], 'molly': ['real', 48.7297297297297, 0.7998626313326866], 'nina': ['real', 48.0, 30.126366546925397], 'poop': ['talking', -35.0540540540541, 3.3144342011452856], 'saiorse': ['madeup', 21.0714285714286, -22.265740425047504], 'silky': ['talking', 36.8055555555556, 16.017929456573185], 'sparrow': ['talking', -6.11111111111111, -12.085065124255106], 'spink': ['madeup', -7.51612903225806, 32.35673659378504], 'steepy': ['talking', -25.8235294117647, -22.624369773785403], 'steg': ['madeup', -39.6571428571429, -20.30334571585218], 'susan': ['real', 49.9565217391304, 2.0558160359905813], 'sybill': ['real', 26.2647058823529, 8.538152270076324], 'titania': ['madeup', 43.972972972973, -10.496865987130608], 'winky': ['talking', -4.0625, -7.565115127323686], 'wren': ['talking', 8.2, -5.902424264075897], 'adelaide': ['real', 45.7272727272727, 29.550764515946774], 'arcturus': ['madeup', -41.5405405405405, 8.242290850608963], 'arobynn': ['madeup', -9.12121212121212, 8.595289515221129], 'asim': ['madeup', -36.3055555555556, -9.752538198127048], 'beatrice': ['real', 47.9166666666667, 14.600538330514798], 'brilliantine': ['talking', 30.8285714285714, 8.100735878961515], 'cackle': ['talking', -12.0512820512821, -12.864926535326022], 'dal': ['talking', -29.6176470588235, 1.9974237689126273], 'dalip': ['madeup', 0.804878048780488, -16.657631686389003], 'diamond': ['talking', 40.3243243243243, 0.13900433773158838], 'dove': ['talking', 42.0833333333333, -1.0064515533372127], 'drill': ['talking', -40.5588235294118, 0.3913083980598806], 'ground': ['talking', -36.2571428571429, -8.936045446778406], 'grunter': ['madeup', -45.8529411764706, -60.40718508260316], 'iorek': ['madeup', -37.3809523809524, -12.475947075282711], 'jude': ['real', -9.19512195121951, 1.9714547749010567], 'just': ['talking', -4.18181818181818, -1.9430210240254215], 'karl': ['real', -47.0810810810811, -10.166667709060533], 'kathryn': ['real', 48.9473684210526, -10.735335215696502], 'lorcan': ['madeup', -36.1111111111111, 6.304661799810347], 'lucy': ['real', 48.8823529411765, 20.974960512693094], 'maud': ['real', 33.0882352941176, 15.623898770313602], 'morgra': ['madeup', 36.9189189189189, 4.1990987407948275], 'mouse': ['talking', 1.96969696969697, 9.361893981754701], 'neoma': ['madeup', 44.4285714285714, -11.762072246680294], 'nessa': ['real', 47.1578947368421, 20.024543028266205], 'skinner': ['talking', -42.0540540540541, 1.7488836610637293], 'stanley': ['real', -49.0, -0.864331537773305], 'stuart': ['real', -48.2, -6.804118663590097], 'sunny': ['talking', 15.7435897435897, 26.409372549277727], 'talentino': ['madeup', -40.3783783783784, 18.788499645781478], 'tenar': ['madeup', -26.3888888888889, 10.181195717943126], 'travers': ['talking', -40.6486486486486, -4.247274181713182], 'tristram': ['real', -35.9459459459459, -6.059800515442672], 'zahara': ['madeup', 41.6756756756757, 34.000894947307366]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "polarity_dict = {'regular' : [], 'formless' : [], 'form_only' : []}                               # initialize the score dictionary for polarity\n",
        "\n",
        "polarity_pred_dict_regular, polarity_pred_dict_formless, \\\n",
        "polarity_pred_dict_formonly = \\\n",
        "nested_cross_validator(df_polarity, 'rating.mean_valence', polarity_dict)       # Perform the 5-fold cross validation, save the prediction dictionaries\n",
        "\n",
        "pickle_polarity_ncv = open(pickle_path + \"polarity_ncv.pickle\", \"wb\")           # Save the dictionary to a pickle\n",
        "pickle.dump(polarity_dict, pickle_polarity_ncv)\n",
        "pickle_polarity_ncv.close()\n",
        "\n",
        "pickle_polarity_preds_regular = open(pickle_path + \"polarity_preds_regular.pickle\", \"wb\")           # Save the regular predictions dictionary to a pickle\n",
        "pickle.dump(polarity_pred_dict_regular, pickle_polarity_preds_regular)\n",
        "pickle_polarity_preds_regular.close()\n",
        "\n",
        "pickle_polarity_preds_formless = open(pickle_path + \"polarity_preds_formless.pickle\", \"wb\")           # Save the formless predictions dictionary to a pickle\n",
        "pickle.dump(polarity_pred_dict_formless, pickle_polarity_preds_formless)\n",
        "pickle_polarity_preds_formless.close()\n",
        "\n",
        "pickle_polarity_preds_formonly = open(pickle_path + \"polarity_preds_formonly.pickle\", \"wb\")           # Save the formless predictions dictionary to a pickle\n",
        "pickle.dump(polarity_pred_dict_formonly, pickle_polarity_preds_formonly)\n",
        "pickle_polarity_preds_formonly.close()\n",
        "\n",
        "nested_cv_addotron(polarity_dict, \"polarity\")                                   # Print all of the values in an orderly fashion"
      ],
      "metadata": {
        "id": "Q4K6DHUV7u5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48746cd4-b7cf-4cef-df13-8c436e5beeb0"
      },
      "id": "Q4K6DHUV7u5e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average regular alpha for polarity = 0.24835051374456635\n",
            "Average regular l1_ratio for polarity = 0.782\n",
            "Average regular n_iters for polarity = 272.0\n",
            "Average regular intercept for polarity = 4.408481760823845\n",
            "Average regular mse_train for polarity = 64.57666972063635\n",
            "Average regular mae_test for polarity = 15.71990488339597\n",
            "Average regular mse_test for polarity = 376.60904549612854\n",
            "Average regular median_ae_test for polarity = 13.381502651483775\n",
            "Average regular r2 for polarity = -0.03191570552598533\n",
            "Average regular mae_madeup for polarity = 17.382957225913962\n",
            "Average regular mae_real for polarity = 12.70376987699233\n",
            "Average regular mae_talking for polarity = 16.720654207838017\n",
            "Average regular median_ae_madeup for polarity = 16.198452548749998\n",
            "Average regular median_ae_real for polarity = 12.926498330940783\n",
            "Average regular median_ae_talking for polarity = 16.796602657566062\n",
            "Average regular mean_or_form_mae for polarity = 15.099909388155663\n",
            "Average regular mean_or_form_median_ae for polarity = 13.343528741382567\n",
            "Average regular mean_or_form_r2 for polarity = -0.004626052166432593\n",
            "Average regular mean_vec_mae_test for polarity = 17.61507358173261\n",
            "Average regular mean_vec_mse_test for polarity = 412.61032850202776\n",
            "Average regular mean_vec_median_ae_test for polarity = 18.631530685497758\n",
            "Average regular mean_vec_r2 for polarity = -0.11769143744667451\n",
            "\n",
            "\n",
            "Average formless alpha for polarity = 0.42302345074498005\n",
            "Average formless l1_ratio for polarity = 0.6039999999999999\n",
            "Average formless n_iters for polarity = 50.8\n",
            "Average formless intercept for polarity = -0.6500950350938446\n",
            "Average formless mse_train for polarity = 86.79677818147493\n",
            "Average formless mae_test for polarity = 15.506894790293615\n",
            "Average formless mse_test for polarity = 412.202658767893\n",
            "Average formless median_ae_test for polarity = 12.82638415772237\n",
            "Average formless r2 for polarity = -0.10953588152851912\n",
            "Average formless mae_madeup for polarity = 14.328867888674784\n",
            "Average formless mae_real for polarity = 14.80940174548689\n",
            "Average formless mae_talking for polarity = 17.375009540740198\n",
            "Average formless median_ae_madeup for polarity = 12.985331543342761\n",
            "Average formless median_ae_real for polarity = 14.576313622654448\n",
            "Average formless median_ae_talking for polarity = 15.31594563204879\n",
            "Average formless mean_or_form_mae for polarity = 15.885048218790354\n",
            "Average formless mean_or_form_median_ae for polarity = 13.05896055875298\n",
            "Average formless mean_or_form_r2 for polarity = -0.11207134276022442\n",
            "Average formless mean_vec_mae_test for polarity = 19.128213595464228\n",
            "Average formless mean_vec_mse_test for polarity = 466.3295071942812\n",
            "Average formless mean_vec_median_ae_test for polarity = 21.758397154465133\n",
            "Average formless mean_vec_r2 for polarity = -0.2658998423202834\n",
            "\n",
            "\n",
            "Average form_only alpha for polarity = 0.253608035187957\n",
            "Average form_only l1_ratio for polarity = 0.7779999999999999\n",
            "Average form_only n_iters for polarity = 500.4\n",
            "Average form_only intercept for polarity = 6.460328330863737\n",
            "Average form_only mse_train for polarity = 164.15241064078754\n",
            "Average form_only mae_test for polarity = 14.109139235764776\n",
            "Average form_only mse_test for polarity = 321.8951173398326\n",
            "Average form_only median_ae_test for polarity = 11.324949252667919\n",
            "Average form_only r2 for polarity = 0.1102306356297702\n",
            "Average form_only mae_madeup for polarity = 14.855356676011402\n",
            "Average form_only mae_real for polarity = 12.983898911878214\n",
            "Average form_only mae_talking for polarity = 14.250348356648919\n",
            "Average form_only median_ae_madeup for polarity = 13.383760654257784\n",
            "Average form_only median_ae_real for polarity = 10.595234233784103\n",
            "Average form_only median_ae_talking for polarity = 12.418374873994951\n",
            "Average form_only mean_or_form_mae for polarity = 14.109139235764776\n",
            "Average form_only mean_or_form_median_ae for polarity = 11.324949252667917\n",
            "Average form_only mean_or_form_r2 for polarity = -0.16984999738226841\n",
            "Average form_only mean_vec_mae_test for polarity = 19.81448145051636\n",
            "Average form_only mean_vec_mse_test for polarity = 499.2945105528747\n",
            "Average form_only mean_vec_median_ae_test for polarity = 21.63301452544934\n",
            "Average form_only mean_vec_r2 for polarity = -0.359633687902026\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Networks"
      ],
      "metadata": {
        "id": "CpDIXYxG81W0"
      },
      "id": "CpDIXYxG81W0"
    },
    {
      "cell_type": "code",
      "source": [
        "def fnn_maker(x_train, y_train, x_test, y_test, nodes, dropout):\n",
        "  seed(17042020)\n",
        "  set_seed(17042020)\n",
        "\n",
        "  fnn_model = Sequential()\n",
        "\n",
        "  fnn_model.add(Dense(nodes, input_dim=300, kernel_initializer=HeNormal(), activation=keras.layers.LeakyReLU()))\n",
        "\n",
        "  fnn_model.add(Dense(1, activation='linear'))\n",
        "\n",
        "  callback = EarlyStopping(monitor = 'loss', patience=3)\n",
        "\n",
        "  fnn_model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "  fnn_model.fit(x_train, y_train, epochs=100, batch_size=len(x_train), callbacks=[callback], verbose=0)\n",
        "\n",
        "  y_pred = fnn_model.predict(x_test)\n",
        "  \n",
        "  mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "  return mse, fnn_model"
      ],
      "metadata": {
        "id": "IWq2EO-683go"
      },
      "id": "IWq2EO-683go",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Grid Search"
      ],
      "metadata": {
        "id": "icxbZNdEm15J"
      },
      "id": "icxbZNdEm15J"
    },
    {
      "cell_type": "code",
      "source": [
        "def nested_neural_network_finder(df, rating, dimension, dictionary):\n",
        "  skf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=17042020)\n",
        "\n",
        "  for train_index, test_index in skf.split(df, df[['name_type']]):\n",
        "    k_fold = KFold(n_splits = len(train_index), shuffle=True, random_state=17042020)\n",
        "\n",
        "    x_train_unfasttexted = df.iloc[train_index]\n",
        "    x_test_unfasttexted = df.iloc[test_index]\n",
        "\n",
        "    x_train = fasttext_xifyer(x_train_unfasttexted)\n",
        "    x_train_formless = fasttext_xifyer_formless(x_train_unfasttexted)\n",
        "\n",
        "    x_test = fasttext_xifyer(x_test_unfasttexted)\n",
        "    x_test_formless = fasttext_xifyer_formless(x_test_unfasttexted)\n",
        "\n",
        "\n",
        "    y_train = df.iloc[train_index][rating]\n",
        "    y_test = df.iloc[test_index][rating]\n",
        "\n",
        "    for nested_train_index, nested_test_index in k_fold.split(x_train):\n",
        "      nested_x_train = x_train[[nested_train_index], :].reshape(len(nested_train_index), 300)\n",
        "      nested_x_train_formless = x_train_formless[[nested_train_index], :].reshape(len(nested_train_index), 300)\n",
        "\n",
        "      nested_x_test = x_train[[nested_test_index], :].reshape(len(nested_test_index), 300)\n",
        "      nested_x_test_formless = x_train_formless[[nested_test_index], :].reshape(len(nested_test_index), 300)\n",
        "\n",
        "      nested_y_train = y_train.iloc[nested_train_index]\n",
        "      nested_y_test = y_train.iloc[nested_test_index]\n",
        "\n",
        "      for nodes in [8, 16, 32, 50, 64, 100, 128, 200, 256, 300, 512]:\n",
        "        for dropout in [0.5, 0.6, 0.7, 0.8]:\n",
        "          age_dict_nn['regular'][str(nodes)][str(dropout)].append(\n",
        "              fnn_maker(nested_x_train, \n",
        "                        nested_y_train, \n",
        "                        nested_x_test, \n",
        "                        nested_y_test, \n",
        "                        nodes, \n",
        "                        dropout))\n",
        "\n",
        "          age_dict_nn['formless'][str(nodes)][str(dropout)].append(\n",
        "              fnn_maker(nested_x_train_formless, \n",
        "                        nested_y_train, \n",
        "                        nested_x_test_formless, \n",
        "                        nested_y_test, \n",
        "                        nodes, \n",
        "                        dropout))\n",
        "  \n",
        "  for nodes in [8, 16, 32, 50, 64, 100, 128, 200, 256, 300, 512]:\n",
        "      for dropout in [0.5, 0.6, 0.7, 0.8]:\n",
        "        regular_mse = sum(age_dict_nn['regular'][str(nodes)][str(dropout)]) / len(age_dict_nn['regular'][str(nodes)][str(dropout)])\n",
        "        formless_mse = sum(age_dict_nn['formless'][str(nodes)][str(dropout)]) / len(age_dict_nn['formless'][str(nodes)][str(dropout)])\n",
        "\n",
        "        print(\"{}, regular, nodes = {}, dropout = {}, Average MSE = {}\".format(dimension, nodes, dropout, regular_mse))\n",
        "        print(\"{}, formless, nodes = {}, dropout = {}, Average MSE = {}\".format(dimension, nodes, dropout, formless_mse))\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "AtODe-sF9eWP"
      },
      "id": "AtODe-sF9eWP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_dict_nn = {'regular' : {'8': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '16' : {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '32': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '50': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '64': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '100': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '128': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '200': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '256': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '300': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '512': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}}, \n",
        "               'formless' : {'8': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '16' : {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '32': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '50': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '64': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '100': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '128': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '200': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '256': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '300': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '512': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}}}\n",
        "\n",
        "nested_neural_network_finder(df_age, 'rating.mean_age', 'Age', age_dict_nn)"
      ],
      "metadata": {
        "id": "zpZlYlMy9g3-"
      },
      "id": "zpZlYlMy9g3-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_dict_nn = {'regular' : {'8': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '16' : {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '32': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '50': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '64': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '100': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '128': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '200': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '256': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '300': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '512': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}}, \n",
        "               'formless' : {'8': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '16' : {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '32': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '50': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '64': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '100': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '128': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '200': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '256': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '300': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '512': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}}}\n",
        "\n",
        "nested_neural_network_finder(df_gender, 'rating.mean_gender', 'Gender', gender_dict_nn)"
      ],
      "metadata": {
        "id": "jqWVrhwF9jDI"
      },
      "id": "jqWVrhwF9jDI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polarity_dict_nn = {'regular' : {'8': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '16' : {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '32': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '50': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '64': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '100': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '128': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '200': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '256': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '300': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '512': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}}, \n",
        "               'formless' : {'8': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '16' : {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '32': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '50': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '64': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '100': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '128': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '200': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '256': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '300': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}, \n",
        "                            '512': {'0.5': [], '0.6': [], '0.7': [], '0.8': []}}}\n",
        "\n",
        "nested_neural_network_finder(df_polarity, 'rating.mean_valence', 'Polarity', polarity_dict_nn)"
      ],
      "metadata": {
        "id": "PA-vgaKE9jwU"
      },
      "id": "PA-vgaKE9jwU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation of Final Model"
      ],
      "metadata": {
        "id": "UW5MsCxom7e0"
      },
      "id": "UW5MsCxom7e0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Functions"
      ],
      "metadata": {
        "id": "5KWWakqlQFf4"
      },
      "id": "5KWWakqlQFf4"
    },
    {
      "cell_type": "code",
      "source": [
        "def fnn_maker_and_evaluator(x_train, y_train, x_test, y_test, pred_dict, \n",
        "                            test_names, test_name_types, nodes, dropout):\n",
        "  # Input:\n",
        "  # - x_train = array of embeddings used to train the model\n",
        "  # - y_train = array of ratings used to train the model\n",
        "  # - x_test = array of embeddings used to test the model\n",
        "  # - y_test = array of ratings used to test the model\n",
        "  # - pred_dict = a dictionary that will be filled with predictions per name\n",
        "  # - test_names = dataframe containing the full names (i.e., not the embeddings) \n",
        "  # - test_name_types = dataframe containing the name type (real, madeup, talking)\n",
        "  # - nodes = \n",
        "  # - dropout = \n",
        "\n",
        "  _, fnn_model = fnn_maker(x_train, y_train, x_test, y_test, nodes, dropout)\n",
        "  \n",
        "  mse_train, mae_test, mse_test, median_ae_test, r2, mae_madeup, mae_real, \\\n",
        "  mae_talking, median_ae_madeup, median_ae_real, median_ae_talking, \\\n",
        "  mean_or_form_mae, mean_or_form_median_ae, mean_or_form_r2, mean_vec_mae_test, \\\n",
        "  mean_vec_mse_test, mean_vec_median_ae_test, mean_vec_r2 = \\\n",
        "  model_evaluator(fnn_model, x_train, x_test, y_train, y_test, \n",
        "                  pred_dict, test_names, test_name_types)\n",
        "\n",
        "  return mse_train, mae_test, mse_test, median_ae_test, r2, mae_madeup, mae_real, \\\n",
        "  mae_talking, median_ae_madeup, median_ae_real, median_ae_talking, \\\n",
        "  mean_or_form_mae, mean_or_form_median_ae, mean_or_form_r2, mean_vec_mae_test, \\\n",
        "  mean_vec_mse_test, mean_vec_median_ae_test, mean_vec_r2 "
      ],
      "metadata": {
        "id": "wJ4Vxbtaar8W"
      },
      "id": "wJ4Vxbtaar8W",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network_5_folder(df, rating, dimension, dictionary, nodes, dropout):\n",
        "  skf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=17042020)\n",
        "  \n",
        "  pred_dict_regular = {}\n",
        "  pred_dict_formless = {}\n",
        "  pred_dict_formonly = {}\n",
        "\n",
        "  for train_index, test_index in skf.split(df, df[['name_type']]):\n",
        "    x_train_unfasttexted = df.iloc[train_index]\n",
        "    x_test_unfasttexted = df.iloc[test_index]\n",
        "\n",
        "    x_train = fasttext_xifyer(x_train_unfasttexted)\n",
        "    x_train_formless = fasttext_xifyer_formless(x_train_unfasttexted)\n",
        "    x_train_formonly = fasttext_xifyer_formonly(x_train_unfasttexted)\n",
        "\n",
        "    x_test = fasttext_xifyer(x_test_unfasttexted)\n",
        "    x_test_formless = fasttext_xifyer_formless(x_test_unfasttexted)\n",
        "    x_test_formonly = fasttext_xifyer_formonly(x_test_unfasttexted)\n",
        "\n",
        "\n",
        "    y_train = df.iloc[train_index][rating]\n",
        "    y_test = df.iloc[test_index][rating]\n",
        "\n",
        "    test_names = df.iloc[test_index]['name']\n",
        "    test_name_types = df.iloc[test_index]['name_type']\n",
        "\n",
        "\n",
        "    mse_train, mae_test, mse_test, median_ae_test, r2, mae_madeup, mae_real, \\\n",
        "    mae_talking, median_ae_madeup, median_ae_real, median_ae_talking, \\\n",
        "    mean_or_form_mae, mean_or_form_median_ae, mean_or_form_r2, mean_vec_mae_test, \\\n",
        "    mean_vec_mse_test, mean_vec_median_ae_test, mean_vec_r2 \\\n",
        "    = fnn_maker_and_evaluator(x_train, \n",
        "                              y_train, \n",
        "                              x_test, \n",
        "                              y_test, \n",
        "                              pred_dict_regular,\n",
        "                              test_names,\n",
        "                              test_name_types, \n",
        "                              nodes,\n",
        "                              dropout)\n",
        "\n",
        "    mse_train_formless, mae_test_formless, mse_test_formless, median_ae_test_formless, \\\n",
        "    r2_formless, mae_madeup_formless, mae_real_formless, mae_talking_formless, \\\n",
        "    median_ae_madeup_formless, median_ae_real_formless, median_ae_talking_formless, \\\n",
        "    mean_or_form_mae_formless, mean_or_form_median_ae_formless, mean_or_form_r2_formless, \\\n",
        "    mean_vec_mae_test_formless, mean_vec_mse_test_formless, mean_vec_median_ae_test_formless, \\\n",
        "    mean_vec_r2_formless \\\n",
        "    = fnn_maker_and_evaluator(x_train_formless, \n",
        "                              y_train, \n",
        "                              x_test_formless, \n",
        "                              y_test, \n",
        "                              pred_dict_formless,\n",
        "                              test_names,\n",
        "                              test_name_types, \n",
        "                              nodes,\n",
        "                              dropout)\n",
        "\n",
        "    mse_train_formonly, mae_test_formonly, mse_test_formonly, median_ae_test_formonly, \\\n",
        "    r2_formonly, mae_madeup_formonly, mae_real_formonly, mae_talking_formonly, \\\n",
        "    median_ae_madeup_formonly, median_ae_real_formonly, median_ae_talking_formonly, \\\n",
        "    mean_or_form_mae_formonly, mean_or_form_median_ae_formonly, mean_or_form_r2_formonly, \\\n",
        "    mean_vec_mae_test_formonly, mean_vec_mse_test_formonly, mean_vec_median_ae_test_formonly, \\\n",
        "    mean_vec_r2_formonly \\\n",
        "    = fnn_maker_and_evaluator(x_train_formonly, \n",
        "                              y_train, \n",
        "                              x_test_formonly, \n",
        "                              y_test, \n",
        "                              pred_dict_formonly,\n",
        "                              test_names,\n",
        "                              test_name_types, \n",
        "                              nodes,\n",
        "                              dropout)\n",
        "\n",
        "    dictionary['regular'].append([mse_train, mae_test, mse_test, median_ae_test, \n",
        "                                  r2, mae_madeup, mae_real, mae_talking, \n",
        "                                  median_ae_madeup, median_ae_real, median_ae_talking,\n",
        "                                  mean_or_form_mae, mean_or_form_median_ae, \n",
        "                                  mean_or_form_r2, mean_vec_mae_test,\n",
        "                                  mean_vec_mse_test, mean_vec_median_ae_test, \n",
        "                                  mean_vec_r2]) \n",
        "\n",
        "    dictionary['formless'].append([mse_train_formless, mae_test_formless, \n",
        "                                   mse_test_formless, median_ae_test_formless, \n",
        "                                   r2_formless, mae_madeup_formless, \n",
        "                                   mae_real_formless, mae_talking_formless, \n",
        "                                   median_ae_madeup_formless, median_ae_real_formless, \n",
        "                                   median_ae_talking_formless, mean_or_form_mae_formless, \n",
        "                                   mean_or_form_median_ae_formless, mean_or_form_r2_formless, \n",
        "                                   mean_vec_mae_test_formless, mean_vec_mse_test_formless, \n",
        "                                   mean_vec_median_ae_test_formless,\n",
        "                                   mean_vec_r2_formless])  \n",
        "    \n",
        "    dictionary['form_only'].append([mse_train_formonly, mae_test_formonly, \n",
        "                                    mse_test_formonly, median_ae_test_formonly, \n",
        "                                    r2_formonly, mae_madeup_formonly, \n",
        "                                    mae_real_formonly, mae_talking_formonly, \n",
        "                                    median_ae_madeup_formonly, median_ae_real_formonly, \n",
        "                                    median_ae_talking_formonly, mean_or_form_mae_formonly, \n",
        "                                    mean_or_form_median_ae_formonly, \n",
        "                                    mean_or_form_r2_formonly, mean_vec_mae_test_formonly, \n",
        "                                    mean_vec_mse_test_formonly, mean_vec_median_ae_test_formonly,\n",
        "                                    mean_vec_r2_formonly])    \n",
        "  \n",
        "  variable_list = ('mse_train', 'mae_test', 'mse_test', 'median_ae_test', 'r2',  # List indicating all of the variables of interest \n",
        "                   'mae_madeup', 'mae_real', 'mae_talking', 'median_ae_madeup',\n",
        "                   'median_ae_real', 'median_ae_talking', 'mean_or_form_mae', \n",
        "                   'mean_or_form_median_ae', 'mean_or_form_r2', 'mean_vec_mae_test', \n",
        "                   'mean_vec_mse_test', 'mean_vec_median_ae_test', 'mean_vec_r2')\n",
        "  \n",
        "  type_list = ['regular', 'formless', 'form_only']                              # List indicating the model type\n",
        "\n",
        "  regular_list = dictionary['regular']\n",
        "  formless_list = dictionary['formless']\n",
        "  formonly_list = dictionary['form_only']\n",
        "  \n",
        "  regular_list = [sum(x) for x in zip(*regular_list)]                           # For all of the variables in the list, sum the scores (regular)\n",
        "  regular_list = [x / 5 for x in regular_list]                                  # For all of the variables in the list, divide the sum by 5 to get the average score (regular)\n",
        "\n",
        "  formless_list = [sum(x) for x in zip(*formless_list)]                         # For all of the variables in the list, sum the scores (formless)\n",
        "  formless_list = [x / 5 for x in formless_list]                                # For all of the variables in the list, divide the sum by 5 to get the average score (formless)\n",
        "\n",
        "  formonly_list = [sum(x) for x in zip(*formonly_list)] \n",
        "  formonly_list = [x / 5 for x in formonly_list]\n",
        "\n",
        "  for value_list, analysis_type in zip([regular_list, formless_list, \n",
        "                                        formonly_list], type_list):\n",
        "    for value, variable in zip(value_list, variable_list):\n",
        "      print(f\"Average {analysis_type} {variable} for {dimension} = {value}\")    # Per variable, and per analysis type, print the corresponding average value\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return pred_dict_regular, pred_dict_formless, pred_dict_formonly"
      ],
      "metadata": {
        "id": "HzXZVhhwfvXG"
      },
      "id": "HzXZVhhwfvXG",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Running the functions"
      ],
      "metadata": {
        "id": "ZwKSkMAaQKCV"
      },
      "id": "ZwKSkMAaQKCV"
    },
    {
      "cell_type": "code",
      "source": [
        "age_dict_nn_final = {'regular' : [], 'formless' : [], 'form_only': []} \n",
        "\n",
        "age_pred_dict_regular_nn, age_pred_dict_formless_nn, \\\n",
        "age_pred_dict_formonly_nn = \\\n",
        "neural_network_5_folder(df_age, 'rating.mean_age', 'age', \n",
        "                        age_dict_nn_final, 512, 0.4)\n",
        "\n",
        "pickle_age_nn_final = open(pickle_path + \"age_nn_final.pickle\", \"wb\")           # Save the dictionary to a pickle\n",
        "pickle.dump(age_dict_nn_final, pickle_age_nn_final)\n",
        "pickle_age_nn_final.close()"
      ],
      "metadata": {
        "id": "uRe68H44NzG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30554c1-34b1-40df-c5cc-b4a03a532178"
      },
      "id": "uRe68H44NzG3",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average regular mse_train for age = 7.880749060894564\n",
            "Average regular mae_test for age = 13.227395134040204\n",
            "Average regular mse_test for age = 272.7477955906326\n",
            "Average regular median_ae_test for age = 10.996755990102764\n",
            "Average regular r2 for age = 0.3402248011855453\n",
            "Average regular mae_madeup for age = 14.683114896501815\n",
            "Average regular mae_real for age = 12.61012601852417\n",
            "Average regular mae_talking for age = 12.43816555870904\n",
            "Average regular median_ae_madeup for age = 13.985541534423827\n",
            "Average regular median_ae_real for age = 10.883024406433105\n",
            "Average regular median_ae_talking for age = 10.124204730987548\n",
            "Average regular mean_or_form_mae for age = 13.756763076782226\n",
            "Average regular mean_or_form_median_ae for age = 11.153623962402344\n",
            "Average regular mean_or_form_r2 for age = 0.295080081193644\n",
            "Average regular mean_vec_mae_test for age = 17.62601406104546\n",
            "Average regular mean_vec_mse_test for age = 424.61261114358683\n",
            "Average regular mean_vec_median_ae_test for age = 17.402948379296465\n",
            "Average regular mean_vec_r2 for age = -0.0339538879421676\n",
            "\n",
            "\n",
            "Average formless mse_train for age = 8.52621974566387\n",
            "Average formless mae_test for age = 17.263213931238166\n",
            "Average formless mse_test for age = 449.5550478149495\n",
            "Average formless median_ae_test for age = 15.765014306671992\n",
            "Average formless r2 for age = -0.0820487330432242\n",
            "Average formless mae_madeup for age = 17.34291169302804\n",
            "Average formless mae_real for age = 17.45381717681885\n",
            "Average formless mae_talking for age = 17.03289099799262\n",
            "Average formless median_ae_madeup for age = 16.52672634124756\n",
            "Average formless median_ae_real for age = 16.585488319396973\n",
            "Average formless median_ae_talking for age = 16.041041755676268\n",
            "Average formless mean_or_form_mae for age = 17.12289810180664\n",
            "Average formless mean_or_form_median_ae for age = 16.415913772583007\n",
            "Average formless mean_or_form_r2 for age = 0.08508656357394495\n",
            "Average formless mean_vec_mae_test for age = 18.136591472805968\n",
            "Average formless mean_vec_mse_test for age = 447.63614424955256\n",
            "Average formless mean_vec_median_ae_test for age = 17.69623930501004\n",
            "Average formless mean_vec_r2 for age = -0.0919948069298305\n",
            "\n",
            "\n",
            "Average form_only mse_train for age = 117.70748771387775\n",
            "Average form_only mae_test for age = 16.098262660591985\n",
            "Average form_only mse_test for age = 371.20723851333435\n",
            "Average form_only median_ae_test for age = 15.617148910067527\n",
            "Average form_only r2 for age = 0.09552596054521298\n",
            "Average form_only mae_madeup for age = 15.774603189740864\n",
            "Average form_only mae_real for age = 15.862488746643066\n",
            "Average form_only mae_talking for age = 16.534800402323405\n",
            "Average form_only median_ae_madeup for age = 17.00265235900879\n",
            "Average form_only median_ae_real for age = 14.863397979736328\n",
            "Average form_only median_ae_talking for age = 14.453538131713866\n",
            "Average form_only mean_or_form_mae for age = 16.09826202392578\n",
            "Average form_only mean_or_form_median_ae for age = 15.617148971557617\n",
            "Average form_only mean_or_form_r2 for age = -0.15702386250425932\n",
            "Average form_only mean_vec_mae_test for age = 17.835673978641918\n",
            "Average form_only mean_vec_mse_test for age = 436.159449225346\n",
            "Average form_only mean_vec_median_ae_test for age = 17.896121503112578\n",
            "Average form_only mean_vec_r2 for age = -0.06458797862923116\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "age_pred_nn_df = []\n",
        "\n",
        "for i in sorted(df_age['name']):\n",
        "  i = i.lower()\n",
        "  age_pred_nn_df.append([i, age_pred_dict_regular_nn[i][0],\n",
        "                         age_pred_dict_regular_nn[i][1], \n",
        "                         age_pred_dict_regular_nn[i][2],\n",
        "                         age_pred_dict_regular_nn[i + '_mean_vector'][2], \n",
        "                         age_pred_dict_formless_nn[i][2], \n",
        "                         age_pred_dict_formless_nn[i + '_mean_vector'][2], \n",
        "                         age_pred_dict_formonly_nn[i][2],\n",
        "                         age_pred_dict_formonly_nn[i + '_mean_vector'][2]])\n",
        "  \n",
        "age_pred_nn_df = pd.DataFrame(age_pred_nn_df, columns=['Name', 'NameType', \n",
        "                                                       'TrueRating', 'RegularRating', \n",
        "                                                       'RegularMeanVecRating',  \n",
        "                                                       'FormlessRating', \n",
        "                                                       'FormlessMeanVecRating', \n",
        "                                                       'FormonlyRating', \n",
        "                                                       'FormonlyMeanVecRating'])\n",
        "\n",
        "age_pred_nn_df.to_csv(pickle_path + 'age_pred_nn.csv', index=False)"
      ],
      "metadata": {
        "id": "GrhHgAbgru1Q"
      },
      "id": "GrhHgAbgru1Q",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_dict_nn_final = {'regular' : [], 'formless' : [], 'form_only': []} \n",
        "\n",
        "gender_pred_dict_regular_nn, gender_pred_dict_formless_nn, \\\n",
        "gender_pred_dict_formonly_nn = \\\n",
        "neural_network_5_folder(df_gender, 'rating.mean_gender', 'gender', \n",
        "                        gender_dict_nn_final, 512, 0.3)\n",
        "\n",
        "pickle_gender_nn_final = open(pickle_path + \"gender_nn_final.pickle\", \"wb\")     # Save the dictionary to a pickle\n",
        "pickle.dump(gender_dict_nn_final, pickle_gender_nn_final)\n",
        "pickle_gender_nn_final.close()"
      ],
      "metadata": {
        "id": "iGyedeucLT-z"
      },
      "id": "iGyedeucLT-z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_pred_nn_df = []\n",
        "\n",
        "for i in sorted(df_gender['name']):\n",
        "  i = i.lower()\n",
        "  gender_pred_nn_df.append([i, gender_pred_dict_regular_nn[i][0],\n",
        "                         gender_pred_dict_regular_nn[i][1], \n",
        "                         gender_pred_dict_regular_nn[i][2],\n",
        "                         gender_pred_dict_regular_nn[i + '_mean_vector'][2], \n",
        "                         gender_pred_dict_formless_nn[i][2], \n",
        "                         gender_pred_dict_formless_nn[i + '_mean_vector'][2], \n",
        "                         gender_pred_dict_formonly_nn[i][2],\n",
        "                         gender_pred_dict_formonly_nn[i + '_mean_vector'][2]])\n",
        "  \n",
        "gender_pred_nn_df = pd.DataFrame(gender_pred_nn_df, \n",
        "                                 columns=['Name', 'NameType', 'TrueRating', \n",
        "                                          'RegularRating', 'RegularMeanVecRating',  \n",
        "                                          'FormlessRating','FormlessMeanVecRating', \n",
        "                                          'FormonlyRating','FormonlyMeanVecRating'])\n",
        "\n",
        "gender_pred_nn_df.to_csv(pickle_path + 'gender_pred_nn.csv', index=False)"
      ],
      "metadata": {
        "id": "KxZU3n2jtv2W"
      },
      "id": "KxZU3n2jtv2W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polarity_dict_nn_final = {'regular' : [], 'formless' : [], 'form_only': []} \n",
        "\n",
        "polarity_pred_dict_regular_nn, polarity_pred_dict_formless_nn, \\\n",
        "polarity_pred_dict_formonly_nn = \\\n",
        "neural_network_5_folder(df_polarity, 'rating.mean_valence', 'polarity', \n",
        "                        polarity_dict_nn_final, 256, 0.5)\n",
        "\n",
        "pickle_polarity_nn_final = open(pickle_path + \"polarity_nn_final.pickle\", \"wb\") # Save the dictionary to a pickle\n",
        "pickle.dump(polarity_dict_nn_final, pickle_polarity_nn_final)\n",
        "pickle_polarity_nn_final.close()"
      ],
      "metadata": {
        "id": "1Lv1Dqcu6n-9"
      },
      "id": "1Lv1Dqcu6n-9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polarity_pred_nn_df = []\n",
        "\n",
        "for i in sorted(df_polarity['name']):\n",
        "  i = i.lower()\n",
        "  polarity_pred_nn_df.append([i, polarity_pred_dict_regular_nn[i][0],\n",
        "                              polarity_pred_dict_regular_nn[i][1], \n",
        "                              polarity_pred_dict_regular_nn[i][2],\n",
        "                              polarity_pred_dict_regular_nn[i + '_mean_vector'][2], \n",
        "                              polarity_pred_dict_formless_nn[i][2], \n",
        "                              polarity_pred_dict_formless_nn[i + '_mean_vector'][2], \n",
        "                              polarity_pred_dict_formonly_nn[i][2],\n",
        "                              polarity_pred_dict_formonly_nn[i + '_mean_vector'][2]])\n",
        "  \n",
        "polarity_pred_nn_df = pd.DataFrame(polarity_pred_nn_df, \n",
        "                                   columns=['Name', 'NameType', 'TrueRating', \n",
        "                                            'RegularRating', 'RegularMeanVecRating',  \n",
        "                                            'FormlessRating','FormlessMeanVecRating', \n",
        "                                            'FormonlyRating','FormonlyMeanVecRating'])\n",
        "\n",
        "polarity_pred_nn_df.to_csv(pickle_path + 'polarity_pred_nn.csv', index=False)"
      ],
      "metadata": {
        "id": "WzWlpaaR6caY"
      },
      "id": "WzWlpaaR6caY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots Playing Ground"
      ],
      "metadata": {
        "id": "hpxfnGBHNzvY"
      },
      "id": "hpxfnGBHNzvY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideas (I'm not listing possible tables for the results, cause those seem like no-brainers to me):\n",
        "\n",
        "- Dimension Y-axis vs Dimension X-axis, real vs predicted, ANN vs ElasticNet\n",
        "- Histogram of predicted 'dimensionness' per dimension & regression type\n",
        "- Histogram of MAE per name type, per dimension & regression type\n",
        "- Real vs predicted regression per dimension & name type (one of these can be done with different colours/lines, while the other can be done with left/right)\n",
        "- Distribution of dimensionness for the predicted (per regression type) and the real --> see if they follow similar distributions!\n",
        "- Schematic depiction of the experimental design (as done by Giovanni in one of his papers)\n",
        "\n",
        "- **ONLY WHEN ENOUGH TIME**: Permutation testing to find the most influential n-grams per dimension & regression type\n",
        "\n",
        "Problems:\n",
        "- 170 names, how do I choose 'exemplary ones'? \n",
        "- How to combine dimenions / regression types / real vs predicted, while keeping things interpretable \n",
        "  - Scola: old/male = left, young/female = right, dimensions = colour\n",
        "- \n"
      ],
      "metadata": {
        "id": "z_Cw_r9mCnuM"
      },
      "id": "z_Cw_r9mCnuM"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fy7g0XZbCkfn"
      },
      "id": "fy7g0XZbCkfn",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "MA Thesis Semantics of Names.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}