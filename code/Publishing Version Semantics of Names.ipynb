{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "22531ca9-e756-46d1-b70a-3e471d5c92a8",
      "metadata": {
        "id": "22531ca9-e756-46d1-b70a-3e471d5c92a8"
      },
      "source": [
        "# Publishing Version of my Master's Thesis on the Semantics of (made-up) Names\n",
        "<br>\n",
        "\n",
        "\n",
        "### Author: Aron Joosse\n",
        "### Supervisor: Giovanni Cassani\n",
        "### Institution: Tilburg University\n",
        "### Grade Acquired: 9"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6549cc30-115e-4d41-a905-85e232d32540",
      "metadata": {
        "id": "6549cc30-115e-4d41-a905-85e232d32540"
      },
      "source": [
        "# Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext --progress-bar off\n",
        "!pip install -U spacy --progress-bar off\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp-pI6xvDejd",
        "outputId": "92b48209-79a1-4cff-cda1-af87d83a34b0"
      },
      "id": "cp-pI6xvDejd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3145124 sha256=de7ec266eee87ede4abb98f45ad411d0162a0ce8a1aadd831f1b81f9bf8d2db2\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.3.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.7.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.17)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.6.15)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e01d4f6-9943-40fb-9f85-ca2aa53b4650",
      "metadata": {
        "id": "9e01d4f6-9943-40fb-9f85-ca2aa53b4650"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Preprocessing\n",
        "import re\n",
        "import os\n",
        "import spacy\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "\n",
        "# FastText\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "\n",
        "import random\n",
        "\n",
        "# MEN and SimLex Benchmarks\n",
        "from os import listdir\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ElasticNet and ANN\n",
        "import sklearn\n",
        "from sklearn.model_selection import cross_val_score, RepeatedKFold, train_test_split, KFold, LeaveOneOut, StratifiedKFold\n",
        "from sklearn.metrics import mean_absolute_error, accuracy_score, mean_squared_error, median_absolute_error\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import absolute\n",
        "from numpy.random import seed\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import time\n",
        "\n",
        "from statistics import median\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from tensorflow.keras.layers import MaxPooling2D, MaxPooling1D, Conv2D, Conv1D, Bidirectional\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f1ffd0-4597-433a-b9c7-bb2423556386",
      "metadata": {
        "id": "92f1ffd0-4597-433a-b9c7-bb2423556386"
      },
      "source": [
        "# Data Import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Getting the list of madeup names:\n",
        "drive.mount(\"/content/drive\", force_remount=True) \n",
        "ratings_csv = pd.read_csv(\"drive/MyDrive/Thesis/Data/giovanni_email_data/avgRatings_annotated.csv\",\n",
        "                          usecols = [\"name\", \"name_type\"])                      ## Importing the names and name types\n",
        "\n",
        "ratings_csv.head(10)\n",
        "\n",
        "madeup_names = []\n",
        "\n",
        "for i in ratings_csv.index:                                                     ## Only choosing madeup names so I can filter them out of the FT vocab\n",
        "  if ratings_csv[\"name_type\"][i] == \"madeup\":\n",
        "    madeup_names.append(str(ratings_csv[\"name\"][i]))\n",
        "\n",
        "madeup_names_lower = list(map(lambda x: x.lower(), madeup_names))               ## Lowercasting the names since my entire vocab will be lowercast\n",
        "\n",
        "print(madeup_names[:5])\n",
        "print(len(madeup_names))\n",
        "print(madeup_names_lower[:5])\n",
        "print(len(madeup_names_lower))"
      ],
      "metadata": {
        "id": "0AUEyOhEpf74",
        "outputId": "5e1c821b-e590-4e3a-8eaa-df88da15dcef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0AUEyOhEpf74",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['Alastor', 'Alecto', 'Amabala', 'Araminta', 'Arcturus']\n",
            "60\n",
            "['alastor', 'alecto', 'amabala', 'araminta', 'arcturus']\n",
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"drive/My Drive/Thesis/Data/CoCA/Text/\"                                  ## These are the paths to easily export/import my dicts, txts, models, and pickles\n",
        "dict_path = \"drive/My Drive/Thesis/Data/CoCA/dict_pickles/\"\n",
        "unclean_path = path + \"texts_combined/all_texts_combined.txt\"\n",
        "model_path = \"drive/My Drive/Thesis/Data/CoCA/models/\"\n",
        "pickle_path = \"drive/MyDrive/Thesis/Data/fastText and others/\"\n",
        "norms_path = \"drive/My Drive/Thesis/Data/Norms/\"\n",
        "csv_path = \"drive/My Drive/Thesis/Data/CSV/\""
      ],
      "metadata": {
        "id": "UwhQ--sVUGHl"
      },
      "id": "UwhQ--sVUGHl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "34910b3e-c2f9-4d54-983d-e75c2127f4db",
      "metadata": {
        "id": "34910b3e-c2f9-4d54-983d-e75c2127f4db"
      },
      "source": [
        "## COCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf7f61e8-e967-4ab9-895e-6fb974df4fc4",
      "metadata": {
        "id": "bf7f61e8-e967-4ab9-895e-6fb974df4fc4"
      },
      "outputs": [],
      "source": [
        "unclean_corpus = open(unclean_path).read()                                      ## Importing the entire coca"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(unclean_corpus))                                                      ## Showing the length and first 100 characters of the coca\n",
        "print(unclean_corpus[:100])"
      ],
      "metadata": {
        "id": "cuIyN1lBMe05",
        "outputId": "486626ae-0cf2-45ff-d70e-eafae963a54c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cuIyN1lBMe05",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2977527143\n",
            "@@4170367 Headnote # A puzzle has long pervaded the criminal law : why are two offenders who commit \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a35ba177-63c9-43e0-8eb0-cc41ada452d6",
      "metadata": {
        "id": "a35ba177-63c9-43e0-8eb0-cc41ada452d6"
      },
      "source": [
        "## Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efb0d50d-543f-4483-9b58-83812e4c7418",
      "metadata": {
        "id": "efb0d50d-543f-4483-9b58-83812e4c7418",
        "outputId": "6040b009-3178-44ab-f6cb-77b777903364",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "119\n",
            "179\n",
            "63\n",
            "       name  rating.mean_age    age name_type\n",
            "0  Adelaide        -0.617647    old      real\n",
            "2  Alasdair        18.709677  young      real\n",
            "3   Alastor        13.812500    old    madeup\n",
            "4    Alecto         3.593750    old    madeup\n",
            "5     Alice       -13.969697  young      real 119\n",
            "       name  rating.mean_gender  gender name_type\n",
            "0  Adelaide           45.727273  female      real\n",
            "1   Adelina           47.771429  female      real\n",
            "2  Alasdair          -35.657143    male      real\n",
            "3   Alastor          -38.833333    male    madeup\n",
            "4    Alecto          -35.722222  female    madeup 179\n",
            "        name  rating.mean_valence polarity name_type\n",
            "1    Adelina            31.621622      bad      real\n",
            "7    Amabala             5.935484     good    madeup\n",
            "8      Apple            32.444444     good   talking\n",
            "11  Arcturus           -11.166667     good    madeup\n",
            "13   Arobynn             7.645161      bad    madeup 63\n"
          ]
        }
      ],
      "source": [
        "### Read CSV File and Delete Unimportant Columns (i.e., everything that isn't the name, name type, rating, or the author's choice)\n",
        "\n",
        "### This is input for the FT model, which itself is the input for the ElasticNet and ANN regressions\n",
        "\n",
        "names_ratings = read_csv(\"drive/MyDrive/Thesis/Data/giovanni_email_data/avgRatings_annotated.csv\")\n",
        "\n",
        "#print(names_ratings.head())\n",
        "\n",
        "print(names_ratings['rating.mean_age'].notna().sum())                           ## Choosing only those rows where all columns are not NA\n",
        "print(names_ratings['rating.mean_gender'].notna().sum())\n",
        "print(names_ratings['rating.mean_valence'].notna().sum())\n",
        "\n",
        "df_age = names_ratings.loc[names_ratings['rating.mean_age'].notna(), ['name', 'rating.mean_age', 'age', 'name_type']]   ## Choosing the relevant columns\n",
        "print(df_age.head(), len(df_age))\n",
        "\n",
        "df_gender = names_ratings.loc[names_ratings['rating.mean_gender'].notna(), ['name', 'rating.mean_gender', 'gender', 'name_type']]\n",
        "print(df_gender.head(), len(df_gender))\n",
        "\n",
        "df_polarity = names_ratings.loc[names_ratings['rating.mean_valence'].notna(), ['name', 'rating.mean_valence', 'polarity', 'name_type']]\n",
        "print(df_polarity.head(), len(df_polarity))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b17c25-4631-4971-8c9e-9b13b9322a08",
      "metadata": {
        "id": "45b17c25-4631-4971-8c9e-9b13b9322a08"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c46490a-70a1-41ae-8f63-ccf7fb08da08",
      "metadata": {
        "id": "6c46490a-70a1-41ae-8f63-ccf7fb08da08"
      },
      "source": [
        "\n",
        "## Cleaning Corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading the English spacy pipeline and removing stopwords (since we are interested in gender bias, it's best to leave these words in)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.max_length = 10000000000\n",
        "\n",
        "nlp.Defaults.stop_words.remove('him')\n",
        "nlp.Defaults.stop_words.remove('her')\n",
        "nlp.Defaults.stop_words.remove('hers')\n",
        "nlp.Defaults.stop_words.remove('his')\n",
        "nlp.Defaults.stop_words.remove('he')\n",
        "nlp.Defaults.stop_words.remove('she')\n",
        "nlp.Defaults.stop_words.remove('himself')\n",
        "nlp.Defaults.stop_words.remove('herself')"
      ],
      "metadata": {
        "id": "xqknf8oNouf3"
      },
      "id": "xqknf8oNouf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_corpus_sentenced(data, corpus_dict, index):\n",
        "  ## Input: \n",
        "  # - Data = A (very) large string of corpus text\n",
        "  # - Corpus_dict = a dictionary to store individual sentences in\n",
        "  # - Index = the last index from the previous batch\n",
        "\n",
        "  ## Process: \n",
        "  # Remove all unwanted tokens, and store individual sentences in the dictionary\n",
        "\n",
        "  ## Output: \n",
        "  # - The dictionary of preprocessed sentences\n",
        "  # - The last sentence index, for the next batch to continue with (so that the order of the sentences is kept)\n",
        "\n",
        "  # Tokenization\n",
        "  with nlp.select_pipes(disable=[\"lemmatizer\", \"tok2vec\", \"tagger\", \"parser\"]):\n",
        "    nlp.enable_pipe(\"senter\")                                                   ## Helps with better segmenting into sentences\n",
        "    doc = nlp(data)\n",
        "\n",
        "  sentence = \"\"                                                                 ## Initialize an empty sentence\n",
        "\n",
        "  for token in doc:\n",
        "    if token.is_sent_start is True:                                             ## If token is the star of the sentence, add the previous sentence to the dictionary\n",
        "      if sentence == \"\":                                                        ## and create a new, clean sentence\n",
        "        continue\n",
        "      else:\n",
        "        corpus_dict[index] = sentence\n",
        "        sentence = \"\"\n",
        "        index += 1\n",
        "    \n",
        "    if token.is_upper is True:                                                  ## Remove all full-caps words\n",
        "      continue\n",
        "    elif token.is_stop is True:                                                 ## Remove all stopwords\n",
        "      continue\n",
        "    elif str(token).lower() in madeup_names_lower:                              ## Remove all words that are in my list of made-up names\n",
        "      continue\n",
        "    elif token.is_alpha:                                                        ## If the token has passed all previous tests, and it consists only of alphabetic\n",
        "      sentence += str(token).lower() + \" \"                                      ## characters, lowercast it and add an extra space to the end; continue to the next\n",
        "                                                                                ## token\n",
        "  return corpus_dict, index"
      ],
      "metadata": {
        "id": "_GxXQaOBOa9N"
      },
      "id": "_GxXQaOBOa9N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def corpus_dict_maker(data, start, end, index):\n",
        "  ## Input: \n",
        "  # - Data = The entire uncleaned corpus\n",
        "  # - Start = The character index to indicate the start of the current batch to preprocess\n",
        "  # - End = The character index to indicate the end of the current batch to preprocess\n",
        "  # - Index = The sentence index from the previous batch, to keep track of the number and order of the sentences\n",
        "\n",
        "  ## Process: \n",
        "  # Preprocess the corpus in batches, since there was not enough RAM to preprocess the entire corpus at once\n",
        "  # So, for every batch, all the characters between the start index and the end index are fed into the clean_corpus_sentenced() function\n",
        "  # and then this dictionary of sentences is saved as a pickle to Google drive\n",
        "\n",
        "  ## Output: \n",
        "  # Nothing; except that the sentence index is printed, which is used as input for the next batch (this was printed, so that it couldn't be lost if the \n",
        "  # runtime would disconnect (which it sadly did very often))\n",
        "\n",
        "  drive.mount(\"/content/drive\", force_remount=True)                             ## Connect to google drive\n",
        "  \n",
        "  corpus_dict = {}                                                              ## Create empty dictionary\n",
        "\n",
        "  prev_i = (start-2)*1000000                                                    ## Start with preprocessing the two million characters before the previous index\n",
        "                                                                                ## since the next range only preprocesses up to but not including the 'end' index\n",
        "\n",
        "  for i in range(start, end, 2):                                                ## In batches of 2 million characters, feed the batch to clean_corpus_sentenced()\n",
        "    print(i)                                                                   \n",
        "    i *= 1000000\n",
        "    corpus_dict, index = clean_corpus_sentenced(data[prev_i:i],\n",
        "                                                corpus_dict,\n",
        "                                                index)\n",
        "    prev_i = i\n",
        "  \n",
        "  if prev_i == 2976000000:                                                      ## Hardcoded; if we get near the end of the corpus, don't preprocess in a batch of\n",
        "    corpus_dict, index = clean_corpus_sentenced(data[prev_i:],                  ## 2 million characters (we would get an out of range error), but rather just \n",
        "                                                corpus_dict,                    ## preprocess the remaining characters, however many that may be\n",
        "                                                index)\n",
        "\n",
        "  print(index)\n",
        "\n",
        "  pickle_out = open(dict_path + \"corpus_dict_until_\" + str(end) + \".pickle\", \"wb\")  ## Save the dictionary as a pickle\n",
        "  pickle.dump(corpus_dict, pickle_out)\n",
        "  pickle_out.close()\n",
        "\n",
        "  drive.flush_and_unmount()                                                     ## Flush the pickle to my drive\n",
        "  print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n"
      ],
      "metadata": {
        "id": "pt5GFTOnoKSQ"
      },
      "id": "pt5GFTOnoKSQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### All of the individual batches:"
      ],
      "metadata": {
        "id": "pFnzewRdf6LG"
      },
      "id": "pFnzewRdf6LG"
    },
    {
      "cell_type": "code",
      "source": [
        "## doing it in batches to \n",
        "## (1) make it possible in terms of time and the Google afk-checker captcha pop-up, and \n",
        "## (2) to not blow out the RAM and have it break down\n",
        "\n",
        "#corpus_dict_maker(unclean_corpus, 2, 500, 0)                   \n",
        "#corpus_dict_maker(unclean_corpus, 500, 640, 3217086)           \n",
        "#corpus_dict_maker(unclean_corpus, 640, 760, 4232218)           \n",
        "#corpus_dict_maker(unclean_corpus, 760, 800, 5439287)           \n",
        "#corpus_dict_maker(unclean_corpus, 800, 900, 5888161)\n",
        "#corpus_dict_maker(unclean_corpus, 900, 980, 7020129)\n",
        "#corpus_dict_maker(unclean_corpus, 980, 1150, 7891661)\n",
        "#corpus_dict_maker(unclean_corpus, 1150, 1200, 9903820) \n",
        "#corpus_dict_maker(unclean_corpus, 1200, 1204, 10502592)"
      ],
      "metadata": {
        "id": "UnBc-d9pjKgx"
      },
      "id": "UnBc-d9pjKgx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (1202)*1000000\n",
        "#i = 1203*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            10547544)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(1203) + \"_post_1204\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################\n",
        "\n",
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (1203)*1000000\n",
        "#i = 1204*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            index + 1)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(1204) + \"_post_1204\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################\n",
        "\n",
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (1204)*1000000\n",
        "#i = 1205*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            10580543)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(1205) + \"_post_1204\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################\n",
        "\n",
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (1205)*1000000\n",
        "#i = 1206*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            index + 1)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(1206) + \"_post_1204\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "057QoMHol9Tb"
      },
      "id": "057QoMHol9Tb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#corpus_dict_maker(unclean_corpus, 1208, 1300, 10611881)\n",
        "#corpus_dict_maker(unclean_corpus, 1300, 1500, 11486498)\n",
        "#corpus_dict_maker(unclean_corpus, 1500, 1750, 13172710)\n",
        "#corpus_dict_maker(unclean_corpus, 1750, 1846, 15332847)\n",
        "#corpus_dict_maker(unclean_corpus, 1846, 1848, 16123425)\n",
        "#corpus_dict_maker(unclean_corpus, 1848, 1850, 16147433)\n",
        "#corpus_dict_maker(unclean_corpus, 1850, 1900, 16172965)\n",
        "#corpus_dict_maker(unclean_corpus, 1900, 1968, 16855832)\n",
        "#corpus_dict_maker(unclean_corpus, 1968, 1970, 17790964)\n",
        "#corpus_dict_maker(unclean_corpus, 1970, 2000, 17819821)\n",
        "#corpus_dict_maker(unclean_corpus, 2000, 2022, 18244076)\n",
        "#corpus_dict_maker(unclean_corpus, 2022, 2024, 18536113)\n",
        "#corpus_dict_maker(unclean_corpus, 2024, 2026, 18558956)\n",
        "#corpus_dict_maker(unclean_corpus, 2026, 2068, 18583534)\n",
        "#corpus_dict_maker(unclean_corpus, 2068, 2070, 19154020)\n",
        "#corpus_dict_maker(unclean_corpus, 2070, 2100, 19179335)\n",
        "#corpus_dict_maker(unclean_corpus, 2100, 2166, 19598984)\n",
        "#corpus_dict_maker(unclean_corpus, 2166, 2168, 20488725)\n",
        "#corpus_dict_maker(unclean_corpus, 2168, 2188, 20524278)"
      ],
      "metadata": {
        "id": "fWVWwqEAmMnb"
      },
      "id": "fWVWwqEAmMnb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (2186)*1000000\n",
        "#i = 2187*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            20779021)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(2187) + \"_post_2188\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################\n",
        "\n",
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (2187)*1000000\n",
        "#i = 2188*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            index + 1)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(2188) + \"_post_2188\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "rzysL5YBbz1M"
      },
      "id": "rzysL5YBbz1M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#corpus_dict_maker(unclean_corpus, 2190, 2200, 20812123)\n",
        "#corpus_dict_maker(unclean_corpus, 2200, 2310, 20945992)\n",
        "#corpus_dict_maker(unclean_corpus, 2310, 2312, 22397914)\n",
        "#corpus_dict_maker(unclean_corpus, 2312, 2400, 22424124)\n",
        "#corpus_dict_maker(unclean_corpus, 2400, 2600, 23465826)\n",
        "#corpus_dict_maker(unclean_corpus, 2600, 2800, 25199888)\n",
        "#corpus_dict_maker(unclean_corpus, 2800, 2977, 26938737)"
      ],
      "metadata": {
        "id": "tyV6gFTWb0VJ"
      },
      "id": "tyV6gFTWb0VJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This block of code opens all of the corpus dict pickles, and combines them together\n",
        "## into one big dictionary: corpus_dict_complete\n",
        "\n",
        "file_number = 1\n",
        "file_list = []\n",
        "for file_name in os.listdir(dict_path):                                         ## Locate all dicts in the folder\n",
        "  with open(dict_path + str(file_name), 'rb') as f:\n",
        "    exec(\"dict_\" + str(file_number) + \" = \" + \"pickle.load(f)\")\n",
        "    file_list.append(\"dict_\" + str(file_number))    \n",
        "    file_number += 1\n",
        "  \n",
        "corpus_dict_complete = {}\n",
        "for file_name in file_list:                                                     \n",
        "  corpus_dict_complete = {**corpus_dict_complete, **globals()[file_name]}       ## Add the contents of the dicts to dict_complete\n",
        "  del globals()[file_name]\n",
        "\n",
        "#print(len(corpus_dict_complete))\n",
        "#del file_number\n",
        "#del file_list\n",
        "\n",
        "#pickle_out = open(dict_path + \"corpus_dict_complete.pickle\", \"wb\")             ## Create a new pickle\n",
        "#pickle.dump(corpus_dict_complete, pickle_out)\n",
        "#del corpus_dict_complete\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()                                                      ## Flush the pickle to my drive\n",
        "#print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "HCY3IGTxeuXN"
      },
      "id": "HCY3IGTxeuXN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This block of code opens the corpus_dict_complete pickle, and loops through the keys by index (from lowest to highest).\n",
        "## So, we loop through every sentence, in order. These are stored in two different .txt files, one where the sentence structure is remained \n",
        "## (i.e., between every sentence, we add a newline character), and one that's unsentenced (i.e., no newline character between sentences).\n",
        "\n",
        "with open(dict_path + \"corpus_dict_complete.pickle\", \"rb\") as d:                ## Open corpus_dict_complete\n",
        "  corpus_dict_complete = pickle.load(d)\n",
        "\n",
        "  with open(path + \"cleaned_sentenced_corpus_complete.txt\", \"w\") as f:          ## Open sentenced corpus .txt file\n",
        "    for key in sorted(corpus_dict_complete):\n",
        "      if len(str(corpus_dict_complete[key]).split()) < 2:                       ## Remove sentences with only 1 word (since there's no 'context' in that case)\n",
        "        continue\n",
        "      else:\n",
        "        if str(corpus_dict_complete[key])[:2] in [\"m \", \"p \", \"s \"]:            ## I have to add this, because based on manual inspection, a significant amount of \n",
        "          f.write(str(corpus_dict_complete[key])[2:] + \"\\n\")                    ## sentences start with just a \"p\", \"m\", or \"s\"\n",
        "        else:\n",
        "          f.write(str(corpus_dict_complete[key]) + \"\\n\")\n",
        "\n",
        "  with open(path + \"cleaned_unsentenced_corpus_complete.txt\", \"w\") as f2:       ## Open unsentenced corpus .txt file\n",
        "    for key in sorted(corpus_dict_complete): \n",
        "      if len(str(corpus_dict_complete[key])) < 4:                               ## Remove sentences with less than 4 characers, since based on visual inspection, I\n",
        "        continue                                                                ## saw that such sentences are mostly nonsense (i.e., not real words)\n",
        "      else:\n",
        "        if str(corpus_dict_complete[key])[:2] in [\"m \", \"p \", \"s \"]:\n",
        "          f2.write(str(corpus_dict_complete[key])[2:])\n",
        "        else:\n",
        "          f2.write(str(corpus_dict_complete[key]))\n",
        "  \n",
        "drive.flush_and_unmount()                                                       ## Flush to drive\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "YkdDjN1AodpH",
        "outputId": "b2b8493b-8584-459f-a74c-6c023c7743f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YkdDjN1AodpH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All changes made in this colab session should now be visible in Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09e790a-e08f-4144-b01e-25af29a8816f",
      "metadata": {
        "id": "a09e790a-e08f-4144-b01e-25af29a8816f"
      },
      "source": [
        "## Training fastText and Validating on MEN and SimLex999 + Training W2V"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fastText"
      ],
      "metadata": {
        "id": "Mk9EPG5rwc0H"
      },
      "id": "Mk9EPG5rwc0H"
    },
    {
      "cell_type": "code",
      "source": [
        "# def fasttext_tuner(data_type):                                                ## I have chosen Skipgram, I won't play around with epochs or learning rate\n",
        "#   for dimensionality in [100, 300]:                                           ## I'm first checking data type and dimensionality, and choosing the most promising combination.\n",
        "#     for window_size in [2, 3, 4, 5, 6, 7]:                                    ## Then I'll check window size and choose the 3 most promising ones\n",
        "#       for min_n in [1, 2, 3]:                                                 ## Then I want to check min_n 2 and 1 to see whether adding n-gram size of 1 makes any sense\n",
        "#         for max_n in [5, 6, 7]:                                               ## Lastly, I will iterate through the 3 * 2 * 3 most promising models, finally choosing the best one and then double checking that with\n",
        "#           model = fasttext.train_unsupervised(input = path + \"cleaned_\" +     ## the 100/300 sentenced/unsentenced options, just to be sure! In total, this means training 4 + 5 + ~16 + 3 = ~30 models instead of 216\n",
        "#                                               data_type + \n",
        "#                                               \"_corpus_complete.txt\",\n",
        "#                                               model = \"skipgram\",\n",
        "#                                               dim = dimensionality, \n",
        "#                                               ws = window_size, \n",
        "#                                               minn = min_n,\n",
        "#                                               maxn = max_n)\n",
        "          \n",
        "#           model.save_model(model_path + data_type + \"_dim\" + str(dimensionality) + \n",
        "#                             \"_ws\" + str(window_size) + \"_minn\" + str(min_n) + \n",
        "#                             \"_maxn\" + str(max_n) + \".bin\")\n",
        "          \n",
        "#           print(data_type + \"_dim\" + str(dimensionality) + \"_ws\" + str(window_size) + \n",
        "#                 \"_minn\" + str(min_n) + \"_maxn\" + str(max_n))\n",
        "\n",
        "#           del model\n",
        "\n",
        "#   drive.flush_and_unmount()\n",
        "#   print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "SVSZbTIchM28"
      },
      "id": "SVSZbTIchM28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa3cc3c-e1a4-489b-b0c6-e25bd045783f",
      "metadata": {
        "id": "aaa3cc3c-e1a4-489b-b0c6-e25bd045783f"
      },
      "outputs": [],
      "source": [
        "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "ft = fasttext.load_model('cc.en.300.bin')\n",
        "ft.save_model(model_path + \"pretrained_model.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fasttext_tuner(data_type, dimensionality = 300, window_size = 5, min_n = 3, max_n = 6):\n",
        "  ## Input: \n",
        "  # - Data Type: Sentenced or unsentenced, to load the corpus type to train model\n",
        "  # - Dimensionality: List of dimensionalities\n",
        "  # - Window size: List of window sizes \n",
        "  # - Min_n: List of minimum n-gram sizes \n",
        "  # - Max-n: List of maximum n-gram sizes \n",
        "\n",
        "  ## Process: \n",
        "  # For every combination of input variables, a fastText model is trained and saved\n",
        "\n",
        "  ## Output:\n",
        "  # Nothing; fastText model is trained and saved\n",
        "\n",
        "  for d in dimensionality:                                           \n",
        "    for ws in window_size:                                    \n",
        "      for minn in min_n:                                                 \n",
        "        for maxn in max_n:\n",
        "          drive.mount(\"/content/drive\", force_remount=True)                     ## Mount drive\n",
        "\n",
        "          model = fasttext.train_unsupervised(input = path + \"cleaned_\" + data_type + \"_corpus_complete_without_p_s_m.txt\",   ## Train model\n",
        "                                              model = \"skipgram\", dim = d, ws = ws, minn = minn, maxn = maxn)\n",
        "\n",
        "          model.save_model(model_path + data_type + \"_dim\" + str(d) + \"_ws\" + str(ws) + \"_minn\" + str(minn) + \"_maxn\" + str(maxn) + \".bin\")   ## Save model\n",
        "\n",
        "          print(data_type + \"_dim\" + str(d) + \"_ws\" + str(ws) + \"_minn\" + str(minn) + \"_maxn\" + str(maxn))\n",
        "\n",
        "          drive.flush_and_unmount()                                             ## Flush model to drive\n",
        "          print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "          del model"
      ],
      "metadata": {
        "id": "QCnCgy1QqXmQ"
      },
      "id": "QCnCgy1QqXmQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Trying out some variations of fastText with Data = sentenced & Dimensionality = 100\n",
        "\n",
        "fasttext_tuner(\"sentenced\", [100], [2], [2], [3])\n",
        "fasttext_tuner(\"sentenced\", [100], [2, 3, 4, 5, 6, 7], [2], [5, 6, 7])\n",
        "fasttext_tuner(\"sentenced\", [100], [5], [3], [6])\n",
        "fasttext_tuner(\"sentenced\", [100], [5], [1], [6])\n",
        "\n",
        "## Trying out some variations of fastText with Data = sentenced & Dimensionality = 300\n",
        "\n",
        "fasttext_tuner(\"sentenced\", [300], [3, 4], [2], [5, 6, 7])\n",
        "fasttext_tuner(\"sentenced\", [300], [5], [2], [5, 6])\n",
        "fasttext_tuner(\"sentenced\", [300], [5], [3], [6])\n",
        "fasttext_tuner(\"sentenced\", [300], [3], [3], [5])\n",
        "fasttext_tuner(\"sentenced\", [300], [3], [2], [4])\n",
        "fasttext_tuner(\"sentenced\", [300], [2, 6, 7], [2], [5])\n",
        "\n",
        "## Trying out some variations of fastText with Data = unsentenced \n",
        "\n",
        "fasttext_tuner(\"unsentenced\", [300], [5], [3], [6])\n",
        "fasttext_tuner(\"unsentenced\", [100], [5], [3], [6])"
      ],
      "metadata": {
        "id": "BEfZcRmNqXfi"
      },
      "id": "BEfZcRmNqXfi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Doing it one last time, this time with seeds + an n-gramless model\n",
        "\n",
        "random.seed(17042020)\n",
        "\n",
        "fasttext_tuner('sentenced', [300], [2], [2], [5])\n",
        "\n",
        "random.seed(17042020)\n",
        "\n",
        "fasttext_tuner('sentenced', [300], [2], [0], [0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoucNSJNgsXD",
        "outputId": "97e164f0-bfce-4dea-eeaa-97781078dcfb"
      },
      "id": "FoucNSJNgsXD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MEN and SimLex Benchmarks"
      ],
      "metadata": {
        "id": "auG91yFzwfxP"
      },
      "id": "auG91yFzwfxP"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarity(model, word1, word2, sub_lexical = False):\n",
        "  # Input:\n",
        "  # - Model = the model that is used to get embeddings for word1 and word2\n",
        "  # - Word1 = the first word\n",
        "  # - Word2 = the second word\n",
        "\n",
        "  # Process: \n",
        "  # Given the embeddings for word1 and word2 derived from the model, calculate\n",
        "  # the cosine similarity between these words\n",
        "\n",
        "  # Output:\n",
        "  # The cosine similarity between word1 and word2 given the current model\n",
        "    if sub_lexical == False:\n",
        "      e1 = model[word1]\n",
        "      e2 = model[word2]\n",
        "      s = cosine_similarity(e1.reshape(1, -1), e2.reshape(1, -1))\n",
        "\n",
        "    else:\n",
        "      e1 = fasttext_xifyer_ngram(pd.DataFrame([word1]))\n",
        "      e2 = fasttext_xifyer_ngram(pd.DataFrame([word2]))\n",
        "      s = cosine_similarity(e1.reshape(1, -1), e2.reshape(1, -1))\n",
        "    \n",
        "    return s[0][0]"
      ],
      "metadata": {
        "id": "GVAVzmt0wixs"
      },
      "id": "GVAVzmt0wixs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_correlation_men(norms, model, sub_lexical = False):\n",
        "  # Input:\n",
        "  # - Norms = The MEN similarity scores for word1 and word2\n",
        "  # - Model = the model that will be scored\n",
        "\n",
        "  # Process: \n",
        "  # For every model, note down the true similarity score (as dictated by the MEN\n",
        "  # benchmark) and then estimate the similarity for the current model using the \n",
        "  # compute_similarity() function\n",
        "\n",
        "  # Output:\n",
        "  # The Spearman R between the true and estimated similarities for the current\n",
        "  # model.\n",
        "\n",
        "        true_similarities = []\n",
        "        estimated_similarities = []\n",
        "        if sub_lexical == False:\n",
        "          for _, row in norms.iterrows():\n",
        "              s = compute_similarity(model, row['w1'], row['w2'])\n",
        "\n",
        "              estimated_similarities.append(s)\n",
        "              true_similarities.append(row['sim'])\n",
        "          \n",
        "        else:\n",
        "          for _, row in norms.iterrows():\n",
        "              s = compute_similarity(model, row['w1'], row['w2'], True)\n",
        "\n",
        "              estimated_similarities.append(s)\n",
        "              true_similarities.append(row['sim'])\n",
        "        \n",
        "        return spearmanr(true_similarities, estimated_similarities)[0]"
      ],
      "metadata": {
        "id": "qRPpzYPdzqvD"
      },
      "id": "qRPpzYPdzqvD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_correlation_simlex(norms, model, sub_lexical = False):\n",
        "  # Input:\n",
        "  # - Norms = The SimLex similarity scores for word1 and word2\n",
        "  # - Model = the model that will be scored\n",
        "\n",
        "  # Process: \n",
        "  # For every model, note down the true similarity score (as dictated by the SimLex\n",
        "  # benchmark) and then estimate the similarity for the current model using the \n",
        "  # compute_similarity() function\n",
        "\n",
        "  # Output:\n",
        "  # The Spearman R between the true and estimated similarities for the current\n",
        "  # model.\n",
        "\n",
        "        true_similarities = []\n",
        "        estimated_similarities = []\n",
        "        if sub_lexical == False:\n",
        "          for _, row in norms.iterrows():\n",
        "              s = compute_similarity(model, row['word1'], row['word2'])\n",
        "\n",
        "              estimated_similarities.append(s)\n",
        "              true_similarities.append(row['SimLex999'])\n",
        "\n",
        "        else:\n",
        "          for _, row in norms.iterrows():\n",
        "              s = compute_similarity(model, row['word1'], row['word2'], True)\n",
        "\n",
        "              estimated_similarities.append(s)\n",
        "              true_similarities.append(row['SimLex999'])\n",
        "        \n",
        "        return spearmanr(true_similarities, estimated_similarities)[0]"
      ],
      "metadata": {
        "id": "dcsEKh_KzrcZ"
      },
      "id": "dcsEKh_KzrcZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "men_norms = pd.read_csv(norms_path + 'norms.dev.csv', sep = ' ')                # Read the MEN benchmark CSV file\n",
        "\n",
        "combi_score = {}\n",
        "men_dict = {}\n",
        "\n",
        "#for model in os.listdir(model_path):                                            # For every saved model, calculate the scores on the MEN benchmarks\n",
        "#  men_dict[str(model)] = compute_correlation_men(men_norms, model)\n",
        "#  combi_score[str(model)] = compute_correlation_men(men_norms, model)\n",
        "\n",
        "#men_dict['pretrained_model'] = compute_correlation_men(men_norms, ft)           # Calculating the scores for the baseline (pretrained) model\n",
        "#combi_score['pretrained_model'] = compute_correlation_men(men_norms, ft)\n",
        "\n",
        "men_dict['ngram'] = compute_correlation_men(men_norms, model = None, sub_lexical = True)\n",
        "combi_score['ngram'] = compute_correlation_men(men_norms, model = None, sub_lexical = True)"
      ],
      "metadata": {
        "id": "kjTCodSdt7OY"
      },
      "id": "kjTCodSdt7OY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simlex_norms = pd.read_csv(norms_path + 'SimLex-999.txt', sep = '\\t')           # Read the SimLex benchmark text file\n",
        "\n",
        "simlex_dict = {}\n",
        "\n",
        "#for model in os.listdir(model_path):                                            # For every saved model, calculate the scores on the SimLex benchmarks\n",
        "#  simlex_dict[str(model)] = compute_correlation_simlex(simlex_norms, model)\n",
        "#  combi_score[str(model)] = compute_correlation_simlex(simlex_norms, model)\n",
        "\n",
        "#simlex_dict['pretrained_model')] = compute_correlation_simlex(simlex_norms, ft) # Calculating the scores for the baseline (pretrained) model\n",
        "#combi_score['pretrained_model'] = compute_correlation_simlex(simlex_norms, ft)\n",
        "\n",
        "simlex_dict['ngram'] = compute_correlation_simlex(simlex_norms, model = None, sub_lexical = True)\n",
        "combi_score['ngram'] += compute_correlation_simlex(simlex_norms, model = None, sub_lexical = True)"
      ],
      "metadata": {
        "id": "lOuL_V6Kt8zt"
      },
      "id": "lOuL_V6Kt8zt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Ngram model MEN score:', men_dict['ngram'])\n",
        "print('Ngram model SimLex score:', simlex_dict['ngram'])\n",
        "print('Ngram model combi score:', combi_score['ngram'])"
      ],
      "metadata": {
        "id": "G03yyE9FUnrq",
        "outputId": "daa2048a-664c-471c-feda-2eb6743f7a79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "G03yyE9FUnrq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngram model MEN score: 0.7690609507514868\n",
            "Ngram model SimLex score: 0.4097994610087465\n",
            "Ngram model combi score: 1.1788604117602333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_men = open(pickle_path + \"men_scores.pickle\", \"wb\")                       # Save the MEN scores as a pickle\n",
        "pickle.dump(men_dict, pickle_men)\n",
        "pickle_men.close()\n",
        "\n",
        "pickle_simlex = open(pickle_path + \"simlex_scores.pickle\", \"wb\")                 # Save the SimLex scores as a pickle\n",
        "pickle.dump(simlex_dict, pickle_simlex)\n",
        "pickle_simlex.close()\n",
        "\n",
        "pickle_combi = open(pickle_path + \"combi_scores.pickle\", \"wb\")                   # Save the combined scores as a pickle\n",
        "pickle.dump(combi_score, pickle_combi)\n",
        "pickle_combi.close()"
      ],
      "metadata": {
        "id": "dVz29qVhzvbh"
      },
      "id": "dVz29qVhzvbh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sorted(men_dict, key = men_dict.get, reverse = True):\n",
        "    print(i, men_dict[i])"
      ],
      "metadata": {
        "id": "an2nkEf9zv2u"
      },
      "id": "an2nkEf9zv2u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sorted(simlex_dict, key = simlex_dict.get, reverse = True):\n",
        "    print(i, simlex_dict[i])"
      ],
      "metadata": {
        "id": "YzNq-FeJz-CS"
      },
      "id": "YzNq-FeJz-CS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sorted(combi_score, key = combi_score.get, reverse = True):\n",
        "    print(i, combi_score[i])"
      ],
      "metadata": {
        "id": "QNqql7wIz-hs"
      },
      "id": "QNqql7wIz-hs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Model"
      ],
      "metadata": {
        "id": "2MhYKRfpPEVJ"
      },
      "id": "2MhYKRfpPEVJ"
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.load_model(model_path + 'sentenced_dim300_ws2_minn2_maxn5.bin')\n",
        "model_subwordless = fasttext.load_model(model_path + 'sentenced_dim300_ws2_minn0_maxn0.bin')"
      ],
      "metadata": {
        "id": "e64L1cxtPGmv",
        "outputId": "19d43ce8-2e76-419a-a271-8faba92e6b07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "e64L1cxtPGmv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Regressions"
      ],
      "metadata": {
        "id": "caLUM8tC6_BY"
      },
      "id": "caLUM8tC6_BY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xifyers"
      ],
      "metadata": {
        "id": "Qu1YAabGVPJ7"
      },
      "id": "Qu1YAabGVPJ7"
    },
    {
      "cell_type": "code",
      "source": [
        "def fasttext_xifyer(input_data):\n",
        "  # Input:\n",
        "  # - input_data = dataframe containing the names of the characters\n",
        "\n",
        "  # Process:\n",
        "  # Convert the character names to embeddings using FT\n",
        "\n",
        "  # Output:\n",
        "  # - df_output = a dataframe of word embeddings; one embedding per character name\n",
        "\n",
        "  df_output = np.zeros((len(input_data), 300))                                  # Create an array of length = number of input rows & width = 300 (because of ft)\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  for row in input_data.iterrows():                                             # For every row in the input data\n",
        "    index = row[0]\n",
        "    name = row[1][0].lower()                                                    # Lowercast the name\n",
        "    df_output[i] = model[name]                                                  # Compute the fastText score of the name, and add it to the output array\n",
        "    i += 1\n",
        "\n",
        "  return df_output"
      ],
      "metadata": {
        "id": "k7lbNPag7CrC"
      },
      "id": "k7lbNPag7CrC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fasttext_xifyer_ngram(input_data):\n",
        "  # Input & Output are exactly the same as fasttext_xifyer\n",
        "\n",
        "  # Process:\n",
        "  # Similar to fasttext_xifyer, however, if the name has a 'surface' form that \n",
        "  # is already present in the model vocabulary, then we only use the embeddings\n",
        "  # for the subwords. I.e., as compared to fasttext_xifyer, this function does \n",
        "  # NOT consider the 'surface' form embedding for the character names\n",
        "\n",
        "  df_output = np.zeros((len(input_data), 300))                                  # Create an array of length = number of input rows & width = 300 (because of ft)\n",
        "\n",
        "  c = 0\n",
        "\n",
        "  for row in input_data.iterrows():                                             # For every row in the input data\n",
        "    name = row[1][0].lower()\n",
        "    if name == model.get_subwords(name)[0][0]:                                  # If the surface form of the name is present in the model vocabulary                                                                        \n",
        "      wordarray = np.zeros(model[name].shape)\n",
        "      subword_counter = 0\n",
        "      for subword_hash in model.get_subwords(name)[1][1:]:\n",
        "        wordarray += model.get_input_vector(subword_hash)\n",
        "        subword_counter += 1\n",
        "      \n",
        "      df_output[c] = wordarray / subword_counter              \n",
        "                                    \n",
        "    else:\n",
        "      df_output[c] = model[name]                                                # Else (if no surface name is present in the model vocabulary), just take the embedding of the name as is\n",
        "    c += 1\n",
        "\n",
        "  return df_output"
      ],
      "metadata": {
        "id": "ii1not4CiDLw"
      },
      "id": "ii1not4CiDLw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fasttext_xifyer_lexical(input_data):\n",
        "  # Input & Output are exactly the same as fasttext_xifyer\n",
        "\n",
        "  # Process:\n",
        "  # Similar to fasttext_xifyer, however, if the name has a 'surface' form that \n",
        "  # is already present in the model vocabulary, then we only use the embeddings\n",
        "  # for the subwords. I.e., as compared to fasttext_xifyer, this function does \n",
        "  # NOT consider the 'surface' form embedding for the character names\n",
        "\n",
        "  df_output = np.zeros((len(input_data), 300))                                  # Create an array of length = number of input rows & width = 300 (because of ft)\n",
        "\n",
        "  c = 0\n",
        "\n",
        "  for row in input_data.iterrows():                                             # For every row in the input data\n",
        "    name = row[1][0].lower()\n",
        "    if name == model.get_subwords(name)[0][0]:                                  # If the surface form of the name is present in the model vocabulary                                                                        \n",
        "      #df_output[c] = model.get_input_vector(model.get_subwords(name)[1][0])                    \n",
        "      df_output[c] = model_subwordless[name]                              \n",
        "    else:\n",
        "      continue\n",
        "      #df_output[c] = mean_vector_subwordless                                    # Else (if no surface name is present in the model vocabulary), just take the mean vector\n",
        "    c += 1\n",
        "\n",
        "  return df_output"
      ],
      "metadata": {
        "id": "c-OqBfJn77PS"
      },
      "id": "c-OqBfJn77PS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean Vectors"
      ],
      "metadata": {
        "id": "LUUBNAjAVUks"
      },
      "id": "LUUBNAjAVUks"
    },
    {
      "cell_type": "code",
      "source": [
        "mean_vector = np.zeros((len(model.words), 300))                                 # Create a mean vector array of length = amount of words in the model vocab, and width = 300\n",
        "\n",
        "i = 0\n",
        "\n",
        "for word in model.words:                          \n",
        "  mean_vector[i] = model.get_word_vector(word)                                  # For every word in the vocab, get it's word embedding vector and add it to the mean_vector array\n",
        "  i += 1\n",
        "\n",
        "mean_vector = np.mean(mean_vector, axis = 0)                                    # Take the mean of the array so that we get a mean_vector of length = 1, and width = 300"
      ],
      "metadata": {
        "id": "SLBg426YYSGw"
      },
      "id": "SLBg426YYSGw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_vector_subwordless = np.zeros((len(model_subwordless.words), 300))         # Create a mean vector array of length = amount of words in the model vocab, and width = 300\n",
        "\n",
        "i = 0\n",
        "\n",
        "for word in model_subwordless.words:                          \n",
        "  mean_vector_subwordless[i] = model_subwordless.get_word_vector(word)          # For every word in the vocab, get it's word embedding vector and add it to the mean_vector array\n",
        "  i += 1\n",
        "\n",
        "mean_vector_subwordless = np.mean(mean_vector_subwordless, axis = 0)            # Take the mean of the array so that we get a mean_vector of length = 1, and width = 300"
      ],
      "metadata": {
        "id": "Jy4WyB7_T3qq"
      },
      "id": "Jy4WyB7_T3qq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search"
      ],
      "metadata": {
        "id": "icxbZNdEm15J"
      },
      "id": "icxbZNdEm15J"
    },
    {
      "cell_type": "code",
      "source": [
        "def nested_neural_network_finder(df, rating, dimension, dictionary):\n",
        "  # Input:\n",
        "  # - df = a dataframe with the name, name_type, and rating for the dimension at  \n",
        "  # hand (i.e., age, gender, or polarity)\n",
        "  # - rating = a string indicating what rating to extract from the df\n",
        "  # - dimension = a string indicating what dimension is considered (i.e., 'age',\n",
        "  # 'gender', or 'polarity')\n",
        "  # - dictionary = an empty dictionary to store the MSE output by fnn_maker() in \n",
        "  # per configuration\n",
        "\n",
        "  # Process:\n",
        "  # Given the df, get 5 train/test splits, and per fold, train a LOOCV NN model\n",
        "  # using fnn_maker() for the combined and ngram data. Then, print the scores.\n",
        "\n",
        "  # Output: \n",
        "  # Nothing, but the MSE for all different configurations in the grid search are\n",
        "  # printed\n",
        "\n",
        "  #### NOTE: For the 'extra' NN grid search, change the list of nodes and \n",
        "  #### dropout values to search and print!! I am not making this an extra \n",
        "  #### function because I already have enough and it's simple to understand.\n",
        "  q=1\n",
        "\n",
        "  skf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=17042020)      # Set up a stratified 5-fold splitter\n",
        "\n",
        "  for train_index, test_index in skf.split(df, df[['name_type']]):              # For every fold (stratified on the name type)\n",
        "    print(\"Fold:\", q, \"Time:\", time.strftime(\"%D:%H:%M:%S\", time.localtime()))\n",
        "    k_fold = KFold(n_splits = len(train_index), shuffle=True, random_state=17042020) # Set up a LOOCV splitter\n",
        "\n",
        "    x_train_unfasttexted = df.iloc[train_index]                                 # Split the data into x_train and x_test\n",
        "    x_test_unfasttexted = df.iloc[test_index]\n",
        "\n",
        "    x_train = fasttext_xifyer(x_train_unfasttexted)                             # Get the word embeddings\n",
        "    x_train_ngram = fasttext_xifyer_ngram(x_train_unfasttexted)\n",
        "    x_train_lexical = fasttext_xifyer_lexical(x_train_unfasttexted)\n",
        "\n",
        "    x_test = fasttext_xifyer(x_test_unfasttexted)\n",
        "    x_test_ngram = fasttext_xifyer_ngram(x_test_unfasttexted)\n",
        "    x_test_ngram = fasttext_xifyer_lexical(x_test_unfasttexted)\n",
        "\n",
        "    y_train = df.iloc[train_index][rating]                                      # Split the data into y_train and y_test\n",
        "    y_test = df.iloc[test_index][rating]\n",
        "\n",
        "    for nested_train_index, nested_test_index in k_fold.split(x_train):         # For every fold in the LOOCV splitter\n",
        "      nested_x_train = x_train[[nested_train_index], :].reshape(len(nested_train_index), 300)   # Get a nested x_train and x_test\n",
        "      nested_x_train_ngram = x_train_ngram[[nested_train_index], :].reshape(len(nested_train_index), 300)\n",
        "      nested_x_train_lexical = x_train_lexical[[nested_train_index], :].reshape(len(nested_train_index), 300)\n",
        "\n",
        "      nested_x_test = x_train[[nested_test_index], :].reshape(len(nested_test_index), 300)\n",
        "      nested_x_test_ngram = x_train_ngram[[nested_test_index], :].reshape(len(nested_test_index), 300)\n",
        "      nested_x_test_lexical = x_train_lexical[[nested_test_index], :].reshape(len(nested_test_index), 300)\n",
        "\n",
        "      nested_y_train = y_train.iloc[nested_train_index]                         # Get a nested y_train and y_test\n",
        "      nested_y_test = y_train.iloc[nested_test_index]\n",
        "\n",
        "      for nodes in [128, 256, 512]:                                        # For every number of nodes considered\n",
        "        for dropout in [0.2, 0.3, 0.4, 0.5, 0.6]:                               # For every initialization of dropout considered\n",
        "          dictionary['combined'][str(nodes)][str(dropout)].append(\n",
        "              fnn_maker(nested_x_train, \n",
        "                        nested_y_train, \n",
        "                        nested_x_test, \n",
        "                        nested_y_test, \n",
        "                        nodes, \n",
        "                        dropout))                                               # Train a NN using the combined data and append the scores to a dict\n",
        "\n",
        "          dictionary['ngram'][str(nodes)][str(dropout)].append(\n",
        "              fnn_maker(nested_x_train_ngram, \n",
        "                        nested_y_train, \n",
        "                        nested_x_test_ngram, \n",
        "                        nested_y_test, \n",
        "                        nodes, \n",
        "                        dropout))                                               # Train a NN using the ngram data and append the scores to a dict\n",
        "                  \n",
        "          dictionary['lexical'][str(nodes)][str(dropout)].append(\n",
        "              fnn_maker(nested_x_train_lexical, \n",
        "                        nested_y_train, \n",
        "                        nested_x_test_lexical, \n",
        "                        nested_y_test, \n",
        "                        nodes, \n",
        "                        dropout))                                               # Train a NN using the ngram data and append the scores to a dict\n",
        "    q += 1\n",
        "  \n",
        "  for nodes in [128, 256, 512, 1024]:                                            # Given all configurations, print the combined and ngram MSE\n",
        "      for dropout in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
        "        ngram_mse = sum(dictionary['ngram'][str(nodes)][str(dropout)]) / len(dictionary['ngram'][str(nodes)][str(dropout)])\n",
        "        lexical_mse = sum(dictionary['lexical'][str(nodes)][str(dropout)]) / len(dictionary['lexical'][str(nodes)][str(dropout)])\n",
        "\n",
        "        print(\"{}, ngram, nodes = {}, dropout = {}, Average MSE = {}\".format(dimension, nodes, dropout, ngram_mse))\n",
        "        print(\"{}, lexical, nodes = {}, dropout = {}, Average MSE = {}\".format(dimension, nodes, dropout, lexical_mse))\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "AtODe-sF9eWP"
      },
      "id": "AtODe-sF9eWP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_dict_nn = {'combined' : {'128': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '256': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '300': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '512': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}}, \n",
        "               'ngram' :{'128': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '256': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '300': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '512': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}},\n",
        "               'lexical' :{'128': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '256': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '300': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '512': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}}} # Initialize a dictionary to store values in\n",
        "\n",
        "nested_neural_network_finder(df_age, 'rating.mean_age', 'Age', age_dict_nn)     # Run the nested neural network finder function"
      ],
      "metadata": {
        "id": "zpZlYlMy9g3-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "1a0e4203-494a-4b3f-f6e8-a9867bec5397"
      },
      "id": "zpZlYlMy9g3-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold: 1 Time: 04/27/22:15:25:25\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f10bbdfbcb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f10bbd508c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d5159430b54a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                             '512': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}}} # Initialize a dictionary to store values in\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mnested_neural_network_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_age\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rating.mean_age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_dict_nn\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Run the nested neural network finder function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-d84627e0f6aa>\u001b[0m in \u001b[0;36mnested_neural_network_finder\u001b[0;34m(df, rating, dimension, dictionary)\u001b[0m\n\u001b[1;32m     70\u001b[0m                         \u001b[0mnested_y_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                         \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                         dropout))                                               # Train a NN using the formless data and append the scores to a dict\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m           dictionary['form_only'][str(nodes)][str(dropout)].append(\n",
            "\u001b[0;32m<ipython-input-14-10f67032857e>\u001b[0m in \u001b[0;36mfnn_maker\u001b[0;34m(x_train, y_train, x_test, y_test, nodes, dropout)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   fnn_model.fit(x_train, y_train, epochs=100, batch_size=len(x_train), \n\u001b[0;32m---> 33\u001b[0;31m                 callbacks=[callback], verbose=0)                                # Fit the model on the train set\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m                                            \u001b[0;31m# Feed the test cases to the model to retrieve predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m-> 2955\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_call_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3292\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3293\u001b[0m           self._function_cache.add(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   3294\u001b[0m                                    graph_function)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3138\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3139\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                     \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0moptional_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautograph_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                     \u001b[0muser_requested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m                 ))\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1008\u001b[0m             run_step, jit_compile=True, experimental_relax_shapes=True)\n\u001b[1;32m   1009\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m   1012\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1311\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2886\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2887\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2888\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3687\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3688\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3689\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3691\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_target_and_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[0;31m# Run backwards pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \"\"\"\n\u001b[1;32m    530\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[0;32m--> 531\u001b[0;31m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/gradients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m       \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     self._assert_valid_dtypes([\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_get_gradients\u001b[0;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;34m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_SquaredDifferenceGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1616\u001b[0m     \u001b[0;31m# The parens ensure that if grad is IndexedSlices, it'll get multiplied by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[0;31m# Tensor (not a number like 2.0) which causes it to convert to Tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1618\u001b[0;31m     \u001b[0mx_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m   if (isinstance(grad, ops.Tensor) and\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mscalar_mul\u001b[0;34m(scalar, x, name)\u001b[0m\n\u001b[1;32m    625\u001b[0m           gen_math_ops.mul(scalar, x.values, name), x.indices, x.dense_shape)\n\u001b[1;32m    626\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6588\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6589\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m-> 6590\u001b[0;31m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   6591\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6592\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    616\u001b[0m             _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    617\u001b[0m                                      \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m                                      param_name=input_name)\n\u001b[0m\u001b[1;32m    619\u001b[0m           \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m           \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allowed_values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mallowed_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallowed_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mallowed_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allowed_values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mallowed_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallowed_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mallowed_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36mas_dtype\u001b[0;34m(type_value)\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtype_value\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m   \"\"\"\n\u001b[0;32m--> 697\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_INTERN_TABLE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gender_dict_nn =  {'combined' : {'128': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '256': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '300': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '512': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}}, \n",
        "               'ngram' :{'128': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '256': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '300': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '512': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}},\n",
        "               'lexical' :{'128': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '256': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '300': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '512': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}}}  # Initialize a dictionary to store values in\n",
        "\n",
        "nested_neural_network_finder(df_gender, 'rating.mean_gender', 'Gender', gender_dict_nn) # Run the nested neural network finder function"
      ],
      "metadata": {
        "id": "jqWVrhwF9jDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abcfb234-33ec-4369-c187-4969cfbc81e0"
      },
      "id": "jqWVrhwF9jDI",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold: 1 Time: 04/27/22:15:26:00\n",
            "Fold: 2 Time: 04/27/22:19:17:26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "polarity_dict_nn =  {'combined' : {'128': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '256': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '300': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '512': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}}, \n",
        "               'ngram' :{'128': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '256': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '300': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '512': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}},\n",
        "               'lexical' :{'128': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '256': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '300': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}, \n",
        "                            '512': {'0.2': [], '0.3': [], '0.4': [], '0.5': [], '0.6': []}}} # Initialize a dictionary to store values in\n",
        "\n",
        "nested_neural_network_finder(df_polarity, 'rating.mean_valence', 'Polarity', polarity_dict_nn) # Run the nested neural network finder function"
      ],
      "metadata": {
        "id": "PA-vgaKE9jwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a3742d-4167-49d6-9027-e44a1fe6cd06"
      },
      "id": "PA-vgaKE9jwU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold: 1 Time: 04/27/22:06:58:16\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f00a32ac680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f00a30e2170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Fold: 2 Time: 04/27/22:07:31:31\n",
            "Fold: 3 Time: 04/27/22:08:05:26\n",
            "Fold: 4 Time: 04/27/22:08:37:19\n",
            "Fold: 5 Time: 04/27/22:09:08:37\n",
            "Polarity, regular, nodes = 128, dropout = 0.2, Average MSE = 321.10995032291936\n",
            "Polarity, formless, nodes = 128, dropout = 0.2, Average MSE = 328.2864228138766\n",
            "Polarity, formonly, nodes = 128, dropout = 0.2, Average MSE = 353.7593838626403\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 128, dropout = 0.3, Average MSE = 339.67209947406394\n",
            "Polarity, formless, nodes = 128, dropout = 0.3, Average MSE = 340.7091010832673\n",
            "Polarity, formonly, nodes = 128, dropout = 0.3, Average MSE = 354.6769563212269\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 128, dropout = 0.4, Average MSE = 347.98369395705856\n",
            "Polarity, formless, nodes = 128, dropout = 0.4, Average MSE = 348.79425140161845\n",
            "Polarity, formonly, nodes = 128, dropout = 0.4, Average MSE = 353.8075563756581\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 128, dropout = 0.5, Average MSE = 360.6970957046829\n",
            "Polarity, formless, nodes = 128, dropout = 0.5, Average MSE = 359.8546526108024\n",
            "Polarity, formonly, nodes = 128, dropout = 0.5, Average MSE = 361.53420400839826\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 128, dropout = 0.6, Average MSE = 368.1581397550921\n",
            "Polarity, formless, nodes = 128, dropout = 0.6, Average MSE = 367.9181115670018\n",
            "Polarity, formonly, nodes = 128, dropout = 0.6, Average MSE = 379.00189728978313\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 256, dropout = 0.2, Average MSE = 312.36282802561465\n",
            "Polarity, formless, nodes = 256, dropout = 0.2, Average MSE = 306.14435316558627\n",
            "Polarity, formonly, nodes = 256, dropout = 0.2, Average MSE = 352.9200840471978\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 256, dropout = 0.3, Average MSE = 322.9599490702838\n",
            "Polarity, formless, nodes = 256, dropout = 0.3, Average MSE = 317.6836294504171\n",
            "Polarity, formonly, nodes = 256, dropout = 0.3, Average MSE = 352.0192026485411\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 256, dropout = 0.4, Average MSE = 330.74472732572195\n",
            "Polarity, formless, nodes = 256, dropout = 0.4, Average MSE = 326.7410110179204\n",
            "Polarity, formonly, nodes = 256, dropout = 0.4, Average MSE = 351.7969919690228\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 256, dropout = 0.5, Average MSE = 332.5107754356895\n",
            "Polarity, formless, nodes = 256, dropout = 0.5, Average MSE = 340.9323999865906\n",
            "Polarity, formonly, nodes = 256, dropout = 0.5, Average MSE = 353.2301691911864\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 256, dropout = 0.6, Average MSE = 354.85223950656786\n",
            "Polarity, formless, nodes = 256, dropout = 0.6, Average MSE = 353.01840914295616\n",
            "Polarity, formonly, nodes = 256, dropout = 0.6, Average MSE = 352.2483202798625\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 300, dropout = 0.2, Average MSE = 315.1572705939226\n",
            "Polarity, formless, nodes = 300, dropout = 0.2, Average MSE = 306.60412916669856\n",
            "Polarity, formonly, nodes = 300, dropout = 0.2, Average MSE = 354.3733622474379\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 300, dropout = 0.3, Average MSE = 316.13629993611016\n",
            "Polarity, formless, nodes = 300, dropout = 0.3, Average MSE = 307.7449058355316\n",
            "Polarity, formonly, nodes = 300, dropout = 0.3, Average MSE = 358.7419420204935\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 300, dropout = 0.4, Average MSE = 317.0434404818963\n",
            "Polarity, formless, nodes = 300, dropout = 0.4, Average MSE = 320.14557525318384\n",
            "Polarity, formonly, nodes = 300, dropout = 0.4, Average MSE = 352.64433513690307\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 300, dropout = 0.5, Average MSE = 338.1603262375927\n",
            "Polarity, formless, nodes = 300, dropout = 0.5, Average MSE = 332.97981459861643\n",
            "Polarity, formonly, nodes = 300, dropout = 0.5, Average MSE = 353.0359957942601\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 300, dropout = 0.6, Average MSE = 346.22283237283597\n",
            "Polarity, formless, nodes = 300, dropout = 0.6, Average MSE = 348.81712434557437\n",
            "Polarity, formonly, nodes = 300, dropout = 0.6, Average MSE = 347.6143721468953\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 512, dropout = 0.2, Average MSE = 324.3434044927634\n",
            "Polarity, formless, nodes = 512, dropout = 0.2, Average MSE = 314.3813624731874\n",
            "Polarity, formonly, nodes = 512, dropout = 0.2, Average MSE = 350.32025809409214\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 512, dropout = 0.3, Average MSE = 320.6519962276674\n",
            "Polarity, formless, nodes = 512, dropout = 0.3, Average MSE = 311.2831920937541\n",
            "Polarity, formonly, nodes = 512, dropout = 0.3, Average MSE = 349.72335274471976\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 512, dropout = 0.4, Average MSE = 318.7907628609108\n",
            "Polarity, formless, nodes = 512, dropout = 0.4, Average MSE = 309.8632493901958\n",
            "Polarity, formonly, nodes = 512, dropout = 0.4, Average MSE = 353.8401698286317\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 512, dropout = 0.5, Average MSE = 316.3071321744264\n",
            "Polarity, formless, nodes = 512, dropout = 0.5, Average MSE = 306.68866324463534\n",
            "Polarity, formonly, nodes = 512, dropout = 0.5, Average MSE = 353.92641135476117\n",
            "\n",
            "\n",
            "Polarity, regular, nodes = 512, dropout = 0.6, Average MSE = 312.1157422394058\n",
            "Polarity, formless, nodes = 512, dropout = 0.6, Average MSE = 320.3502297523637\n",
            "Polarity, formonly, nodes = 512, dropout = 0.6, Average MSE = 357.28040536884896\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Maker and Evaluator"
      ],
      "metadata": {
        "id": "bk6Zl3StVbSb"
      },
      "id": "bk6Zl3StVbSb"
    },
    {
      "cell_type": "code",
      "source": [
        "def fnn_maker(x_train, y_train, x_test, y_test, nodes, dropout):\n",
        "  # Input:\n",
        "  # - x_train = array of embeddings used to train the model\n",
        "  # - y_train = array of ratings used to train the model\n",
        "  # - x_test = array of embeddings used to test the model\n",
        "  # - y_test = array of ratings used to test the model\n",
        "  # - nodes = integer indicating the number of nodes to use in the hidden layer\n",
        "  # - dropout = integer indicating the amount of dropout to use in the hidden layer\n",
        "\n",
        "  # Process:\n",
        "  # Train a sequential NN using the train set and return the model & test MSE\n",
        "\n",
        "  # Output:\n",
        "  # - mse = test set mean squared error\n",
        "  # - fnn_model = trained neural network model\n",
        "\n",
        "  random.seed(17042020)                                                         # Set the seed using python's built-in seed function\n",
        "  set_seed(17042020)                                                            # Set the seed using keras/tensorflow's seed function, just to be sure\n",
        "\n",
        "  fnn_model = Sequential()                                                      # Initialize a sequential NN\n",
        "\n",
        "  fnn_model.add(Dense(nodes, input_dim=300, kernel_initializer=HeNormal(), \n",
        "                      activation=keras.layers.LeakyReLU()))                     # Add a dense layer with the specified nodes\n",
        "  fnn_model.add(Dropout(dropout))\n",
        "\n",
        "  fnn_model.add(Dense(1, activation='linear'))                                  # Add a final layer\n",
        "\n",
        "  callback = EarlyStopping(monitor = 'loss', patience=3)                        # Add early stopping that stops after 3 rounds without improvement\n",
        "\n",
        "  fnn_model.compile(optimizer=Adam(), loss='mean_squared_error')                # Compile the model with mean squared loss\n",
        "\n",
        "  fnn_model.fit(x_train, y_train, epochs=100, batch_size=len(x_train), \n",
        "                callbacks=[callback], verbose=0)                                # Fit the model on the train set\n",
        "\n",
        "  #y_pred = fnn_model.predict(x_test)                                            # Feed the test cases to the model to retrieve predictions\n",
        "  \n",
        "  #mse = mean_squared_error(y_test, y_pred)                                      # Calculate test-set MSE\n",
        "\n",
        "  #return mse, fnn_model\n",
        "\n",
        "  return fnn_model"
      ],
      "metadata": {
        "id": "IWq2EO-683go"
      },
      "id": "IWq2EO-683go",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluator(trained_model, x_train, x_test, y_train, y_test, pred_dict, \n",
        "                    test_names, test_name_types, lexical = None):\n",
        "  \n",
        "  #### Add predictions to the pred_dict\n",
        "  if lexical is True:\n",
        "    mean_vec_prediction = float(trained_model.predict(mean_vector_subwordless.reshape(1, -1)))\n",
        "  else:\n",
        "    mean_vec_prediction = float(trained_model.predict(mean_vector.reshape(1, -1)))\n",
        "\n",
        "  for n, t, i, j in zip(test_names, test_name_types, y_test, x_test):           # For every name in the test set\n",
        "      n = n.lower()                                                             # Convert name to lowercase\n",
        "      pred_dict[n] = [t, i, float(trained_model.predict(j.reshape(1, -1)))]     # Add predictions to the predictions dictionary\n",
        "      pred_dict[n + '_mean_vector'] = [t, i, mean_vec_prediction]               # Add predictions to the mean_vector predictions dictionary\n",
        "\n",
        "  \n",
        "  ##############################################################################\n",
        "\n",
        "  y_pred = trained_model.predict(x_test)\n",
        "\n",
        "  mae_test = mean_absolute_error(y_test, y_pred)\n",
        "  \n",
        "  ##### MAE per name type ######################################################\n",
        "\n",
        "  type_dict = {}\n",
        "  type_counter = {}\n",
        "  for n, i, j in zip(test_name_types, y_test, x_test):                          # For every name type (i.e., real, talking, and madeup)\n",
        "    if n in type_dict.keys():\n",
        "      type_dict[n] = type_dict[n] + abs(i - trained_model.predict(j.reshape(1, -1)))   # Append the MAE for every name given that type (so that you get a sum of MAEs; one for each name)\n",
        "      type_counter[n] = type_counter[n] + 1                                     # And count the number of names given that type\n",
        "    else:\n",
        "      type_dict[n] = abs(i - trained_model.predict(j.reshape(1, -1)))\n",
        "      type_counter[n] = 1\n",
        "\n",
        "  for i in type_dict.keys():\n",
        "    globals()[f\"mae_{i}\"] = float(type_dict[i])/float(type_counter[i])          # Calculate the average MAE per name type: (sum of MAEs for name type / name counter for name type)\n",
        "\n",
        "  if 'madeup' not in type_dict.keys():\n",
        "    globals()[f\"mae_madeup\"] = None\n",
        "\n",
        "  if 'talking' not in type_dict.keys():\n",
        "    globals()[f\"mae_talking\"] = None\n",
        "\n",
        "  if 'real' not in type_dict.keys():\n",
        "    globals()[f\"mae_real\"] = None\n",
        "\n",
        "  ##### Mean Only ##############################################################\n",
        "\n",
        "  if lexical is True:\n",
        "    mean_vec_array = np.full((len(x_test), 300), mean_vector_subwordless)\n",
        "  else:\n",
        "    mean_vec_array = np.full((len(x_test), 300), mean_vector)                     # Create a mean vector array with length = test_set_length, and width = 300\n",
        "\n",
        "  mean_vec_mae_test = mean_absolute_error(y_test, trained_model.predict(mean_vec_array))# Retrieve the MAE for the mean vector array\n",
        "\n",
        "  return mae_test, mae_madeup, mae_real, mae_talking, mean_vec_mae_test"
      ],
      "metadata": {
        "id": "TkYmoAHHhkMn"
      },
      "id": "TkYmoAHHhkMn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_list_pred = ['Name', 'NameType', 'TrueRating', 'CombinedRating',          # Initialize a list of column names for the prediction CSVs that will be created later\n",
        "                    'CombinedMeanVecRating', 'NgramRating',\n",
        "                    'NgramMeanVecRating', 'LexicalRating', \n",
        "                    'LexicalMeanVecRating']\n",
        "\n",
        "column_list_pred_only_comb = ['Name', 'NameType', 'TrueRating', 'CombinedRating',          # Initialize a list of column names for the prediction CSVs that will be created later\n",
        "                              'CombinedMeanVecRating']              "
      ],
      "metadata": {
        "id": "27OABdtQp29q"
      },
      "id": "27OABdtQp29q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Final Model"
      ],
      "metadata": {
        "id": "UW5MsCxom7e0"
      },
      "id": "UW5MsCxom7e0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Functions"
      ],
      "metadata": {
        "id": "5KWWakqlQFf4"
      },
      "id": "5KWWakqlQFf4"
    },
    {
      "cell_type": "code",
      "source": [
        "def fnn_maker_and_evaluator(x_train, y_train, x_test, y_test, pred_dict, \n",
        "                            test_names, test_name_types, nodes, dropout, lexical = None):\n",
        "  # Input:\n",
        "  # - x_train = array of embeddings used to train the model\n",
        "  # - y_train = array of ratings used to train the model\n",
        "  # - x_test = array of embeddings used to test the model\n",
        "  # - y_test = array of ratings used to test the model\n",
        "  # - pred_dict = a dictionary that will be filled with predictions per name\n",
        "  # - test_names = dataframe containing the full names (i.e., not the embeddings) \n",
        "  # - test_name_types = dataframe containing the name type (real, madeup, talking)\n",
        "  # - nodes = integer indicating the number of nodes to use in the hidden layer\n",
        "  # - dropout = integer indicating the amount of dropout to use in the hidden layer\n",
        "\n",
        "  # Process:\n",
        "  # Train a sequential NN using the train set using fnn_maker(). Then, evaluate\n",
        "  # the model using model_evaluator and return the values.\n",
        "\n",
        "  # Output:\n",
        "  # Too many to explain here. Basically, a bunch of metrics to test the model.\n",
        "\n",
        "  #_, fnn_model = fnn_maker(x_train, y_train, x_test, y_test, nodes, dropout)\n",
        "  \n",
        "  fnn_model = fnn_maker(x_train, y_train, x_test, y_test, nodes, dropout)\n",
        "\n",
        "  mae_test, mae_madeup, mae_real, mae_talking, mean_vec_mae_test \\\n",
        "  = model_evaluator(fnn_model, x_train, x_test, y_train, y_test, \n",
        "                    pred_dict, test_names, test_name_types, lexical)\n",
        "\n",
        "  return mae_test, mae_madeup, mae_real, mae_talking, mean_vec_mae_test"
      ],
      "metadata": {
        "id": "wJ4Vxbtaar8W"
      },
      "id": "wJ4Vxbtaar8W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network_k_folder(df, rating, dimension, dictionary, nodes, dropout):\n",
        "  # Input:\n",
        "  # - df = a dataframe with the name, name_type, and rating for the dimension at  \n",
        "  # hand (i.e., age, gender, or polarity)\n",
        "  # - rating = a string indicating what rating to extract from the df\n",
        "  # - dimension = a string indicating what dimension is considered (i.e., 'age',\n",
        "  # 'gender', or 'polarity')\n",
        "  # - dictionary = an empty dictionary to store the MSE output by fnn_maker() in \n",
        "  # per configuration\n",
        "  # - nodes = integer indicating the number of nodes to use in the hidden layer\n",
        "  # - dropout = integer indicating the amount of dropout to use in the hidden layer\n",
        "\n",
        "  # Process:\n",
        "  # Given the df, get 5 train/test splits, and per fold, train a NN model using\n",
        "  # fnn_maker() for the combined, ngram, and lexical data. Then, store these\n",
        "  # metrics in a .csv file.\n",
        "\n",
        "  # Output: \n",
        "  # - pred_dict_combined: y_true and y_pred (for several conditions) per name (combined)\n",
        "  # - pred_dict_ngram: y_true and y_pred (for several conditions) per name (ngram)\n",
        "  # - pred_dict_lexical: y_true and y_pred (for several conditions) per name (lexical)\n",
        "\n",
        "  loocv = LeaveOneOut()                                                         # Set up stratified LOOCV\n",
        "  \n",
        "  pred_dict_combined = {}                                                        # Set up three dictionaries to store the predicted y-values for the test names in\n",
        "  pred_dict_ngram = {}\n",
        "  pred_dict_lexical = {}\n",
        "\n",
        "  for train_index, test_index in loocv.split(df):                               # For every fold (stratified on the name type, i.e., real, madeup, or talking)\n",
        "    x_train_unfasttexted = df.iloc[train_index]                                 # Split the data into x_train and x_test\n",
        "    x_test_unfasttexted = df.iloc[test_index]\n",
        "\n",
        "    x_train_unf_lexical = x_train_unfasttexted[x_train_unfasttexted.name_type != 'madeup']\n",
        "    x_test_unf_lexical = x_test_unfasttexted[x_test_unfasttexted.name_type != 'madeup']\n",
        "\n",
        "    #x_train = fasttext_xifyer(x_train_unfasttexted)                             # Get the word embeddings\n",
        "    #x_train_ngram = fasttext_xifyer_ngram(x_train_unfasttexted)\n",
        "\n",
        "    x_train = fasttext_xifyer(x_train_unf_lexical)\n",
        "    x_train_ngram = fasttext_xifyer_ngram(x_train_unf_lexical)\n",
        "    \n",
        "    x_train_lexical = fasttext_xifyer_lexical(x_train_unf_lexical)\n",
        "\n",
        "    x_test = fasttext_xifyer(x_test_unf_lexical)\n",
        "    x_test_ngram = fasttext_xifyer_ngram(x_test_unf_lexical)\n",
        "    x_test_lexical = fasttext_xifyer_lexical(x_test_unf_lexical)\n",
        "\n",
        "\n",
        "    y_train = df.iloc[train_index][rating]                                      # Split the data into y_train and y_test\n",
        "    y_test = df.iloc[test_index][rating]\n",
        "\n",
        "    y_train_lexical = df.iloc[train_index][df.name_type != 'madeup'][rating] \n",
        "    y_test_lexical = df.iloc[test_index][df.name_type != 'madeup'][rating]\n",
        "\n",
        "\n",
        "    test_names = df.iloc[test_index]['name']                                    # Get a list of the names in the test set\n",
        "    test_name_types = df.iloc[test_index]['name_type']                          # Get a list of name types corresponding to the names in the test set\n",
        "\n",
        "    test_names_lexical = df.iloc[test_index][df.name_type != 'madeup']['name']                                    \n",
        "    test_name_types_lexical = df.iloc[test_index][df.name_type != 'madeup']['name_type']                          \n",
        "\n",
        "    try:\n",
        "      if x_test[0] == \"\":\n",
        "        print('hi')\n",
        "    \n",
        "    except IndexError:\n",
        "      continue\n",
        "\n",
        "    #try:\n",
        "    mae_test, mae_madeup, mae_real, mae_talking, mean_vec_mae_test \\\n",
        "    = fnn_maker_and_evaluator(x_train, \n",
        "                              y_train_lexical, \n",
        "                              x_test, \n",
        "                              y_test_lexical, \n",
        "                              pred_dict_combined,\n",
        "                              test_names_lexical,\n",
        "                              test_name_types_lexical, \n",
        "                              nodes,\n",
        "                              dropout)                                          # Train the NN given the fold, and return all of the variables of interest (combined)\n",
        "\n",
        "    #except ValueError:\n",
        "    #  continue\n",
        "\n",
        "    mae_test_ngram, mae_madeup_ngram, mae_real_ngram, mae_talking_ngram, \\\n",
        "    mean_vec_mae_test_ngram \\\n",
        "    = fnn_maker_and_evaluator(x_train_ngram, \n",
        "                              y_train_lexical, \n",
        "                              x_test_ngram, \n",
        "                              y_test_lexical, \n",
        "                              pred_dict_ngram,\n",
        "                              test_names_lexical,\n",
        "                              test_name_types_lexical, \n",
        "                              nodes,\n",
        "                              dropout)                                          # Train the NN given the fold, and return all of the variables of interest (ngram)\n",
        "    \n",
        "    #try:\n",
        "    mae_test_lexical, mae_madeup_lexical, mae_real_lexical, mae_talking_lexical, \\\n",
        "    mean_vec_mae_test_lexical, \\\n",
        "    = fnn_maker_and_evaluator(x_train_lexical, \n",
        "                              y_train_lexical, \n",
        "                              x_test_lexical, \n",
        "                              y_test_lexical, \n",
        "                              pred_dict_lexical,\n",
        "                              test_names_lexical,\n",
        "                              test_name_types_lexical, \n",
        "                              nodes,\n",
        "                              dropout, \n",
        "                              lexical = True)                                 # Train the NN given the fold, and return all of the variables of interest (lexical)\n",
        "\n",
        "    dictionary['lexical'].append([mae_test_lexical, mae_madeup_lexical, \n",
        "                                mae_real_lexical, mae_talking_lexical,\n",
        "                                mean_vec_mae_test_lexical])  # Append the variables of interest to the dictionary\n",
        "\n",
        "\n",
        "    #except ValueError:\n",
        "    #  pass\n",
        "\n",
        "    dictionary['combined'].append([mae_test, mae_madeup, mae_real, mae_talking, \n",
        "                                  mean_vec_mae_test])                    # Append the variables of interest to the dictionary\n",
        "\n",
        "    dictionary['ngram'].append([mae_test_ngram, mae_madeup_ngram, \n",
        "                                mae_real_ngram, mae_talking_ngram, \n",
        "                                mean_vec_mae_test_ngram])    # Append the variables of interest to the dictionary\n",
        "    \n",
        "  \n",
        "  column_list = ('dimension', 'analysis_type', 'mae_test', 'mae_madeup', \n",
        "                 'mae_real', 'mae_talking', 'mean_vec_mae_test', 'sd_mae_total', \n",
        "                 'sd_mae_madeup', 'sd_mae_real', 'sd_mae_talking', 'sd_mean_vec_mae')         # List indicating all of the variables of interest \n",
        "  \n",
        "  type_list = ['combined', 'ngram', 'lexical']                                  # List indicating the model type\n",
        "\n",
        "  sd_dict_nn = {'combined' : {'total' : [], 'madeup': [], 'real': [], 'talking': [], 'mean_vec_mae': []}, \n",
        "                'ngram' : {'total' : [], 'madeup': [], 'real': [], 'talking': [], 'mean_vec_mae': []}, \n",
        "                'lexical': {'total' : [], 'madeup': [], 'real': [], 'talking': [], 'mean_vec_mae': []}}\n",
        "\n",
        "  for analysis_type in type_list:    \n",
        "    sd_mae_total = []\n",
        "    sd_mae_madeup = []\n",
        "    sd_mae_real = []\n",
        "    sd_mae_talking = []\n",
        "\n",
        "    sd_mean_vec_mae = []\n",
        "\n",
        "    for iteration in dictionary[analysis_type]:\n",
        "      sd_mae_total.append(iteration[0])\n",
        "      sd_mae_madeup.append(iteration[1])\n",
        "      sd_mae_real.append(iteration[2])\n",
        "      sd_mae_talking.append(iteration[3])\n",
        "\n",
        "      sd_mean_vec_mae.append(iteration[4])\n",
        "\n",
        "    sd_dict_nn[analysis_type]['total'] = np.std(sd_mae_total)\n",
        "    if analysis_type != None: #'lexical':\n",
        "      sd_mae_madeup = list(filter(None, sd_mae_madeup))\n",
        "      sd_dict_nn[analysis_type]['madeup'] = np.std(sd_mae_madeup)\n",
        "    else:\n",
        "      sd_dict_nn[analysis_type]['madeup'] = None\n",
        "    \n",
        "    sd_mae_real = list(filter(None, sd_mae_real))\n",
        "    sd_dict_nn[analysis_type]['real'] = np.std(sd_mae_real)\n",
        "\n",
        "    sd_mae_talking = list(filter(None, sd_mae_talking))\n",
        "    sd_dict_nn[analysis_type]['talking'] = np.std(sd_mae_talking)\n",
        "\n",
        "    sd_mae_sd_mean_vec_mae = list(filter(None, sd_mean_vec_mae))\n",
        "    sd_dict_nn[analysis_type]['mean_vec_mae'] = np.std(sd_mean_vec_mae)\n",
        "\n",
        "  combined_list = dictionary['combined']\n",
        "  ngram_list = dictionary['ngram']\n",
        "  lexical_list = dictionary['lexical']\n",
        "\n",
        "  combined_list = [np.mean(x) for x in [list(filter(None, x)) for x in zip(*combined_list)]]                          \n",
        "\n",
        "  ngram_list = [np.mean(x) for x in [list(filter(None, x)) for x in zip(*ngram_list)]]                                                \n",
        "\n",
        "  lexical_list = [np.mean(x) for x in [list(filter(None, x)) for x in zip(*lexical_list)]]                  \n",
        "\n",
        "  csv_df = []                                                                   # Create a list of lists that will be converted to a dataframe\n",
        "\n",
        "  for value_list, analysis_type in zip([combined_list, ngram_list,            # Given the list of metrics for every model (combined, ngram, lexical)\n",
        "                                        lexical_list], type_list):\n",
        "    value_list.insert(0, analysis_type)                                         # Insert the name of the analysis type to the values (i.e., 'combined', etc.)\n",
        "    value_list.insert(0, dimension)                                             # Insert the name of the dimension (i.e., 'age', 'gender', 'polarity') to the values\n",
        "    value_list.append(sd_dict_nn[analysis_type]['total'])\n",
        "    value_list.append(sd_dict_nn[analysis_type]['madeup'])\n",
        "    value_list.append(sd_dict_nn[analysis_type]['real'])\n",
        "    value_list.append(sd_dict_nn[analysis_type]['talking'])\n",
        "    value_list.append(sd_dict_nn[analysis_type]['mean_vec_mae'])\n",
        "    csv_df.append(value_list)                                                   # Add the list of values as a row to the DF list of lists\n",
        "  \n",
        "  csv_df = pd.DataFrame(csv_df, columns=column_list)                            # Convert the list of lists to a DF\n",
        "  csv_df.to_csv(csv_path + dimension +'_nn_metrics.csv', index=False)           # Save the DF as a .csv file\n",
        "\n",
        "  return pred_dict_combined, pred_dict_ngram, pred_dict_lexical"
      ],
      "metadata": {
        "id": "HzXZVhhwfvXG"
      },
      "id": "HzXZVhhwfvXG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Running the functions"
      ],
      "metadata": {
        "id": "ZwKSkMAaQKCV"
      },
      "id": "ZwKSkMAaQKCV"
    },
    {
      "cell_type": "code",
      "source": [
        "age_dict_nn_final = {'combined' : [], 'ngram' : [], 'lexical': []}          # initialize the score dictionary for age\n",
        "\n",
        "age_pred_dict_combined_nn, age_pred_dict_ngram_nn, \\\n",
        "age_pred_dict_lexical_nn = \\\n",
        "neural_network_k_folder(df_age, 'rating.mean_age', 'age', \n",
        "                        age_dict_nn_final, 300, 0.5)                            # Perform the 5-fold cross validation and save the metrics as a .csv file\n",
        "\n",
        "pickle_age_nn_final = open(pickle_path + \"age_nn_final.pickle\", \"wb\")           # Save the dictionary to a pickle\n",
        "pickle.dump(age_dict_nn_final, pickle_age_nn_final)\n",
        "pickle_age_nn_final.close()"
      ],
      "metadata": {
        "id": "uRe68H44NzG3"
      },
      "id": "uRe68H44NzG3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_pred_nn_df = []                                                             # Create a list of lists that will be converted to a dataframe\n",
        "\n",
        "for i in sorted(df_age['name']):                                                # For every name in the dataframe\n",
        "  i = i.lower()\n",
        "\n",
        "  try:\n",
        "    age_pred_nn_df.append([i, age_pred_dict_combined_nn[i][0],                     # Append the name, name type, and normal + mean_vector only predictions for the\n",
        "                          age_pred_dict_combined_nn[i][1],                        # three model types as a row to the list of lists\n",
        "                          age_pred_dict_combined_nn[i][2],\n",
        "                          age_pred_dict_combined_nn[i + '_mean_vector'][2], \n",
        "                          age_pred_dict_ngram_nn[i][2], \n",
        "                          age_pred_dict_ngram_nn[i + '_mean_vector'][2], \n",
        "                          age_pred_dict_lexical_nn[i][2],\n",
        "                          age_pred_dict_lexical_nn[i + '_mean_vector'][2]])\n",
        "    \n",
        "  except KeyError:\n",
        "    continue\n",
        "    age_pred_nn_df.append([i, age_pred_dict_combined_nn[i][0],                     # Append the name, name type, and normal + mean_vector only predictions for the\n",
        "                          age_pred_dict_combined_nn[i][1],                        # three model types as a row to the list of lists\n",
        "                          age_pred_dict_combined_nn[i][2],\n",
        "                          age_pred_dict_combined_nn[i + '_mean_vector'][2], \n",
        "                          age_pred_dict_ngram_nn[i][2], \n",
        "                          age_pred_dict_ngram_nn[i + '_mean_vector'][2], \n",
        "                          None, None])\n",
        "  \n",
        "age_pred_nn_df = pd.DataFrame(age_pred_nn_df, columns=column_list_pred)         # Convert list of lists to DF\n",
        "\n",
        "age_pred_nn_df.to_csv(csv_path + 'age_pred_nn.csv', index=False)                # Save DF as .csv"
      ],
      "metadata": {
        "id": "GrhHgAbgru1Q"
      },
      "id": "GrhHgAbgru1Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_dict_nn_final = {'combined' : [], 'ngram' : [], 'lexical': []}       # initialize the score dictionary for gender\n",
        "\n",
        "gender_pred_dict_combined_nn, gender_pred_dict_ngram_nn, \\\n",
        "gender_pred_dict_lexical_nn = \\\n",
        "neural_network_k_folder(df_gender, 'rating.mean_gender', 'gender', \n",
        "                        gender_dict_nn_final, 512, 0.3)                         # Perform the 5-fold cross validation and save the metrics as a .csv file\n",
        "\n",
        "pickle_gender_nn_final = open(pickle_path + \"gender_nn_final.pickle\", \"wb\")     # Save the dictionary to a pickle\n",
        "pickle.dump(gender_dict_nn_final, pickle_gender_nn_final)\n",
        "pickle_gender_nn_final.close()"
      ],
      "metadata": {
        "id": "iGyedeucLT-z"
      },
      "id": "iGyedeucLT-z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_pred_nn_df = []                                                          # Create a list of lists that will be converted to a dataframe\n",
        "\n",
        "for i in sorted(df_gender['name']):                                             # For every name in the dataframe\n",
        "  i = i.lower()\n",
        "  try:\n",
        "    gender_pred_nn_df.append([i, gender_pred_dict_combined_nn[i][0],               # Append the name, name type, and normal + mean_vector only predictions for the\n",
        "                          gender_pred_dict_combined_nn[i][1],                     # three model types as a row to the list of lists\n",
        "                          gender_pred_dict_combined_nn[i][2],\n",
        "                          gender_pred_dict_combined_nn[i + '_mean_vector'][2], \n",
        "                          gender_pred_dict_ngram_nn[i][2], \n",
        "                          gender_pred_dict_ngram_nn[i + '_mean_vector'][2], \n",
        "                          gender_pred_dict_lexical_nn[i][2],\n",
        "                          gender_pred_dict_lexical_nn[i + '_mean_vector'][2]])\n",
        "\n",
        "  except KeyError:\n",
        "   continue\n",
        "   gender_pred_nn_df.append([i, gender_pred_dict_combined_nn[i][0],               # Append the name, name type, and normal + mean_vector only predictions for the\n",
        "                          gender_pred_dict_combined_nn[i][1],                     # three model types as a row to the list of lists\n",
        "                          gender_pred_dict_combined_nn[i][2],\n",
        "                          gender_pred_dict_combined_nn[i + '_mean_vector'][2], \n",
        "                          gender_pred_dict_ngram_nn[i][2], \n",
        "                          gender_pred_dict_ngram_nn[i + '_mean_vector'][2], \n",
        "                          None,\n",
        "                          None])\n",
        "\n",
        "gender_pred_nn_df = pd.DataFrame(gender_pred_nn_df, columns=column_list_pred)   # Convert list of lists to DF\n",
        "\n",
        "gender_pred_nn_df.to_csv(csv_path + 'gender_pred_nn.csv', index=False)          # Save DF as .csv "
      ],
      "metadata": {
        "id": "KxZU3n2jtv2W"
      },
      "id": "KxZU3n2jtv2W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polarity_dict_nn_final = {'combined' : [], 'ngram' : [], 'lexical': []}     # initialize the score dictionary for polarity\n",
        "\n",
        "polarity_pred_dict_combined_nn, polarity_pred_dict_ngram_nn, \\\n",
        "polarity_pred_dict_lexical_nn = \\\n",
        "neural_network_k_folder(df_polarity, 'rating.mean_valence', 'polarity', \n",
        "                        polarity_dict_nn_final, 512, 0.5)                       # Perform the 5-fold cross validation and save the metrics as a .csv file\n",
        "\n",
        "pickle_polarity_nn_final = open(pickle_path + \"polarity_nn_final.pickle\", \"wb\") # Save the dictionary to a pickle\n",
        "pickle.dump(polarity_dict_nn_final, pickle_polarity_nn_final)\n",
        "pickle_polarity_nn_final.close()"
      ],
      "metadata": {
        "id": "1Lv1Dqcu6n-9",
        "outputId": "face5887-d367-4036-e948-5a8064c7bcdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1Lv1Dqcu6n-9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:263: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  keepdims=keepdims, where=where)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in true_divide\n",
            "  subok=False)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12.62522841454591, nan, 11.387783288955688, 13.86267385482788, 16.383217949512876]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "polarity_pred_nn_df = []                                                        # Create a list of lists that will be converted to a dataframe\n",
        "\n",
        "for i in sorted(df_polarity['name']):                                            # For every name in the dataframe\n",
        "  i = i.lower()\n",
        "  \n",
        "  try:\n",
        "    polarity_pred_nn_df.append([i, polarity_pred_dict_combined_nn[i][0],           # Append the name, name type, and normal + mean_vector only predictions for the\n",
        "                                polarity_pred_dict_combined_nn[i][1],              # three model types as a row to the list of lists\n",
        "                                polarity_pred_dict_combined_nn[i][2],\n",
        "                                polarity_pred_dict_combined_nn[i + '_mean_vector'][2], \n",
        "                                polarity_pred_dict_ngram_nn[i][2], \n",
        "                                polarity_pred_dict_ngram_nn[i + '_mean_vector'][2], \n",
        "                                polarity_pred_dict_lexical_nn[i][2],\n",
        "                                polarity_pred_dict_lexical_nn[i + '_mean_vector'][2]])\n",
        "    \n",
        "  except KeyError:\n",
        "    continue\n",
        "    polarity_pred_nn_df.append([i, polarity_pred_dict_combined_nn[i][0],           # Append the name, name type, and normal + mean_vector only predictions for the\n",
        "                                polarity_pred_dict_combined_nn[i][1],              # three model types as a row to the list of lists\n",
        "                                polarity_pred_dict_combined_nn[i][2],\n",
        "                                polarity_pred_dict_combined_nn[i + '_mean_vector'][2], \n",
        "                                polarity_pred_dict_ngram_nn[i][2], \n",
        "                                polarity_pred_dict_ngram_nn[i + '_mean_vector'][2], \n",
        "                                None, None])\n",
        "  \n",
        "polarity_pred_nn_df = pd.DataFrame(polarity_pred_nn_df, columns=column_list_pred) # Convert list of lists to DF\n",
        "\n",
        "polarity_pred_nn_df.to_csv(csv_path + 'polarity_pred_nn.csv', index=False)   # Save DF as .csv"
      ],
      "metadata": {
        "id": "WzWlpaaR6caY"
      },
      "id": "WzWlpaaR6caY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adding all metrics CSVs together"
      ],
      "metadata": {
        "id": "nCzfcZcwOH9D"
      },
      "id": "nCzfcZcwOH9D"
    },
    {
      "cell_type": "code",
      "source": [
        "csv_complete = []                                                               # Create list of lists, that will be converted to a DF\n",
        "\n",
        "for file_name in sorted(os.listdir(csv_path)):                                  # Locate all files in the folder\n",
        "  if file_name == 'complete_metrics.csv':\n",
        "    pass\n",
        "  else:\n",
        "    if file_name[-11:] == 'metrics.csv':                                          # If the files ends with 'metrics.csv'\n",
        "      df_temp = pd.read_csv(csv_path + file_name)                                 # Convert the .csv file to a DF\n",
        "\n",
        "      if file_name[-14:] == 'nn_metrics.csv':                                     # If the file has neural network metrics\n",
        "        for row in df_temp.values.tolist():                                       # For every row\n",
        "          row = row[:2] + ['Neural Network'] + row[2:] + [None, None, None, None] # Add none to the metrics that don't apply to neural networks (e.g., alpha, l1_ratio)\n",
        "          csv_complete.append(row)                                                # Append the row to the list of lists\n",
        "      else:                                                                       # Else\n",
        "        columns = list(df_temp.columns)                                           # Save the column names as a list\n",
        "        columns = columns[:2] + ['model_type'] + columns[6:] + columns[2:6]               \n",
        "        for row in df_temp.values.tolist():                                       # For every row in the ElasticNet metrics\n",
        "          row = row[:2] + ['ElasticNet'] + row[6:] + row[2:6]\n",
        "          csv_complete.append(row)                                                # Append the row to the list of lists\n",
        "\n",
        "csv_complete = pd.DataFrame(csv_complete, columns=columns)                  # Convert list of lists to a DF\n",
        "csv_complete.to_csv(csv_path + 'complete_metrics.csv', index=False)             # Save DF as a .csv file"
      ],
      "metadata": {
        "id": "oj4ZTaCZOE4s"
      },
      "id": "oj4ZTaCZOE4s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid search (row robust)\n",
        "e.g., x = nodes, y = dropout rate\n",
        "\n",
        "Grid search: \n",
        "- **Nodes** between 128 - 512 in powers of two for number of nodes (edit: I like 128 to 1024!)\n",
        "- **Dropout rate** (My own idea for how many) between 0.3 - 0.7 (do I also want half-steps?)\n",
        "\n",
        "→ Create a grid with squares showing how good all models are (\n"
      ],
      "metadata": {
        "id": "hpxfnGBHNzvY"
      },
      "id": "hpxfnGBHNzvY"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GlR04KyQqc-R"
      },
      "id": "GlR04KyQqc-R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delta Good/Bad ; Unsupervised (basically WEAT, but not WEAT)\n",
        "We can also move to some place that is less unsupervised.\n",
        "\n",
        "→ This is the delta(good - bad) idea\n",
        "\n",
        "<br>\n",
        "\n",
        "⇒ For every model, you can get the embedding for good and for bad, and for every name you can check whether it’s closer to good or closer to bad. (using the words used in the participant questionnaire). \n",
        "\n",
        "<br>\n",
        "\n",
        "So: cos(name, ‘good’) - cos(name, ‘evil’) for both the n-gram and the lexical model. So for the n-gram model, don’t use the lexical form for the name, but do include it for ‘good’ and ‘bad’\n",
        "\n",
        "<br>\n",
        "\n",
        "So dataset has: name, model (n-gram or lexical), name type, attribute, delta, ratings\n",
        "\n",
        "Then predict: delta ~ rating (maybe also * name type)\n",
        "\n",
        "→ per attribute\n"
      ],
      "metadata": {
        "id": "0rF5Rb9BrBrk"
      },
      "id": "0rF5Rb9BrBrk"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GGQN7IfcrTu-"
      },
      "id": "GGQN7IfcrTu-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Hypothesis Testing\n",
        "\n",
        "So for each, we need a dataset where we have the name, the attribute (age, pol, gen), the model, the name type, and the absolute error (per name). \n",
        "\n",
        "→ Then, I need to create a model: AE ~ name type * model\n",
        "\n",
        "→ Fit a different model per attribute (or create an extra * attribute interaction, but for clarity of presentation, this isn’t the preferred thing since we don’t have a comparison hypothesis, so keep them separate)\n",
        "\n",
        "⇒ Expectation is that the model with \n"
      ],
      "metadata": {
        "id": "s88o0QiOqe8d"
      },
      "id": "s88o0QiOqe8d"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "edP6MW9ZVBjZ"
      },
      "id": "edP6MW9ZVBjZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature importance / Permutation Testing (and interpretation)\n",
        "For feature importance: find the AE per ngram per name, see whether there is an n-gram that is responsible for most of the prediction or not. \n",
        "\n",
        "→ Can choose to do e.g., only tri-grams\n",
        "\n",
        "<br>\n",
        "\n",
        "_What I wrote Here, I did initially under a tab called 'permutation testing', although I now believe that I can combine the two (do feature importance + directionality) at the same time_ (**see below for current (up-to-date) thoughts/discussion**)\n",
        "\n",
        "<br>\n",
        "\n",
        "This isn't really well-defined yet, but the idea is as follows: per attribute (age, gender, polarity), take the names (or randomly create names) using the bi- and tri-grams that exist in the pool of all names. Then randomly swap them out, or change them in order to find out what trigrams relate to meaning \n",
        "\n",
        "<br>\n",
        "\n",
        "**Is this the same as feature importance??** ==> What would make it different is if it would be 'valid' to do analyses on attributes with tri-grams that appear in none of the names of that attribute; else, we can basically do what I wrote under feature importance (one small caveat, what we can also still do is consider all those bi- and tri-grams that appeared in the names for a single attribute, and mess around with those??)\n",
        "\n",
        "<br>\n",
        "\n",
        "After some extra deliberation, permutation testing **is not** the same as feature importance. \n",
        "\n",
        "**Here's why:** feature importance merely tells us how important it was for the prediction, but permutation testing (or at least the analysis that I want to do) would also indicate the <u>direction of the effect</u>!!\n",
        "\n",
        "<br>\n",
        "\n",
        "**Current Ideas:**\n",
        "- Per dimension, per name, randomly drop one of the n-grams and\n",
        "    - Record the difference in AE (note down the AE and the AE change compared to to the full name)\n",
        "    - Record the difference in prediction (note down the prediction and the prediction change compared to the full name)\n",
        "    - --> So, now I can interpret these differences per name, **but** I can also aggregate the scores per tri-gram over all of the names (if it appears in more than one), so that I can try to generalize it more. Because, what would it mean if a tri-gram in one name makes it more feminine, and more masculine in another?\n",
        "\n",
        "- **Tentative idea**: Per dimension, mash bi-grams and tri-grams together that have appeared in the name, but irrespective of whether they make sense together (do keep in mind that there can be a hard maximum of 1 BoS and 1 EoS symbol). By doing this, we kind of create our own samples for testing feature importance / directionality.\n"
      ],
      "metadata": {
        "id": "or5ojbQXrcRL"
      },
      "id": "or5ojbQXrcRL"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iS72L7h-rcup"
      },
      "id": "iS72L7h-rcup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PCA → Visualization\n",
        "Get it down to 2 dimensions, because we want to visualize in 2-d\n",
        "\n",
        "<br>\n",
        "\n",
        "So, get all the vectors in the names per model type and attribute, colour could indicate the rating for the attribute, size could be the prediction, shape could be the name type (all of this is for example)\n",
        "\n",
        "→ Can also add the two semantic differentials per attribute\n",
        "\n",
        "→ Make sure that the visualizations are nice to look at, do a lot of permutations until they’re good\n"
      ],
      "metadata": {
        "id": "vqEI01XhrdME"
      },
      "id": "vqEI01XhrdME"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2kDalCK6rd5Q"
      },
      "id": "2kDalCK6rd5Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction visualization\n",
        "→ make sure to visualize the predictions (e.g., use ggpredict in R)\n"
      ],
      "metadata": {
        "id": "WAMBlZMnreNF"
      },
      "id": "WAMBlZMnreNF"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BoLVKUH-req2"
      },
      "id": "BoLVKUH-req2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Publishing Version Semantics of Names.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
