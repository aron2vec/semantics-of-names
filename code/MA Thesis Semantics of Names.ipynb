{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "22531ca9-e756-46d1-b70a-3e471d5c92a8",
      "metadata": {
        "id": "22531ca9-e756-46d1-b70a-3e471d5c92a8"
      },
      "source": [
        "# Master Thesis on the Semantics of (made-up) Names\n",
        "\n",
        "* Author: Aron Joosse\n",
        "* Supervisor: Giovanni Cassani\n",
        "* Institution: Tilburg University\n",
        "\n",
        "Can take inspiration from: https://github.com/Masetto96/BA-Thesis-form-meaning-mapping/blob/master/form_meaning_mapping.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6549cc30-115e-4d41-a905-85e232d32540",
      "metadata": {
        "id": "6549cc30-115e-4d41-a905-85e232d32540"
      },
      "source": [
        "# Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9e01d4f6-9943-40fb-9f85-ca2aa53b4650",
      "metadata": {
        "id": "9e01d4f6-9943-40fb-9f85-ca2aa53b4650",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7777aca-7624-40c6-8bd2-d3b9611ebce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.1-py2.py3-none-any.whl (211 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3137895 sha256=0521e103abee33e15dd2e777f2834d5212584ad5099acd4aefe6b19c9d5db88c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.1\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.2.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.3 spacy-legacy-3.0.9 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext --progress-bar off\n",
        "!pip install -U spacy --progress-bar off\n",
        "!python -m spacy download en_core_web_sm\n",
        "import fasttext\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f1ffd0-4597-433a-b9c7-bb2423556386",
      "metadata": {
        "id": "92f1ffd0-4597-433a-b9c7-bb2423556386"
      },
      "source": [
        "# Data Import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Being able to access Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True) "
      ],
      "metadata": {
        "id": "_kihXMIxsBnq",
        "outputId": "d0231f3d-2fcc-4650-f272-3d6f8382aba1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_kihXMIxsBnq",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Getting the list of madeup names:\n",
        "\n",
        "ratings_csv = pd.read_csv(\"drive/MyDrive/Thesis/Data/giovanni_email_data/avgRatings_annotated.csv\",\n",
        "                          usecols = [\"name\", \"name_type\"])\n",
        "\n",
        "ratings_csv.head(10)\n",
        "\n",
        "madeup_names = []\n",
        "\n",
        "for i in ratings_csv.index:                                           ## I can do exactly the same thing for talking & real\n",
        "  if ratings_csv[\"name_type\"][i] == \"madeup\":\n",
        "    madeup_names.append(str(ratings_csv[\"name\"][i]))\n",
        "\n",
        "madeup_names_lower = list(map(lambda x: x.lower(), madeup_names))\n",
        "\n",
        "print(madeup_names[:5])\n",
        "print(len(madeup_names))\n",
        "print(madeup_names_lower[:5])\n",
        "print(len(madeup_names_lower))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AUEyOhEpf74",
        "outputId": "63f39bbf-6f34-4873-de4e-2c74284f9d23"
      },
      "id": "0AUEyOhEpf74",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Alastor', 'Alecto', 'Amabala', 'Araminta', 'Arcturus']\n",
            "60\n",
            "['alastor', 'alecto', 'amabala', 'araminta', 'arcturus']\n",
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NdAKmpO_raz-"
      },
      "id": "NdAKmpO_raz-",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "34910b3e-c2f9-4d54-983d-e75c2127f4db",
      "metadata": {
        "id": "34910b3e-c2f9-4d54-983d-e75c2127f4db"
      },
      "source": [
        "## COCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bf7f61e8-e967-4ab9-895e-6fb974df4fc4",
      "metadata": {
        "id": "bf7f61e8-e967-4ab9-895e-6fb974df4fc4"
      },
      "outputs": [],
      "source": [
        "path = \"drive/My Drive/Thesis/Data/CoCA/Text/\"\n",
        "unclean_path = path + \"texts_combined/all_texts_combined.txt\"\n",
        "unclean_corpus = open(unclean_path).read()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(unclean_corpus))\n",
        "print(unclean_corpus[:100])"
      ],
      "metadata": {
        "id": "cuIyN1lBMe05",
        "outputId": "392644c0-7a7e-4451-931a-b81cd959632d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cuIyN1lBMe05",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2977527143\n",
            "@@4170367 Headnote # A puzzle has long pervaded the criminal law : why are two offenders who commit \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a35ba177-63c9-43e0-8eb0-cc41ada452d6",
      "metadata": {
        "id": "a35ba177-63c9-43e0-8eb0-cc41ada452d6"
      },
      "source": [
        "## Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "efb0d50d-543f-4483-9b58-83812e4c7418",
      "metadata": {
        "id": "efb0d50d-543f-4483-9b58-83812e4c7418"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b17c25-4631-4971-8c9e-9b13b9322a08",
      "metadata": {
        "id": "45b17c25-4631-4971-8c9e-9b13b9322a08"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c46490a-70a1-41ae-8f63-ccf7fb08da08",
      "metadata": {
        "id": "6c46490a-70a1-41ae-8f63-ccf7fb08da08"
      },
      "source": [
        "\n",
        "## Cleaning Corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading the English spacy pipeline and removing stopwords\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.max_length = 10000000000\n",
        "\n",
        "nlp.Defaults.stop_words.remove('him')\n",
        "nlp.Defaults.stop_words.remove('her')\n",
        "nlp.Defaults.stop_words.remove('hers')\n",
        "nlp.Defaults.stop_words.remove('his')\n",
        "nlp.Defaults.stop_words.remove('he')\n",
        "nlp.Defaults.stop_words.remove('she')\n",
        "nlp.Defaults.stop_words.remove('himself')\n",
        "nlp.Defaults.stop_words.remove('herself')"
      ],
      "metadata": {
        "id": "xqknf8oNouf3"
      },
      "id": "xqknf8oNouf3",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_corpus_sentenced(data, corpus_dict, index):\n",
        "  # Tokenization\n",
        "  with nlp.select_pipes(disable=[\"lemmatizer\", \"tok2vec\", \"tagger\", \"parser\"]):\n",
        "    nlp.enable_pipe(\"senter\")\n",
        "    doc = nlp(data)\n",
        "\n",
        "  sentence = \"\"\n",
        "\n",
        "  for token in doc:\n",
        "    if token.is_sent_start is True:\n",
        "      if sentence == \"\":\n",
        "        continue\n",
        "      else:\n",
        "        corpus_dict[index] = sentence\n",
        "        sentence = \"\"\n",
        "        index += 1\n",
        "    \n",
        "    if token.is_upper is True:\n",
        "      continue\n",
        "    elif token.is_stop is True:\n",
        "      continue\n",
        "    elif str(token).lower() in madeup_names_lower:\n",
        "      continue\n",
        "    elif token.is_alpha:\n",
        "      sentence += str(token).lower()\n",
        "\n",
        "  return corpus_dict, index"
      ],
      "metadata": {
        "id": "_GxXQaOBOa9N"
      },
      "id": "_GxXQaOBOa9N",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_dict = {}\n",
        "index = 0\n",
        "\n",
        "prev_i = 0\n",
        "\n",
        "for i in range(2, 500, 2): # first until 500\n",
        "  print(i)\n",
        "  i *= 1000000\n",
        "  corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "                                              corpus_dict,\n",
        "                                              index)\n",
        "  prev_i = i\n",
        "\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:],\n",
        "#                                            corpus_dict,\n",
        "#                                            index)\n",
        "\n",
        "print(\"----------------------\")\n",
        "print(len(corpus_dict), index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrkJY7cyAQdB",
        "outputId": "eb4df837-3f1f-432c-a04d-8a7a39f4c715"
      },
      "id": "yrkJY7cyAQdB",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n",
            "12\n",
            "14\n",
            "16\n",
            "18\n",
            "20\n",
            "22\n",
            "24\n",
            "26\n",
            "28\n",
            "30\n",
            "32\n",
            "34\n",
            "36\n",
            "38\n",
            "40\n",
            "42\n",
            "44\n",
            "46\n",
            "48\n",
            "50\n",
            "52\n",
            "54\n",
            "56\n",
            "58\n",
            "60\n",
            "62\n",
            "64\n",
            "66\n",
            "68\n",
            "70\n",
            "72\n",
            "74\n",
            "76\n",
            "78\n",
            "80\n",
            "82\n",
            "84\n",
            "86\n",
            "88\n",
            "90\n",
            "92\n",
            "94\n",
            "96\n",
            "98\n",
            "100\n",
            "102\n",
            "104\n",
            "106\n",
            "108\n",
            "110\n",
            "112\n",
            "114\n",
            "116\n",
            "118\n",
            "120\n",
            "122\n",
            "124\n",
            "126\n",
            "128\n",
            "130\n",
            "132\n",
            "134\n",
            "136\n",
            "138\n",
            "140\n",
            "142\n",
            "144\n",
            "146\n",
            "148\n",
            "150\n",
            "152\n",
            "154\n",
            "156\n",
            "158\n",
            "160\n",
            "162\n",
            "164\n",
            "166\n",
            "168\n",
            "170\n",
            "172\n",
            "174\n",
            "176\n",
            "178\n",
            "180\n",
            "182\n",
            "184\n",
            "186\n",
            "188\n",
            "190\n",
            "192\n",
            "194\n",
            "196\n",
            "198\n",
            "200\n",
            "202\n",
            "204\n",
            "206\n",
            "208\n",
            "210\n",
            "212\n",
            "214\n",
            "216\n",
            "218\n",
            "220\n",
            "222\n",
            "224\n",
            "226\n",
            "228\n",
            "230\n",
            "232\n",
            "234\n",
            "236\n",
            "238\n",
            "240\n",
            "242\n",
            "244\n",
            "246\n",
            "248\n",
            "250\n",
            "252\n",
            "254\n",
            "256\n",
            "258\n",
            "260\n",
            "262\n",
            "264\n",
            "266\n",
            "268\n",
            "270\n",
            "272\n",
            "274\n",
            "276\n",
            "278\n",
            "280\n",
            "282\n",
            "284\n",
            "286\n",
            "288\n",
            "290\n",
            "292\n",
            "294\n",
            "296\n",
            "298\n",
            "300\n",
            "302\n",
            "304\n",
            "306\n",
            "308\n",
            "310\n",
            "312\n",
            "314\n",
            "316\n",
            "318\n",
            "320\n",
            "322\n",
            "324\n",
            "326\n",
            "328\n",
            "330\n",
            "332\n",
            "334\n",
            "336\n",
            "338\n",
            "340\n",
            "342\n",
            "344\n",
            "346\n",
            "348\n",
            "350\n",
            "352\n",
            "354\n",
            "356\n",
            "358\n",
            "360\n",
            "362\n",
            "364\n",
            "366\n",
            "368\n",
            "370\n",
            "372\n",
            "374\n",
            "376\n",
            "378\n",
            "380\n",
            "382\n",
            "384\n",
            "386\n",
            "388\n",
            "390\n",
            "392\n",
            "394\n",
            "396\n",
            "398\n",
            "400\n",
            "402\n",
            "404\n",
            "406\n",
            "408\n",
            "410\n",
            "412\n",
            "414\n",
            "416\n",
            "418\n",
            "420\n",
            "422\n",
            "424\n",
            "426\n",
            "428\n",
            "430\n",
            "432\n",
            "434\n",
            "436\n",
            "438\n",
            "440\n",
            "442\n",
            "444\n",
            "446\n",
            "448\n",
            "450\n",
            "452\n",
            "454\n",
            "456\n",
            "458\n",
            "460\n",
            "462\n",
            "464\n",
            "466\n",
            "468\n",
            "470\n",
            "472\n",
            "474\n",
            "476\n",
            "478\n",
            "480\n",
            "482\n",
            "484\n",
            "486\n",
            "488\n",
            "490\n",
            "492\n",
            "494\n",
            "496\n",
            "498\n",
            "----------------------\n",
            "3217085 3217085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_out = open(path + \"corpus_dict_until_500.pickle\", \"wb\")\n",
        "pickle.dump(corpus_dict, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "ieIpHHevlleF",
        "outputId": "415cd14f-bf3b-4a1a-9717-bf85e3849b89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ieIpHHevlleF",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All changes made in this colab session should now be visible in Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prev_i, i)"
      ],
      "metadata": {
        "id": "mwAxE8KIsc2Y",
        "outputId": "adda818f-fe26-429d-fe9b-a05f1aa8b889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mwAxE8KIsc2Y",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "498000000 498000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "Yy8xp4ksmVME"
      },
      "id": "Yy8xp4ksmVME",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\", force_remount=True) \n",
        "corpus_dict = {}\n",
        "\n",
        "for i in range(500, 1000, 2): #  secondly until 1000\n",
        "  print(i)\n",
        "  i *= 1000000\n",
        "  corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "                                              corpus_dict,\n",
        "                                              index)\n",
        "  prev_i = i\n",
        "\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:],\n",
        "#                                            corpus_dict,\n",
        "#                                            index)\n",
        "\n",
        "print(\"----------------------\")\n",
        "print(len(corpus_dict), index)"
      ],
      "metadata": {
        "id": "GstQVTX7j7gV",
        "outputId": "70bb2f64-5408-4501-e516-e1bed1d982cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GstQVTX7j7gV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "500\n",
            "502\n",
            "504\n",
            "506\n",
            "508\n",
            "510\n",
            "512\n",
            "514\n",
            "516\n",
            "518\n",
            "520\n",
            "522\n",
            "524\n",
            "526\n",
            "528\n",
            "530\n",
            "532\n",
            "534\n",
            "536\n",
            "538\n",
            "540\n",
            "542\n",
            "544\n",
            "546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_out = open(path + \"corpus_dict_until_1000.pickle\", \"wb\")\n",
        "pickle.dump(corpus_dict, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "yYC_tk96lesU"
      },
      "id": "yYC_tk96lesU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\", force_remount=True) \n",
        "corpus_dict = {}\n",
        "\n",
        "for i in range(1000, 1500, 2): # thirdly until 1500\n",
        "  print(i)\n",
        "  i *= 1000000\n",
        "  corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "                                              corpus_dict,\n",
        "                                              index)\n",
        "  prev_i = i\n",
        "\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:],\n",
        "#                                            corpus_dict,\n",
        "#                                            index)\n",
        "\n",
        "print(\"----------------------\")\n",
        "print(len(corpus_dict), index)"
      ],
      "metadata": {
        "id": "gohTO2KmkIQr"
      },
      "id": "gohTO2KmkIQr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_out = open(path + \"corpus_dict_until_1500.pickle\", \"wb\")\n",
        "pickle.dump(corpus_dict, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "DV6xmIQ2ltun"
      },
      "id": "DV6xmIQ2ltun",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\", force_remount=True) \n",
        "corpus_dict = {}\n",
        "\n",
        "for i in range(1500, 2000, 2): # fourthly until 2000\n",
        "  print(i)\n",
        "  i *= 1000000\n",
        "  corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "                                              corpus_dict,\n",
        "                                              index)\n",
        "  prev_i = i\n",
        "\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:],\n",
        "#                                            corpus_dict,\n",
        "#                                            index)\n",
        "\n",
        "print(\"----------------------\")\n",
        "print(len(corpus_dict), index)"
      ],
      "metadata": {
        "id": "bZsre5hAkaYV"
      },
      "id": "bZsre5hAkaYV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_out = open(path + \"corpus_dict_until_2000.pickle\", \"wb\")\n",
        "pickle.dump(corpus_dict, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "FZ225tf4lyZK"
      },
      "id": "FZ225tf4lyZK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\", force_remount=True) \n",
        "corpus_dict = {}\n",
        "\n",
        "for i in range(2000, 2500, 2): # fifthly until 2500\n",
        "  print(i)\n",
        "  i *= 1000000\n",
        "  corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "                                              corpus_dict,\n",
        "                                              index)\n",
        "  prev_i = i\n",
        "\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:],\n",
        "#                                            corpus_dict,\n",
        "#                                            index)\n",
        "\n",
        "print(\"----------------------\")\n",
        "print(len(corpus_dict), index)"
      ],
      "metadata": {
        "id": "0QNyLjA6kh1D"
      },
      "id": "0QNyLjA6kh1D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_out = open(path + \"corpus_dict_until_2500.pickle\", \"wb\")\n",
        "pickle.dump(corpus_dict, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "izwUF5AwmEcC"
      },
      "id": "izwUF5AwmEcC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\", force_remount=True) \n",
        "corpus_dict = {}\n",
        "\n",
        "for i in range(2500, 2976, 2): # lastly until end\n",
        "  print(i)\n",
        "  i *= 1000000\n",
        "  corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "                                              corpus_dict,\n",
        "                                              index)\n",
        "  prev_i = i\n",
        "\n",
        "corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:],\n",
        "                                            corpus_dict,\n",
        "                                            index)\n",
        "\n",
        "print(\"----------------------\")\n",
        "print(len(corpus_dict), index)"
      ],
      "metadata": {
        "id": "um9Ymfr2kRw7"
      },
      "id": "um9Ymfr2kRw7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "voE0vp-_kAol"
      },
      "id": "voE0vp-_kAol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_out = open(path + \"corpus_dict_until_end.pickle\", \"wb\")\n",
        "pickle.dump(corpus_dict, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "wVPMKCOeBMOo"
      },
      "id": "wVPMKCOeBMOo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a34cc32e-33c0-4f47-a08f-e7b37dd30f8a",
      "metadata": {
        "id": "a34cc32e-33c0-4f47-a08f-e7b37dd30f8a"
      },
      "outputs": [],
      "source": [
        "def clean_corpus_unsentenced(data):\n",
        "    # Tokenization\n",
        "    with nlp.select_pipes(disable=[\"lemmatizer\", \"tok2vec\", \"tagger\", \"parser\"]):\n",
        "      nlp.enable_pipe(\"senter\")\n",
        "      doc = nlp(data)\n",
        "    print(doc[:150])\n",
        "\n",
        "    doc_filtered = []\n",
        "\n",
        "    for token in doc:\n",
        "      if token.is_upper is True:\n",
        "        continue\n",
        "      elif token.is_stop is True:\n",
        "        continue\n",
        "      elif str(token).lower() in madeup_names_lower:\n",
        "        continue\n",
        "      elif token.is_alpha:\n",
        "        doc_filtered.append(str(token).lower())\n",
        "      else: \n",
        "        continue\n",
        "\n",
        "    doc_filtered = \" \".join(doc_filtered)\n",
        "\n",
        "    print(doc_filtered[:500])\n",
        "\n",
        "    # Remove words with freq < XX\n",
        "\n",
        "clean_corpus_unsentenced(unclean_corpus)#[:1000000])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09e790a-e08f-4144-b01e-25af29a8816f",
      "metadata": {
        "id": "a09e790a-e08f-4144-b01e-25af29a8816f"
      },
      "source": [
        "## Training fastText and Validating on Word Embeddings Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3909f0-5298-4ef8-b578-c759a62ab17c",
      "metadata": {
        "id": "0d3909f0-5298-4ef8-b578-c759a62ab17c"
      },
      "outputs": [],
      "source": [
        "# Skipgram model :\n",
        "#model = fasttext.train_unsupervised('data.txt', model='skipgram')\n",
        "\n",
        "#model.save_model(\"model_filename.bin\")\n",
        "\n",
        "#model = fasttext.load_model(\"model_filename.bin\")\n",
        "\n",
        "#model.get_nearest_neighbors('asparagus')\n",
        "\n",
        "#In a similar spirit, one can play around with word analogies. For example, we can see if our model can guess what is to France, and what Berlin is to Germany.\n",
        "#This can be done with the analogies functionality. It takes a word triplet (like Germany Berlin France) and outputs the analogy:\n",
        "#model.get_analogies(\"berlin\", \"germany\", \"france\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "MCJ592ZYMSXB",
        "outputId": "7ef56a52-2e44-478d-de55-13a6292ad0cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MCJ592ZYMSXB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All changes made in this colab session should now be visible in Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa3cc3c-e1a4-489b-b0c6-e25bd045783f",
      "metadata": {
        "id": "aaa3cc3c-e1a4-489b-b0c6-e25bd045783f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "MA Thesis Semantics of Names.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}