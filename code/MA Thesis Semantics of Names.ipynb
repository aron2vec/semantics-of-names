{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "22531ca9-e756-46d1-b70a-3e471d5c92a8",
      "metadata": {
        "id": "22531ca9-e756-46d1-b70a-3e471d5c92a8"
      },
      "source": [
        "# Master Thesis on the Semantics of (made-up) Names\n",
        "\n",
        "* Author: Aron Joosse\n",
        "* Supervisor: Giovanni Cassani\n",
        "* Institution: Tilburg University\n",
        "\n",
        "Can take inspiration from: https://github.com/Masetto96/BA-Thesis-form-meaning-mapping/blob/master/form_meaning_mapping.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6549cc30-115e-4d41-a905-85e232d32540",
      "metadata": {
        "id": "6549cc30-115e-4d41-a905-85e232d32540"
      },
      "source": [
        "# Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9e01d4f6-9943-40fb-9f85-ca2aa53b4650",
      "metadata": {
        "id": "9e01d4f6-9943-40fb-9f85-ca2aa53b4650",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cadc777e-2c47-47fc-ecc1-dded51b86790"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3146874 sha256=36c84a7eec760cf1bc68b6c1ad278a4ca63805431b6c31e74eb85919fb87ee55\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.2\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.2.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hRequirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.1.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (653 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l\n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.8)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: typing-extensions, catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "Successfully installed catalogue-2.0.7 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.4 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.15 typer-0.4.1 typing-extensions-3.10.0.2\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 29.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.4)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\n",
            "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext --progress-bar off\n",
        "!pip install -U spacy --progress-bar off\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Preprocessing\n",
        "import re\n",
        "import os\n",
        "import spacy\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "\n",
        "# FastText\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "\n",
        "# MEN and SimLex Benchmarks\n",
        "from os import listdir\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ElasticNet and ANN\n",
        "import sklearn\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import absolute\n",
        "from sklearn.model_selection import cross_val_score, RepeatedKFold, train_test_split, StratifiedKFold, KFold\n",
        "from sklearn.linear_model import ElasticNetCV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f1ffd0-4597-433a-b9c7-bb2423556386",
      "metadata": {
        "id": "92f1ffd0-4597-433a-b9c7-bb2423556386"
      },
      "source": [
        "# Data Import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Getting the list of madeup names:\n",
        "drive.mount(\"/content/drive\", force_remount=True) \n",
        "ratings_csv = pd.read_csv(\"drive/MyDrive/Thesis/Data/giovanni_email_data/avgRatings_annotated.csv\",\n",
        "                          usecols = [\"name\", \"name_type\"])                      ## Importing the names and name types\n",
        "\n",
        "ratings_csv.head(10)\n",
        "\n",
        "madeup_names = []\n",
        "\n",
        "for i in ratings_csv.index:                                                     ## Only choosing madeup names so I can filter them out of the FT vocab\n",
        "  if ratings_csv[\"name_type\"][i] == \"madeup\":\n",
        "    madeup_names.append(str(ratings_csv[\"name\"][i]))\n",
        "\n",
        "madeup_names_lower = list(map(lambda x: x.lower(), madeup_names))               ## Lowercasting the names since my entire vocab will be lowercast\n",
        "\n",
        "print(madeup_names[:5])\n",
        "print(len(madeup_names))\n",
        "print(madeup_names_lower[:5])\n",
        "print(len(madeup_names_lower))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AUEyOhEpf74",
        "outputId": "6439bfaa-b43b-4359-90f0-4ac86429f0eb"
      },
      "id": "0AUEyOhEpf74",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['Alastor', 'Alecto', 'Amabala', 'Araminta', 'Arcturus']\n",
            "60\n",
            "['alastor', 'alecto', 'amabala', 'araminta', 'arcturus']\n",
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"drive/My Drive/Thesis/Data/CoCA/Text/\"                                  ## These are the paths to easily export/import my dicts, txts, models, and pickles\n",
        "dict_path = \"drive/My Drive/Thesis/Data/CoCA/dict_pickles/\"\n",
        "unclean_path = path + \"texts_combined/all_texts_combined.txt\"\n",
        "model_path = \"drive/My Drive/Thesis/Data/CoCA/models/\"\n",
        "pickle_path = \"drive/MyDrive/Thesis/Data/fastText and others/\""
      ],
      "metadata": {
        "id": "UwhQ--sVUGHl"
      },
      "id": "UwhQ--sVUGHl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "34910b3e-c2f9-4d54-983d-e75c2127f4db",
      "metadata": {
        "id": "34910b3e-c2f9-4d54-983d-e75c2127f4db"
      },
      "source": [
        "## COCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf7f61e8-e967-4ab9-895e-6fb974df4fc4",
      "metadata": {
        "id": "bf7f61e8-e967-4ab9-895e-6fb974df4fc4"
      },
      "outputs": [],
      "source": [
        "unclean_corpus = open(unclean_path).read()                                      ## Importing the entire coca"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(unclean_corpus))                                                      ## Showing the length and first 100 characters of the coca\n",
        "print(unclean_corpus[:100])"
      ],
      "metadata": {
        "id": "cuIyN1lBMe05",
        "outputId": "486626ae-0cf2-45ff-d70e-eafae963a54c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cuIyN1lBMe05",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2977527143\n",
            "@@4170367 Headnote # A puzzle has long pervaded the criminal law : why are two offenders who commit \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a35ba177-63c9-43e0-8eb0-cc41ada452d6",
      "metadata": {
        "id": "a35ba177-63c9-43e0-8eb0-cc41ada452d6"
      },
      "source": [
        "## Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "efb0d50d-543f-4483-9b58-83812e4c7418",
      "metadata": {
        "id": "efb0d50d-543f-4483-9b58-83812e4c7418",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3541eb62-a3f0-4442-d9b5-b26d9c52b1e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "119\n",
            "179\n",
            "63\n",
            "       name  rating.mean_age    age name_type\n",
            "0  Adelaide        -0.617647    old      real\n",
            "2  Alasdair        18.709677  young      real\n",
            "3   Alastor        13.812500    old    madeup\n",
            "4    Alecto         3.593750    old    madeup\n",
            "5     Alice       -13.969697  young      real 119\n",
            "       name  rating.mean_gender  gender name_type\n",
            "0  Adelaide           45.727273  female      real\n",
            "1   Adelina           47.771429  female      real\n",
            "2  Alasdair          -35.657143    male      real\n",
            "3   Alastor          -38.833333    male    madeup\n",
            "4    Alecto          -35.722222  female    madeup 179\n",
            "        name  rating.mean_valence polarity name_type\n",
            "1    Adelina            31.621622      bad      real\n",
            "7    Amabala             5.935484     good    madeup\n",
            "8      Apple            32.444444     good   talking\n",
            "11  Arcturus           -11.166667     good    madeup\n",
            "13   Arobynn             7.645161      bad    madeup 63\n"
          ]
        }
      ],
      "source": [
        "### Read CSV File and Delete Unimportant Columns (i.e., everything that isn't the name, name type, rating, or the author's choice)\n",
        "\n",
        "### This is input for the FT model, which itself is the input for the ElasticNet and ANN regressions\n",
        "\n",
        "names_ratings = read_csv(\"drive/MyDrive/Thesis/Data/giovanni_email_data/avgRatings_annotated.csv\")\n",
        "\n",
        "#print(names_ratings.head())\n",
        "\n",
        "print(names_ratings['rating.mean_age'].notna().sum())                           ## Choosing only those rows where all columns are not NA\n",
        "print(names_ratings['rating.mean_gender'].notna().sum())\n",
        "print(names_ratings['rating.mean_valence'].notna().sum())\n",
        "\n",
        "df_age = names_ratings.loc[names_ratings['rating.mean_age'].notna(), ['name', 'rating.mean_age', 'age', 'name_type']]   ## Choosing the relevant columns\n",
        "print(df_age.head(), len(df_age))\n",
        "\n",
        "df_gender = names_ratings.loc[names_ratings['rating.mean_gender'].notna(), ['name', 'rating.mean_gender', 'gender', 'name_type']]\n",
        "print(df_gender.head(), len(df_gender))\n",
        "\n",
        "df_polarity = names_ratings.loc[names_ratings['rating.mean_valence'].notna(), ['name', 'rating.mean_valence', 'polarity', 'name_type']]\n",
        "print(df_polarity.head(), len(df_polarity))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b17c25-4631-4971-8c9e-9b13b9322a08",
      "metadata": {
        "id": "45b17c25-4631-4971-8c9e-9b13b9322a08"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c46490a-70a1-41ae-8f63-ccf7fb08da08",
      "metadata": {
        "id": "6c46490a-70a1-41ae-8f63-ccf7fb08da08"
      },
      "source": [
        "\n",
        "## Cleaning Corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading the English spacy pipeline and removing stopwords (since we are interested in gender bias, it's best to leave these words in)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.max_length = 10000000000\n",
        "\n",
        "nlp.Defaults.stop_words.remove('him')\n",
        "nlp.Defaults.stop_words.remove('her')\n",
        "nlp.Defaults.stop_words.remove('hers')\n",
        "nlp.Defaults.stop_words.remove('his')\n",
        "nlp.Defaults.stop_words.remove('he')\n",
        "nlp.Defaults.stop_words.remove('she')\n",
        "nlp.Defaults.stop_words.remove('himself')\n",
        "nlp.Defaults.stop_words.remove('herself')"
      ],
      "metadata": {
        "id": "xqknf8oNouf3"
      },
      "id": "xqknf8oNouf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_corpus_sentenced(data, corpus_dict, index):\n",
        "  ## Input: \n",
        "  # - Data = A (very) large string of corpus text\n",
        "  # - Corpus_dict = a dictionary to store individual sentences in\n",
        "  # - Index = the last index from the previous batch\n",
        "\n",
        "  ## Process: \n",
        "  # Remove all unwanted tokens, and store individual sentences in the dictionary\n",
        "\n",
        "  ## Output: \n",
        "  # - The dictionary of preprocessed sentences\n",
        "  # - The last sentence index, for the next batch to continue with (so that the order of the sentences is kept)\n",
        "\n",
        "  # Tokenization\n",
        "  with nlp.select_pipes(disable=[\"lemmatizer\", \"tok2vec\", \"tagger\", \"parser\"]):\n",
        "    nlp.enable_pipe(\"senter\")                                                   ## Helps with better segmenting into sentences\n",
        "    doc = nlp(data)\n",
        "\n",
        "  sentence = \"\"                                                                 ## Initialize an empty sentence\n",
        "\n",
        "  for token in doc:\n",
        "    if token.is_sent_start is True:                                             ## If token is the star of the sentence, add the previous sentence to the dictionary\n",
        "      if sentence == \"\":                                                        ## and create a new, clean sentence\n",
        "        continue\n",
        "      else:\n",
        "        corpus_dict[index] = sentence\n",
        "        sentence = \"\"\n",
        "        index += 1\n",
        "    \n",
        "    if token.is_upper is True:                                                  ## Remove all full-caps words\n",
        "      continue\n",
        "    elif token.is_stop is True:                                                 ## Remove all stopwords\n",
        "      continue\n",
        "    elif str(token).lower() in madeup_names_lower:                              ## Remove all words that are in my list of made-up names\n",
        "      continue\n",
        "    elif token.is_alpha:                                                        ## If the token has passed all previous tests, and it consists only of alphabetic\n",
        "      sentence += str(token).lower() + \" \"                                      ## characters, lowercast it and add an extra space to the end; continue to the next\n",
        "                                                                                ## token\n",
        "  return corpus_dict, index"
      ],
      "metadata": {
        "id": "_GxXQaOBOa9N"
      },
      "id": "_GxXQaOBOa9N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def corpus_dict_maker(data, start, end, index):\n",
        "  ## Input: \n",
        "  # - Data = The entire uncleaned corpus\n",
        "  # - Start = The character index to indicate the start of the current batch to preprocess\n",
        "  # - End = The character index to indicate the end of the current batch to preprocess\n",
        "  # - Index = The sentence index from the previous batch, to keep track of the number and order of the sentences\n",
        "\n",
        "  ## Process: \n",
        "  # Preprocess the corpus in batches, since there was not enough RAM to preprocess the entire corpus at once\n",
        "  # So, for every batch, all the characters between the start index and the end index are fed into the clean_corpus_sentenced() function\n",
        "  # and then this dictionary of sentences is saved as a pickle to Google drive\n",
        "\n",
        "  ## Output: \n",
        "  # Nothing; except that the sentence index is printed, which is used as input for the next batch (this was printed, so that it couldn't be lost if the \n",
        "  # runtime would disconnect (which it sadly did very often))\n",
        "\n",
        "  drive.mount(\"/content/drive\", force_remount=True)                             ## Connect to google drive\n",
        "  \n",
        "  corpus_dict = {}                                                              ## Create empty dictionary\n",
        "\n",
        "  prev_i = (start-2)*1000000                                                    ## Start with preprocessing the two million characters before the previous index\n",
        "                                                                                ## since the next range only preprocesses up to but not including the 'end' index\n",
        "\n",
        "  for i in range(start, end, 2):                                                ## In batches of 2 million characters, feed the batch to clean_corpus_sentenced()\n",
        "    print(i)                                                                   \n",
        "    i *= 1000000\n",
        "    corpus_dict, index = clean_corpus_sentenced(data[prev_i:i],\n",
        "                                                corpus_dict,\n",
        "                                                index)\n",
        "    prev_i = i\n",
        "  \n",
        "  if prev_i == 2976000000:                                                      ## Hardcoded; if we get near the end of the corpus, don't preprocess in a batch of\n",
        "    corpus_dict, index = clean_corpus_sentenced(data[prev_i:],                  ## 2 million characters (we would get an out of range error), but rather just \n",
        "                                                corpus_dict,                    ## preprocess the remaining characters, however many that may be\n",
        "                                                index)\n",
        "\n",
        "  print(index)\n",
        "\n",
        "  pickle_out = open(dict_path + \"corpus_dict_until_\" + str(end) + \".pickle\", \"wb\")  ## Save the dictionary as a pickle\n",
        "  pickle.dump(corpus_dict, pickle_out)\n",
        "  pickle_out.close()\n",
        "\n",
        "  drive.flush_and_unmount()                                                     ## Flush the pickle to my drive\n",
        "  print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n"
      ],
      "metadata": {
        "id": "pt5GFTOnoKSQ"
      },
      "id": "pt5GFTOnoKSQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### All of the individual batches:"
      ],
      "metadata": {
        "id": "pFnzewRdf6LG"
      },
      "id": "pFnzewRdf6LG"
    },
    {
      "cell_type": "code",
      "source": [
        "## doing it in batches to \n",
        "## (1) make it possible in terms of time and the Google afk-checker captcha pop-up, and \n",
        "## (2) to not blow out the RAM and have it break down\n",
        "\n",
        "#corpus_dict_maker(unclean_corpus, 2, 500, 0)                   \n",
        "#corpus_dict_maker(unclean_corpus, 500, 640, 3217086)           \n",
        "#corpus_dict_maker(unclean_corpus, 640, 760, 4232218)           \n",
        "#corpus_dict_maker(unclean_corpus, 760, 800, 5439287)           \n",
        "#corpus_dict_maker(unclean_corpus, 800, 900, 5888161)\n",
        "#corpus_dict_maker(unclean_corpus, 900, 980, 7020129)\n",
        "#corpus_dict_maker(unclean_corpus, 980, 1150, 7891661)\n",
        "#corpus_dict_maker(unclean_corpus, 1150, 1200, 9903820) \n",
        "#corpus_dict_maker(unclean_corpus, 1200, 1204, 10502592)"
      ],
      "metadata": {
        "id": "UnBc-d9pjKgx"
      },
      "id": "UnBc-d9pjKgx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (1202)*1000000\n",
        "#i = 1203*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            10547544)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(1203) + \"_post_1204\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################\n",
        "\n",
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (1203)*1000000\n",
        "#i = 1204*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            index + 1)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(1204) + \"_post_1204\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################\n",
        "\n",
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (1204)*1000000\n",
        "#i = 1205*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            10580543)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(1205) + \"_post_1204\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################\n",
        "\n",
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (1205)*1000000\n",
        "#i = 1206*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            index + 1)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(1206) + \"_post_1204\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "057QoMHol9Tb"
      },
      "id": "057QoMHol9Tb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#corpus_dict_maker(unclean_corpus, 1208, 1300, 10611881)\n",
        "#corpus_dict_maker(unclean_corpus, 1300, 1500, 11486498)\n",
        "#corpus_dict_maker(unclean_corpus, 1500, 1750, 13172710)\n",
        "#corpus_dict_maker(unclean_corpus, 1750, 1846, 15332847)\n",
        "#corpus_dict_maker(unclean_corpus, 1846, 1848, 16123425)\n",
        "#corpus_dict_maker(unclean_corpus, 1848, 1850, 16147433)\n",
        "#corpus_dict_maker(unclean_corpus, 1850, 1900, 16172965)\n",
        "#corpus_dict_maker(unclean_corpus, 1900, 1968, 16855832)\n",
        "#corpus_dict_maker(unclean_corpus, 1968, 1970, 17790964)\n",
        "#corpus_dict_maker(unclean_corpus, 1970, 2000, 17819821)\n",
        "#corpus_dict_maker(unclean_corpus, 2000, 2022, 18244076)\n",
        "#corpus_dict_maker(unclean_corpus, 2022, 2024, 18536113)\n",
        "#corpus_dict_maker(unclean_corpus, 2024, 2026, 18558956)\n",
        "#corpus_dict_maker(unclean_corpus, 2026, 2068, 18583534)\n",
        "#corpus_dict_maker(unclean_corpus, 2068, 2070, 19154020)\n",
        "#corpus_dict_maker(unclean_corpus, 2070, 2100, 19179335)\n",
        "#corpus_dict_maker(unclean_corpus, 2100, 2166, 19598984)\n",
        "#corpus_dict_maker(unclean_corpus, 2166, 2168, 20488725)\n",
        "#corpus_dict_maker(unclean_corpus, 2168, 2188, 20524278)"
      ],
      "metadata": {
        "id": "fWVWwqEAmMnb"
      },
      "id": "fWVWwqEAmMnb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (2186)*1000000\n",
        "#i = 2187*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            20779021)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(2187) + \"_post_2188\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################\n",
        "\n",
        "######### This block is separate because it kept crashing if I did it otherwise:\n",
        "\n",
        "#drive.mount(\"/content/drive\", force_remount=True) \n",
        "#corpus_dict = {}\n",
        "\n",
        "#prev_i = (2187)*1000000\n",
        "#i = 2188*1000000\n",
        "#corpus_dict, index = clean_corpus_sentenced(unclean_corpus[prev_i:i],\n",
        "#                                            corpus_dict,\n",
        "#                                            index + 1)\n",
        "#prev_i = i\n",
        "\n",
        "#print(index)\n",
        "\n",
        "#pickle_out = open(path + \"corpus_dict_until_\" + str(2188) + \"_post_2188\" + \".pickle\", \"wb\")\n",
        "#pickle.dump(corpus_dict, pickle_out)\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()\n",
        "#print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "rzysL5YBbz1M"
      },
      "id": "rzysL5YBbz1M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#corpus_dict_maker(unclean_corpus, 2190, 2200, 20812123)\n",
        "#corpus_dict_maker(unclean_corpus, 2200, 2310, 20945992)\n",
        "#corpus_dict_maker(unclean_corpus, 2310, 2312, 22397914)\n",
        "#corpus_dict_maker(unclean_corpus, 2312, 2400, 22424124)\n",
        "#corpus_dict_maker(unclean_corpus, 2400, 2600, 23465826)\n",
        "#corpus_dict_maker(unclean_corpus, 2600, 2800, 25199888)\n",
        "#corpus_dict_maker(unclean_corpus, 2800, 2977, 26938737)"
      ],
      "metadata": {
        "id": "tyV6gFTWb0VJ"
      },
      "id": "tyV6gFTWb0VJ",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This block of code opens all of the corpus dict pickles, and combines them together\n",
        "## into one big dictionary: corpus_dict_complete\n",
        "\n",
        "file_number = 1\n",
        "file_list = []\n",
        "for file_name in os.listdir(dict_path):                                         ## Locate all dicts in the folder\n",
        "  with open(dict_path + str(file_name), 'rb') as f:\n",
        "    exec(\"dict_\" + str(file_number) + \" = \" + \"pickle.load(f)\")\n",
        "    file_list.append(\"dict_\" + str(file_number))    \n",
        "    file_number += 1\n",
        "  \n",
        "corpus_dict_complete = {}\n",
        "for file_name in file_list:                                                     \n",
        "  corpus_dict_complete = {**corpus_dict_complete, **globals()[file_name]}       ## Add the contents of the dicts to dict_complete\n",
        "  del globals()[file_name]\n",
        "\n",
        "#print(len(corpus_dict_complete))\n",
        "#del file_number\n",
        "#del file_list\n",
        "\n",
        "#pickle_out = open(dict_path + \"corpus_dict_complete.pickle\", \"wb\")             ## Create a new pickle\n",
        "#pickle.dump(corpus_dict_complete, pickle_out)\n",
        "#del corpus_dict_complete\n",
        "#pickle_out.close()\n",
        "\n",
        "#drive.flush_and_unmount()                                                      ## Flush the pickle to my drive\n",
        "#print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "HCY3IGTxeuXN"
      },
      "id": "HCY3IGTxeuXN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This block of code opens the corpus_dict_complete pickle, and loops through the keys by index (from lowest to highest).\n",
        "## So, we loop through every sentence, in order. These are stored in two different .txt files, one where the sentence structure is remained \n",
        "## (i.e., between every sentence, we add a newline character), and one that's unsentenced (i.e., no newline character between sentences).\n",
        "\n",
        "with open(dict_path + \"corpus_dict_complete.pickle\", \"rb\") as d:                ## Open corpus_dict_complete\n",
        "  corpus_dict_complete = pickle.load(d)\n",
        "\n",
        "  with open(path + \"cleaned_sentenced_corpus_complete.txt\", \"w\") as f:          ## Open sentenced corpus .txt file\n",
        "    for key in sorted(corpus_dict_complete):\n",
        "      if len(str(corpus_dict_complete[key]).split()) < 2:                       ## Remove sentences with only 1 word (since there's no 'context' in that case)\n",
        "        continue\n",
        "      else:\n",
        "        if str(corpus_dict_complete[key])[:2] in [\"m \", \"p \", \"s \"]:            ## I have to add this, because based on manual inspection, a significant amount of \n",
        "          f.write(str(corpus_dict_complete[key])[2:] + \"\\n\")                    ## sentences start with just a \"p\", \"m\", or \"s\"\n",
        "        else:\n",
        "          f.write(str(corpus_dict_complete[key]) + \"\\n\")\n",
        "\n",
        "  with open(path + \"cleaned_unsentenced_corpus_complete.txt\", \"w\") as f2:       ## Open unsentenced corpus .txt file\n",
        "    for key in sorted(corpus_dict_complete): \n",
        "      if len(str(corpus_dict_complete[key])) < 4:                               ## Remove sentences with less than 4 characers, since based on visual inspection, I\n",
        "        continue                                                                ## saw that such sentences are mostly nonsense (i.e., not real words)\n",
        "      else:\n",
        "        if str(corpus_dict_complete[key])[:2] in [\"m \", \"p \", \"s \"]:\n",
        "          f2.write(str(corpus_dict_complete[key])[2:])\n",
        "        else:\n",
        "          f2.write(str(corpus_dict_complete[key]))\n",
        "  \n",
        "drive.flush_and_unmount()                                                       ## Flush to drive\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "YkdDjN1AodpH",
        "outputId": "b2b8493b-8584-459f-a74c-6c023c7743f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YkdDjN1AodpH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All changes made in this colab session should now be visible in Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09e790a-e08f-4144-b01e-25af29a8816f",
      "metadata": {
        "id": "a09e790a-e08f-4144-b01e-25af29a8816f"
      },
      "source": [
        "## Training fastText and Validating on MEN and SimLex999"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fastText"
      ],
      "metadata": {
        "id": "Mk9EPG5rwc0H"
      },
      "id": "Mk9EPG5rwc0H"
    },
    {
      "cell_type": "code",
      "source": [
        "# def fasttext_tuner(data_type):                                                ## I have chosen Skipgram, I won't play around with epochs or learning rate\n",
        "#   for dimensionality in [100, 300]:                                           ## I'm first checking data type and dimensionality, and choosing the most promising combination.\n",
        "#     for window_size in [2, 3, 4, 5, 6, 7]:                                    ## Then I'll check window size and choose the 3 most promising ones\n",
        "#       for min_n in [1, 2, 3]:                                                 ## Then I want to check min_n 2 and 1 to see whether adding n-gram size of 1 makes any sense\n",
        "#         for max_n in [5, 6, 7]:                                               ## Lastly, I will iterate through the 3 * 2 * 3 most promising models, finally choosing the best one and then double checking that with\n",
        "#           model = fasttext.train_unsupervised(input = path + \"cleaned_\" +     ## the 100/300 sentenced/unsentenced options, just to be sure! In total, this means training 4 + 5 + ~16 + 3 = ~30 models instead of 216\n",
        "#                                               data_type + \n",
        "#                                               \"_corpus_complete.txt\",\n",
        "#                                               model = \"skipgram\",\n",
        "#                                               dim = dimensionality, \n",
        "#                                               ws = window_size, \n",
        "#                                               minn = min_n,\n",
        "#                                               maxn = max_n)\n",
        "          \n",
        "#           model.save_model(model_path + data_type + \"_dim\" + str(dimensionality) + \n",
        "#                             \"_ws\" + str(window_size) + \"_minn\" + str(min_n) + \n",
        "#                             \"_maxn\" + str(max_n) + \".bin\")\n",
        "          \n",
        "#           print(data_type + \"_dim\" + str(dimensionality) + \"_ws\" + str(window_size) + \n",
        "#                 \"_minn\" + str(min_n) + \"_maxn\" + str(max_n))\n",
        "\n",
        "#           del model\n",
        "\n",
        "#   drive.flush_and_unmount()\n",
        "#   print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "id": "SVSZbTIchM28"
      },
      "id": "SVSZbTIchM28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa3cc3c-e1a4-489b-b0c6-e25bd045783f",
      "metadata": {
        "id": "aaa3cc3c-e1a4-489b-b0c6-e25bd045783f"
      },
      "outputs": [],
      "source": [
        "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "ft = fasttext.load_model('cc.en.300.bin')\n",
        "ft.save_model(model_path + \"pretrained_model.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fasttext_tuner(data_type, dimensionality = 300, window_size = 5, min_n = 3, max_n = 6):\n",
        "  ## Input: \n",
        "  # - Data Type: Sentenced or unsentenced, to load the corpus type to train model\n",
        "  # - Dimensionality: List of dimensionalities\n",
        "  # - Window size: List of window sizes \n",
        "  # - Min_n: List of minimum n-gram sizes \n",
        "  # - Max-n: List of maximum n-gram sizes \n",
        "\n",
        "  ## Process: \n",
        "  # For every combination of input variables, a fastText model is trained and saved\n",
        "\n",
        "  ## Output:\n",
        "  # Nothing; fastText model is trained and saved\n",
        "\n",
        "  for d in dimensionality:                                           \n",
        "    for ws in window_size:                                    \n",
        "      for minn in min_n:                                                 \n",
        "        for maxn in max_n:\n",
        "          drive.mount(\"/content/drive\", force_remount=True) \n",
        "\n",
        "          model = fasttext.train_unsupervised(input = path + \"cleaned_\" + data_type + \"_corpus_complete_without_p_s_m.txt\",\n",
        "                                              model = \"skipgram\", dim = d, ws = ws, minn = minn, maxn = maxn)\n",
        "\n",
        "          model.save_model(model_path + data_type + \"_dim\" + str(d) + \"_ws\" + str(ws) + \"_minn\" + str(minn) + \"_maxn\" + str(maxn) + \".bin\")\n",
        "\n",
        "          print(data_type + \"_dim\" + str(d) + \"_ws\" + str(ws) + \"_minn\" + str(minn) + \"_maxn\" + str(maxn))\n",
        "\n",
        "          drive.flush_and_unmount()\n",
        "          print('All changes made in this colab session should now be visible in Drive.')\n",
        "\n",
        "          del model"
      ],
      "metadata": {
        "id": "QCnCgy1QqXmQ"
      },
      "id": "QCnCgy1QqXmQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Trying out some variations of fastText with Data = sentenced & Dimensionality = 100\n",
        "\n",
        "fasttext_tuner(\"sentenced\", [100], [2], [2], [3])\n",
        "fasttext_tuner(\"sentenced\", [100], [2, 3, 4, 5, 6, 7], [2], [5, 6, 7])\n",
        "fasttext_tuner(\"sentenced\", [100], [5], [3], [6])\n",
        "fasttext_tuner(\"sentenced\", [100], [5], [1], [6])\n",
        "\n",
        "## Trying out some variations of fastText with Data = sentenced & Dimensionality = 300\n",
        "\n",
        "fasttext_tuner(\"sentenced\", [300], [3, 4], [2], [5, 6, 7])\n",
        "fasttext_tuner(\"sentenced\", [300], [5], [2], [5, 6])\n",
        "fasttext_tuner(\"sentenced\", [300], [5], [3], [6])\n",
        "fasttext_tuner(\"sentenced\", [300], [3], [3], [5])\n",
        "fasttext_tuner(\"sentenced\", [300], [3], [2], [4])\n",
        "fasttext_tuner(\"sentenced\", [300], [2, 6, 7], [2], [5])\n",
        "\n",
        "## Trying out some variations of fastText with Data = unsentenced \n",
        "\n",
        "fasttext_tuner(\"unsentenced\", [300], [5], [3], [6])\n",
        "fasttext_tuner(\"unsentenced\", [100], [5], [3], [6])"
      ],
      "metadata": {
        "id": "BEfZcRmNqXfi"
      },
      "id": "BEfZcRmNqXfi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MEN and SimLex Benchmarks"
      ],
      "metadata": {
        "id": "auG91yFzwfxP"
      },
      "id": "auG91yFzwfxP"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GVAVzmt0wixs"
      },
      "id": "GVAVzmt0wixs",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "MA Thesis Semantics of Names.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}